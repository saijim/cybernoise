[{"name":"Artificial Intelligence","feed":[{"title":"Backdoor for Debias: Mitigating Model Bias with Backdoor Attack-based Artificial Bias.","link":"http://arxiv.org/abs/2303.01504","abstract":"With the swift advancement of deep learning, state-of-the-art algorithms have been utilized in various social situations. Nonetheless, some algorithms have been discovered to exhibit biases and provide unequal results. The current debiasing methods face challenges such as poor utilization of data or intricate training requirements. In this work, we found that the backdoor attack can construct an artificial bias similar to the model bias derived in standard training. Considering the strong adjustability of backdoor triggers, we are motivated to mitigate the model bias by carefully designing reverse artificial bias created from backdoor attack. Based on this, we propose a backdoor debiasing framework based on knowledge distillation, which effectively reduces the model bias from original data and minimizes security risks from the backdoor attack. The proposed solution is validated on both image and structured datasets, showing promising results. This work advances the understanding of backdoor attacks and highlights its potential for beneficial applications. The code for the study can be found at \\url{https://anonymous.4open.science/r/DwB-BC07/}."},{"title":"Ternary Quantization: A Survey.","link":"http://arxiv.org/abs/2303.01505","abstract":"Inference time, model size, and accuracy are critical for deploying deep neural network models. Numerous research efforts have been made to compress neural network models with faster inference and higher accuracy. Pruning and quantization are mainstream methods to this end. During model quantization, converting individual float values of layer weights to low-precision ones can substantially reduce the computational overhead and improve the inference speed. Many quantization methods have been studied, for example, vector quantization, low-bit quantization, and binary/ternary quantization. This survey focuses on ternary quantization. We review the evolution of ternary quantization and investigate the relationships among existing ternary quantization methods from the perspective of projection function and optimization methods."},{"title":"Understanding and Unifying Fourteen Attribution Methods with Taylor Interactions.","link":"http://arxiv.org/abs/2303.01506","abstract":"Various attribution methods have been developed to explain deep neural networks (DNNs) by inferring the attribution/importance/contribution score of each input variable to the final output. However, existing attribution methods are often built upon different heuristics. There remains a lack of a unified theoretical understanding of why these methods are effective and how they are related. To this end, for the first time, we formulate core mechanisms of fourteen attribution methods, which were designed on different heuristics, into the same mathematical system, i.e., the system of Taylor interactions. Specifically, we prove that attribution scores estimated by fourteen attribution methods can all be reformulated as the weighted sum of two types of effects, i.e., independent effects of each individual input variable and interaction effects between input variables. The essential difference among the fourteen attribution methods mainly lies in the weights of allocating different effects. Based on the above findings, we propose three principles for a fair allocation of effects to evaluate the faithfulness of the fourteen attribution methods."},{"title":"Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities.","link":"http://arxiv.org/abs/2303.01508","abstract":"State-of-the-art Text-To-Speech (TTS) models are capable of producing high-quality speech. The generated speech, however, is usually neutral in emotional expression, whereas very often one would want fine-grained emotional control of words or phonemes. Although still challenging, the first TTS models have been recently proposed that are able to control voice by manually assigning emotion intensity. Unfortunately, due to the neglect of intra-class distance, the intensity differences are often unrecognizable. In this paper, we propose a fine-grained controllable emotional TTS, that considers both inter- and intra-class distances and be able to synthesize speech with recognizable intensity difference. Our subjective and objective experiments demonstrate that our model exceeds two state-of-the-art controllable TTS models for controllability, emotion expressiveness and naturalness."},{"title":"EPAM: A Predictive Energy Model for Mobile AI.","link":"http://arxiv.org/abs/2303.01509","abstract":"Artificial intelligence (AI) has enabled a new paradigm of smart applications -- changing our way of living entirely. Many of these AI-enabled applications have very stringent latency requirements, especially for applications on mobile devices (e.g., smartphones, wearable devices, and vehicles). Hence, smaller and quantized deep neural network (DNN) models are developed for mobile devices, which provide faster and more energy-efficient computation for mobile AI applications. However, how AI models consume energy in a mobile device is still unexplored. Predicting the energy consumption of these models, along with their different applications, such as vision and non-vision, requires a thorough investigation of their behavior using various processing sources. In this paper, we introduce a comprehensive study of mobile AI applications considering different DNN models and processing sources, focusing on computational resource utilization, delay, and energy consumption. We measure the latency, energy consumption, and memory usage of all the models using four processing sources through extensive experiments. We explain the challenges in such investigations and how we propose to overcome them. Our study highlights important insights, such as how mobile AI behaves in different applications (vision and non-vision) using CPU, GPU, and NNAPI. Finally, we propose a novel Gaussian process regression-based general predictive energy model based on DNN structures, computation resources, and processors, which can predict the energy for each complete application cycle irrespective of device configuration and application. This study provides crucial facts and an energy prediction mechanism to the AI research community to help bring energy efficiency to mobile AI applications."},{"title":"INO at Factify 2: Structure Coherence based Multi-Modal Fact Verification.","link":"http://arxiv.org/abs/2303.01510","abstract":"This paper describes our approach to the multi-modal fact verification (FACTIFY) challenge at AAAI2023. In recent years, with the widespread use of social media, fake news can spread rapidly and negatively impact social security. Automatic claim verification becomes more and more crucial to combat fake news. In fact verification involving multiple modal data, there should be a structural coherence between claim and document. Therefore, we proposed a structure coherence-based multi-modal fact verification scheme to classify fake news. Our structure coherence includes the following four aspects: sentence length, vocabulary similarity, semantic similarity, and image similarity. Specifically, CLIP and Sentence BERT are combined to extract text features, and ResNet50 is used to extract image features. In addition, we also extract the length of the text as well as the lexical similarity. Then the features were concatenated and passed through the random forest classifier. Finally, our weighted average F1 score has reached 0.8079, achieving 2nd place in FACTIFY2."},{"title":"Learning machines for health and beyond.","link":"http://arxiv.org/abs/2303.01513","abstract":"Machine learning techniques are effective for building predictive models because they are good at identifying patterns in large datasets. Development of a model for complex real life problems often stops at the point of publication, proof of concept or when made accessible through some mode of deployment. However, a model in the medical domain risks becoming obsolete as soon as patient demographic changes. The maintenance and monitoring of predictive models post-publication is crucial to guarantee their safe and effective long term use. As machine learning techniques are effectively trained to look for patterns in available datasets, the performance of a model for complex real life problems will not peak and remain fixed at the point of publication or even point of deployment. Rather, data changes over time, and they also changed when models are transported to new places to be used by new demography."},{"title":"Simultaneous prediction of hand gestures, handedness, and hand keypoints using thermal images.","link":"http://arxiv.org/abs/2303.01547","abstract":"Hand gesture detection is a well-explored area in computer vision with applications in various forms of Human-Computer Interactions. In this work, we propose a technique for simultaneous hand gesture classification, handedness detection, and hand keypoints localization using thermal data captured by an infrared camera. Our method uses a novel deep multi-task learning architecture that includes shared encoderdecoder layers followed by three branches dedicated for each mentioned task. We performed extensive experimental validation of our model on an in-house dataset consisting of 24 users data. The results confirm higher than 98 percent accuracy for gesture classification, handedness detection, and fingertips localization, and more than 91 percent accuracy for wrist points localization."},{"title":"Counterfactual Edits for Generative Evaluation.","link":"http://arxiv.org/abs/2303.01555","abstract":"Evaluation of generative models has been an underrepresented field despite the surge of generative architectures. Most recent models are evaluated upon rather obsolete metrics which suffer from robustness issues, while being unable to assess more aspects of visual quality, such as compositionality and logic of synthesis. At the same time, the explainability of generative models remains a limited, though important, research direction with several current attempts requiring access to the inner functionalities of generative models. Contrary to prior literature, we view generative models as a black box, and we propose a framework for the evaluation and explanation of synthesized results based on concepts instead of pixels. Our framework exploits knowledge-based counterfactual edits that underline which objects or attributes should be inserted, removed, or replaced from generated images to approach their ground truth conditioning. Moreover, global explanations produced by accumulating local edits can also reveal what concepts a model cannot generate in total. The application of our framework on various models designed for the challenging tasks of Story Visualization and Scene Synthesis verifies the power of our approach in the model-agnostic setting."},{"title":"BenchDirect: A Directed Language Model for Compiler Benchmarks.","link":"http://arxiv.org/abs/2303.01557","abstract":"The exponential increase of hardware-software complexity has made it impossible for compiler engineers to find the right optimization heuristics manually. Predictive models have been shown to find near optimal heuristics with little human effort but they are limited by a severe lack of diverse benchmarks to train on. Generative AI has been used by researchers to synthesize benchmarks into existing datasets. However, the synthetic programs are short, exceedingly simple and lacking diversity in their features.  We develop BenchPress, the first ML compiler benchmark generator that can be directed within source code feature representations. BenchPress synthesizes executable functions by infilling code that conditions on the program's left and right context. BenchPress uses active learning to introduce new benchmarks with unseen features into the dataset of Grewe's et al. CPU vs GPU heuristic, improving its acquired performance by 50%. BenchPress targets features that has been impossible for other synthesizers to reach. In 3 feature spaces, we outperform human-written code from GitHub, CLgen, CLSmith and the SRCIROR mutator in targeting the features of Rodinia benchmarks.  BenchPress steers generation with beam search over a feature-agnostic language model. We improve this with BenchDirect which utilizes a directed LM that infills programs by jointly observing source code context and the compiler features that are targeted. BenchDirect achieves up to 36% better accuracy in targeting the features of Rodinia benchmarks, it is 1.8x more likely to give an exact match and it speeds up execution time by up to 72% compared to BenchPress. Both our models produce code that is difficult to distinguish from human-written code. We conduct a Turing test which shows our models' synthetic benchmarks are labelled as 'human-written' as often as human-written code from GitHub."},{"title":"Improving GAN Training via Feature Space Shrinkage.","link":"http://arxiv.org/abs/2303.01559","abstract":"Due to the outstanding capability for data generation, Generative Adversarial Networks (GANs) have attracted considerable attention in unsupervised learning. However, training GANs is difficult, since the training distribution is dynamic for the discriminator, leading to unstable image representation. In this paper, we address the problem of training GANs from a novel perspective, \\emph{i.e.,} robust image classification. Motivated by studies on robust image representation, we propose a simple yet effective module, namely AdaptiveMix, for GANs, which shrinks the regions of training data in the image representation space of the discriminator. Considering it is intractable to directly bound feature space, we propose to construct hard samples and narrow down the feature distance between hard and easy samples. The hard samples are constructed by mixing a pair of training images. We evaluate the effectiveness of our AdaptiveMix with widely-used and state-of-the-art GAN architectures. The evaluation results demonstrate that our AdaptiveMix can facilitate the training of GANs and effectively improve the image quality of generated samples. We also show that our AdaptiveMix can be further applied to image classification and Out-Of-Distribution (OOD) detection tasks, by equipping it with state-of-the-art methods. Extensive experiments on seven publicly available datasets show that our method effectively boosts the performance of baselines. The code is publicly available at https://github.com/WentianZhang-ML/AdaptiveMix."},{"title":"Evolutionary Augmentation Policy Optimization for Self-supervised Learning.","link":"http://arxiv.org/abs/2303.01584","abstract":"Self-supervised learning (SSL) is a Machine Learning algorithm for pretraining Deep Neural Networks (DNNs) without requiring manually labeled data. The central idea of this learning technique is based on an auxiliary stage aka pretext task in which labeled data are created automatically through data augmentation and exploited for pretraining the DNN. However, the effect of each pretext task is not well studied or compared in the literature. In this paper, we study the contribution of augmentation operators on the performance of self supervised learning algorithms in a constrained settings. We propose an evolutionary search method for optimization of data augmentation pipeline in pretext tasks and measure the impact of augmentation operators in several SOTA SSL algorithms. By encoding different combination of augmentation operators in chromosomes we seek the optimal augmentation policies through an evolutionary optimization mechanism. We further introduce methods for analyzing and explaining the performance of optimized SSL algorithms. Our results indicate that our proposed method can find solutions that outperform the accuracy of classification of SSL algorithms which confirms the influence of augmentation policy choice on the overall performance of SSL algorithms. We also compare optimal SSL solutions found by our evolutionary search mechanism and show the effect of batch size in the pretext task on two visual datasets."},{"title":"Alexa Arena: A User-Centric Interactive Platform for Embodied AI.","link":"http://arxiv.org/abs/2303.01586","abstract":"We introduce Alexa Arena, a user-centric simulation platform for Embodied AI (EAI) research. Alexa Arena provides a variety of multi-room layouts and interactable objects, for the creation of human-robot interaction (HRI) missions. With user-friendly graphics and control mechanisms, Alexa Arena supports the development of gamified robotic tasks readily accessible to general human users, thus opening a new venue for high-efficiency HRI data collection and EAI system evaluation. Along with the platform, we introduce a dialog-enabled instruction-following benchmark and provide baseline results for it. We make Alexa Arena publicly available to facilitate research in building generalizable and assistive embodied agents."},{"title":"QAID: Question Answering Inspired Few-shot Intent Detection.","link":"http://arxiv.org/abs/2303.01593","abstract":"Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance."},{"title":"Hierarchical discriminative learning improves visual representations of biomedical microscopy.","link":"http://arxiv.org/abs/2303.01605","abstract":"Learning high-quality, self-supervised, visual representations is essential to advance the role of computer vision in biomedical microscopy and clinical medicine. Previous work has focused on self-supervised representation learning (SSL) methods developed for instance discrimination and applied them directly to image patches, or fields-of-view, sampled from gigapixel whole-slide images (WSIs) used for cancer diagnosis. However, this strategy is limited because it (1) assumes patches from the same patient are independent, (2) neglects the patient-slide-patch hierarchy of clinical biomedical microscopy, and (3) requires strong data augmentations that can degrade downstream performance. Importantly, sampled patches from WSIs of a patient's tumor are a diverse set of image examples that capture the same underlying cancer diagnosis. This motivated HiDisc, a data-driven method that leverages the inherent patient-slide-patch hierarchy of clinical biomedical microscopy to define a hierarchical discriminative learning task that implicitly learns features of the underlying diagnosis. HiDisc uses a self-supervised contrastive learning framework in which positive patch pairs are defined based on a common ancestry in the data hierarchy, and a unified patch, slide, and patient discriminative learning objective is used for visual SSL. We benchmark HiDisc visual representations on two vision tasks using two biomedical microscopy datasets, and demonstrate that (1) HiDisc pretraining outperforms current state-of-the-art self-supervised pretraining methods for cancer diagnosis and genetic mutation prediction, and (2) HiDisc learns high-quality visual representations using natural patch diversity without strong data augmentations."},{"title":"Deconstructing deep active inference.","link":"http://arxiv.org/abs/2303.01618","abstract":"Active inference is a theory of perception, learning and decision making, which can be applied to neuroscience, robotics, and machine learning. Recently, reasearch has been taking place to scale up this framework using Monte-Carlo tree search and deep learning. The goal of this activity is to solve more complicated tasks using deep active inference. First, we review the existing literature, then, we progresively build a deep active inference agent. For two agents, we have experimented with five definitions of the expected free energy and three different action selection strategies. According to our experiments, the models able to solve the dSprites environment are the ones that maximise rewards. Finally, we compare the similarity of the representation learned by the layers of various agents using centered kernel alignment. Importantly, the agent maximising reward and the agent minimising expected free energy learn very similar representations except for the last layer of the critic network (reflecting the difference in learning objective), and the variance layers of the transition and encoder networks. We found that the reward maximising agent is a lot more certain than the agent minimising expected free energy. This is because the agent minimising expected free energy always picks the action down, and does not gather enough data for the other actions. In contrast, the agent maximising reward, keeps on selecting the actions left and right, enabling it to successfully solve the task. The only difference between those two agents is the epistemic value, which aims to make the outputs of the transition and encoder networks as close as possible. Thus, the agent minimising expected free energy picks a single action (down), and becomes an expert at predicting the future when selecting this action. This makes the KL divergence between the output of the transition and encoder networks small."},{"title":"Non-Gaussian Uncertainty Minimization Based Control of Stochastic Nonlinear Robotic Systems.","link":"http://arxiv.org/abs/2303.01628","abstract":"In this paper, we consider the closed-loop control problem of nonlinear robotic systems in the presence of probabilistic uncertainties and disturbances. More precisely, we design a state feedback controller that minimizes deviations of the states of the system from the nominal state trajectories due to uncertainties and disturbances. Existing approaches to address the control problem of probabilistic systems are limited to particular classes of uncertainties and systems such as Gaussian uncertainties and processes and linearized systems. We present an approach that deals with nonlinear dynamics models and arbitrary known probabilistic uncertainties. We formulate the controller design problem as an optimization problem in terms of statistics of the probability distributions including moments and characteristic functions. In particular, in the provided optimization problem, we use moments and characteristic functions to propagate uncertainties throughout the nonlinear motion model of robotic systems. In order to reduce the tracking deviations, we minimize the uncertainty of the probabilistic states around the nominal trajectory by minimizing the trace and the determinant of the covariance matrix of the probabilistic states. To obtain the state feedback gains, we solve deterministic optimization problems in terms of moments, characteristic functions, and state feedback gains using off-the-shelf interior-point optimization solvers. To illustrate the performance of the proposed method, we compare our method with existing probabilistic control methods."},{"title":"Real-Time Tube-Based Non-Gaussian Risk Bounded Motion Planning for Stochastic Nonlinear Systems in Uncertain Environments via Motion Primitives.","link":"http://arxiv.org/abs/2303.01631","abstract":"We consider the motion planning problem for stochastic nonlinear systems in uncertain environments. More precisely, in this problem the robot has stochastic nonlinear dynamics and uncertain initial locations, and the environment contains multiple dynamic uncertain obstacles. Obstacles can be of arbitrary shape, can deform, and can move. All uncertainties do not necessarily have Gaussian distribution. This general setting has been considered and solved in [1]. In addition to the assumptions above, in this paper, we consider long-term tasks, where the planning method in [1] would fail, as the uncertainty of the system states grows too large over a long time horizon. Unlike [1], we present a real-time online motion planning algorithm. We build discrete-time motion primitives and their corresponding continuous-time tubes offline, so that almost all system states of each motion primitive are guaranteed to stay inside the corresponding tube. We convert probabilistic safety constraints into a set of deterministic constraints called risk contours. During online execution, we verify the safety of the tubes against deterministic risk contours using sum-of-squares (SOS) programming. The provided SOS-based method verifies the safety of the tube in the presence of uncertain obstacles without the need for uncertainty samples and time discretization in real-time. By bounding the probability the system states staying inside the tube and bounding the probability of the tube colliding with obstacles, our approach guarantees bounded probability of system states colliding with obstacles. We demonstrate our approach on several long-term robotics tasks."},{"title":"RIOT: Recursive Inertial Odometry Transformer for Localisation from Low-Cost IMU Measurements.","link":"http://arxiv.org/abs/2303.01641","abstract":"Inertial localisation is an important technique as it enables ego-motion estimation in conditions where external observers are unavailable. However, low-cost inertial sensors are inherently corrupted by bias and noise, which lead to unbound errors, making straight integration for position intractable. Traditional mathematical approaches are reliant on prior system knowledge, geometric theories and are constrained by predefined dynamics. Recent advances in deep learning, that benefit from ever-increasing volumes of data and computational power, allow for data driven solutions that offer more comprehensive understanding. Existing deep inertial odometry solutions rely on estimating the latent states, such as velocity, or are dependant on fixed sensor positions and periodic motion patterns. In this work we propose taking the traditional state estimation recursive methodology and applying it in the deep learning domain. Our approach, which incorporates the true position priors in the training process, is trained on inertial measurements and ground truth displacement data, allowing recursion and to learn both motion characteristics and systemic error bias and drift. We present two end-to-end frameworks for pose invariant deep inertial odometry that utilise self-attention to capture both spatial features and long-range dependencies in inertial data. We evaluate our approaches against a custom 2-layer Gated Recurrent Unit, trained in the same manner on the same data, and tested each approach on a number of different users, devices and activities. Each network had a sequence length weighted relative trajectory error mean $\\leq0.4594$m, highlighting the effectiveness of our learning process used in the development of the models."},{"title":"RePreM: Representation Pre-training with Masked Model for Reinforcement Learning.","link":"http://arxiv.org/abs/2303.01668","abstract":"Inspired by the recent success of sequence modeling in RL and the use of masked language model for pre-training, we propose a masked model for pre-training in RL, RePreM (Representation Pre-training with Masked Model), which trains the encoder combined with transformer blocks to predict the masked states or actions in a trajectory. RePreM is simple but effective compared to existing representation pre-training methods in RL. It avoids algorithmic sophistication (such as data augmentation or estimating multiple models) with sequence modeling and generates a representation that captures long-term dynamics well. Empirically, we demonstrate the effectiveness of RePreM in various tasks, including dynamic prediction, transfer learning, and sample-efficient RL with both value-based and actor-critic methods. Moreover, we show that RePreM scales well with dataset size, dataset quality, and the scale of the encoder, which indicates its potential towards big RL models."},{"title":"Tile Networks: Learning Optimal Geometric Layout for Whole-page Recommendation.","link":"http://arxiv.org/abs/2303.01671","abstract":"Finding optimal configurations in a geometric space is a key challenge in many technological disciplines. Current approaches either rely heavily on human domain expertise and are difficult to scale. In this paper we show it is possible to solve configuration optimization problems for whole-page recommendation using reinforcement learning. The proposed \\textit{Tile Networks} is a neural architecture that optimizes 2D geometric configurations by arranging items on proper positions. Empirical results on real dataset demonstrate its superior performance compared to traditional learning to rank approaches and recent deep models."},{"title":"Near Optimal Memory-Regret Tradeoff for Online Learning.","link":"http://arxiv.org/abs/2303.01673","abstract":"In the experts problem, on each of $T$ days, an agent needs to follow the advice of one of $n$ ``experts''. After each day, the loss associated with each expert's advice is revealed. A fundamental result in learning theory says that the agent can achieve vanishing regret, i.e. their cumulative loss is within $o(T)$ of the cumulative loss of the best-in-hindsight expert.  Can the agent perform well without sufficient space to remember all the experts? We extend a nascent line of research on this question in two directions:  $\\bullet$ We give a new algorithm against the oblivious adversary, improving over the memory-regret tradeoff obtained by [PZ23], and nearly matching the lower bound of [SWXZ22].  $\\bullet$ We also consider an adaptive adversary who can observe past experts chosen by the agent. In this setting we give both a new algorithm and a novel lower bound, proving that roughly $\\sqrt{n}$ memory is both necessary and sufficient for obtaining $o(T)$ regret."},{"title":"BO-Muse: A human expert and AI teaming framework for accelerated experimental design.","link":"http://arxiv.org/abs/2303.01684","abstract":"In this paper we introduce BO-Muse, a new approach to human-AI teaming for the optimization of expensive black-box functions. Inspired by the intrinsic difficulty of extracting expert knowledge and distilling it back into AI models and by observations of human behaviour in real-world experimental design, our algorithm lets the human expert take the lead in the experimental process. The human expert can use their domain expertise to its full potential, while the AI plays the role of a muse, injecting novelty and searching for areas of weakness to break the human out of over-exploitation induced by cognitive entrenchment. With mild assumptions, we show that our algorithm converges sub-linearly, at a rate faster than the AI or human alone. We validate our algorithm using synthetic data and with human experts performing real-world experiments."},{"title":"Enhancing Fairness in AI-based Travel Demand Forecasting Models.","link":"http://arxiv.org/abs/2303.01692","abstract":"Artificial Intelligence (AI) and machine learning have been increasingly adopted for forecasting real-time travel demand. These AI-based travel demand forecasting models, though generate highly-accurate predictions, may produce prediction biases and thus raise fairness issues. Using such models for decision-making, we may develop transportation policies that could exacerbate social inequalities. However, limited studies have been focused on addressing the fairness issues of AI-based travel demand forecasting models. Therefore, in this study, we propose a novel methodology to develop fairness-aware travel demand forecasting models, which are highly accurate and fair. Specifically, we add a fairness regularization term, i.e., the correlation between prediction accuracy and the protected attribute such as race or income, into the loss function of the travel demand forecasting model. We include an interactive weight coefficient to both accuracy loss term and fairness loss term. The travel demand forecasting models can thus simultaneously account for prediction accuracy and fairness. An empirical analysis is conducted using real-world ridesourcing-trip data in Chicago. Results show that our proposed methodology effectively addresses the accuracy-fairness trade-off. It can significantly enhance fairness for multiple protected attributes (i.e., race, education, age and income) by only sacrificing a small accuracy drop. This study provides transportation professionals a new type of decision-support tool to achieve fair and accurate travel demand forecasting."},{"title":"NovPhy: A Testbed for Physical Reasoning in Open-world Environments.","link":"http://arxiv.org/abs/2303.01711","abstract":"Due to the emergence of AI systems that interact with the physical environment, there is an increased interest in incorporating physical reasoning capabilities into those AI systems. But is it enough to only have physical reasoning capabilities to operate in a real physical environment? In the real world, we constantly face novel situations we have not encountered before. As humans, we are competent at successfully adapting to those situations. Similarly, an agent needs to have the ability to function under the impact of novelties in order to properly operate in an open-world physical environment. To facilitate the development of such AI systems, we propose a new testbed, NovPhy, that requires an agent to reason about physical scenarios in the presence of novelties and take actions accordingly. The testbed consists of tasks that require agents to detect and adapt to novelties in physical scenarios. To create tasks in the testbed, we develop eight novelties representing a diverse novelty space and apply them to five commonly encountered scenarios in a physical environment. According to our testbed design, we evaluate two capabilities of an agent: the performance on a novelty when it is applied to different physical scenarios and the performance on a physical scenario when different novelties are applied to it. We conduct a thorough evaluation with human players, learning agents, and heuristic agents. Our evaluation shows that humans' performance is far beyond the agents' performance. Some agents, even with good normal task performance, perform significantly worse when there is a novelty, and the agents that can adapt to novelties typically adapt slower than humans. We promote the development of intelligent agents capable of performing at the human level or above when operating in open-world physical environments. Testbed website: https://github.com/phy-q/novphy"},{"title":"AI-Empowered Hybrid MIMO Beamforming.","link":"http://arxiv.org/abs/2303.01723","abstract":"Hybrid multiple-input multiple-output (MIMO) is an attractive technology for realizing extreme massive MIMO systems envisioned for future wireless communications in a scalable and power-efficient manner. However, the fact that hybrid MIMO systems implement part of their beamforming in analog and part in digital makes the optimization of their beampattern notably more challenging compared with conventional fully digital MIMO. Consequently, recent years have witnessed a growing interest in using data-aided artificial intelligence (AI) tools for hybrid beamforming design. This article reviews candidate strategies to leverage data to improve real-time hybrid beamforming design. We discuss the architectural constraints and characterize the core challenges associated with hybrid beamforming optimization. We then present how these challenges are treated via conventional optimization, and identify different AI-aided design approaches. These can be roughly divided into purely data-driven deep learning models and different forms of deep unfolding techniques for combining AI with classical optimization.We provide a systematic comparative study between existing approaches including both numerical evaluations and qualitative measures. We conclude by presenting future research opportunities associated with the incorporation of AI in hybrid MIMO systems."},{"title":"Guarded Policy Optimization with Imperfect Online Demonstrations.","link":"http://arxiv.org/abs/2303.01728","abstract":"The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher's own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C."},{"title":"Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning.","link":"http://arxiv.org/abs/2303.01772","abstract":"Energy markets can provide incentives for undesired behavior of market participants. Multi-agent Reinforcement learning (MARL) is a promising new approach to determine the expected behavior of energy market participants. However, reinforcement learning requires many interactions with the system to converge, and the power system environment often consists of extensive computations, e.g., optimal power flow (OPF) calculation for market clearing. To tackle this complexity, we provide a model of the energy market to a basic MARL algorithm, in form of a learned OPF approximation and explicit market rules. The learned OPF surrogate model makes an explicit solving of the OPF completely unnecessary. Our experiments demonstrate that the model additionally reduces training time by about one order of magnitude, but at the cost of a slightly worse approximation of the Nash equilibrium. Potential applications of our method are market design, more realistic modeling of market participants, and analysis of manipulative behavior."},{"title":"Exploiting Language Relatedness in Machine Translation Through Domain Adaptation Techniques.","link":"http://arxiv.org/abs/2303.01793","abstract":"One of the significant challenges of Machine Translation (MT) is the scarcity of large amounts of data, mainly parallel sentence aligned corpora. If the evaluation is as rigorous as resource-rich languages, both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) can produce good results with such large amounts of data. However, it is challenging to improve the quality of MT output for low resource languages, especially in NMT and SMT. In order to tackle the challenges faced by MT, we present a novel approach of using a scaled similarity score of sentences, especially for related languages based on a 5-gram KenLM language model with Kneser-ney smoothing technique for filtering in-domain data from out-of-domain corpora that boost the translation quality of MT. Furthermore, we employ other domain adaptation techniques such as multi-domain, fine-tuning and iterative back-translation approach to compare our novel approach on the Hindi-Nepali language pair for NMT and SMT. Our approach succeeds in increasing ~2 BLEU point on multi-domain approach, ~3 BLEU point on fine-tuning for NMT and ~2 BLEU point on iterative back-translation approach."},{"title":"Team Hitachi at SemEval-2023 Task 3: Exploring Cross-lingual Multi-task Strategies for Genre and Framing Detection in Online News.","link":"http://arxiv.org/abs/2303.01794","abstract":"This paper explains the participation of team Hitachi to SemEval-2023 Task 3 \"Detecting the genre, the framing, and the persuasion techniques in online news in a multi-lingual setup.\" Based on the multilingual, multi-task nature of the task and the setting that training data is limited, we investigated different strategies for training the pretrained language models under low resource settings. Through extensive experiments, we found that (a) cross-lingual/multi-task training, and (b) collecting an external balanced dataset, can benefit the genre and framing detection. We constructed ensemble models from the results and achieved the highest macro-averaged F1 scores in Italian and Russian genre categorization subtasks."},{"title":"Are All Point Clouds Suitable for Completion? Weakly Supervised Quality Evaluation Network for Point Cloud Completion.","link":"http://arxiv.org/abs/2303.01804","abstract":"In the practical application of point cloud completion tasks, real data quality is usually much worse than the CAD datasets used for training. A small amount of noisy data will usually significantly impact the overall system's accuracy. In this paper, we propose a quality evaluation network to score the point clouds and help judge the quality of the point cloud before applying the completion model. We believe our scoring method can help researchers select more appropriate point clouds for subsequent completion and reconstruction and avoid manual parameter adjustment. Moreover, our evaluation model is fast and straightforward and can be directly inserted into any model's training or use process to facilitate the automatic selection and post-processing of point clouds. We propose a complete dataset construction and model evaluation method based on ShapeNet. We verify our network using detection and flow estimation tasks on KITTI, a real-world dataset for autonomous driving. The experimental results show that our model can effectively distinguish the quality of point clouds and help in practical tasks."},{"title":"Word-As-Image for Semantic Typography.","link":"http://arxiv.org/abs/2303.01818","abstract":"A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. This task is highly challenging as it requires semantic understanding of the word and a creative idea of where and how to depict these semantics in a visually pleasing and legible manner. We rely on the remarkable ability of recent large pretrained language-vision models to distill textual concepts visually. We target simple, concise, black-and-white designs that convey the semantics clearly. We deliberately do not change the color or texture of the letters and do not use embellishments. Our method optimizes the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model. We incorporate additional loss terms to ensure the legibility of the text and the preservation of the style of the font. We show high quality and engaging results on numerous examples and compare to alternative techniques."},{"title":"TopSpark: A Timestep Optimization Methodology for Energy-Efficient Spiking Neural Networks on Autonomous Mobile Agents.","link":"http://arxiv.org/abs/2303.01826","abstract":"Autonomous mobile agents require low-power/energy-efficient machine learning (ML) algorithms to complete their ML-based tasks while adapting to diverse environments, as mobile agents are usually powered by batteries. These requirements can be fulfilled by Spiking Neural Networks (SNNs) as they offer low power/energy processing due to their sparse computations and efficient online learning with bio-inspired learning mechanisms for adapting to different environments. Recent works studied that the energy consumption of SNNs can be optimized by reducing the computation time of each neuron for processing a sequence of spikes (timestep). However, state-of-the-art techniques rely on intensive design searches to determine fixed timestep settings for only inference, thereby hindering SNNs from achieving further energy efficiency gains in both training and inference. These techniques also restrict SNNs from performing efficient online learning at run time. Toward this, we propose TopSpark, a novel methodology that leverages adaptive timestep reduction to enable energy-efficient SNN processing in both training and inference, while keeping its accuracy close to the accuracy of SNNs without timestep reduction. The ideas of TopSpark include analyzing the impact of different timesteps on the accuracy; identifying neuron parameters that have a significant impact on accuracy in different timesteps; employing parameter enhancements that make SNNs effectively perform learning and inference using less spiking activity; and developing a strategy to trade-off accuracy, latency, and energy to meet the design requirements. The results show that, TopSpark saves the SNN latency by 3.9x as well as energy consumption by 3.5x for training and 3.3x for inference on average, across different network sizes, learning rules, and workloads, while maintaining the accuracy within 2% of SNNs without timestep reduction."},{"title":"Anamnesic Neural Differential Equations with Orthogonal Polynomial Projections.","link":"http://arxiv.org/abs/2303.01841","abstract":"Neural ordinary differential equations (Neural ODEs) are an effective framework for learning dynamical systems from irregularly sampled time series data. These models provide a continuous-time latent representation of the underlying dynamical system where new observations at arbitrary time points can be used to update the latent representation of the dynamical system. Existing parameterizations for the dynamics functions of Neural ODEs limit the ability of the model to retain global information about the time series; specifically, a piece-wise integration of the latent process between observations can result in a loss of memory on the dynamic patterns of previously observed data points. We propose PolyODE, a Neural ODE that models the latent continuous-time process as a projection onto a basis of orthogonal polynomials. This formulation enforces long-range memory and preserves a global representation of the underlying dynamical system. Our construction is backed by favourable theoretical guarantees and in a series of experiments, we demonstrate that it outperforms previous works in the reconstruction of past and future data, and in downstream prediction tasks."},{"title":"Learning Permutation-Invariant Embeddings for Description Logic Concepts.","link":"http://arxiv.org/abs/2303.01844","abstract":"Concept learning deals with learning description logic concepts from a background knowledge and input examples. The goal is to learn a concept that covers all positive examples, while not covering any negative examples. This non-trivial task is often formulated as a search problem within an infinite quasi-ordered concept space. Although state-of-the-art models have been successfully applied to tackle this problem, their large-scale applications have been severely hindered due to their excessive exploration incurring impractical runtimes. Here, we propose a remedy for this limitation. We reformulate the learning problem as a multi-label classification problem and propose a neural embedding model (NERO) that learns permutation-invariant embeddings for sets of examples tailored towards predicting $F_1$ scores of pre-selected description logic concepts. By ranking such concepts in descending order of predicted scores, a possible goal concept can be detected within few retrieval operations, i.e., no excessive exploration. Importantly, top-ranked concepts can be used to start the search procedure of state-of-the-art symbolic models in multiple advantageous regions of a concept space, rather than starting it in the most general concept $\\top$. Our experiments on 5 benchmark datasets with 770 learning problems firmly suggest that NERO significantly (p-value &lt;1%) outperforms the state-of-the-art models in terms of $F_1$ score, the number of explored concepts, and the total runtime. We provide an open-source implementation of our approach."},{"title":"LBCIM: Loyalty Based Competitive Influence Maximization with epsilon-greedy MCTS strategy.","link":"http://arxiv.org/abs/2303.01850","abstract":"Competitive influence maximization has been studied for several years, and various frameworks have been proposed to model different aspects of information diffusion under the competitive environment. This work presents a new gameboard for two competing parties with some new features representing loyalty in social networks and reflecting the attitude of not completely being loyal to a party when the opponent offers better suggestions. This behavior can be observed in most political occasions where each party tries to attract people by making better suggestions than the opponent and even seeks to impress the fans of the opposition party to change their minds. In order to identify the best move in each step of the game framework, an improved Monte Carlo tree search is developed, which uses some predefined heuristics to apply them on the simulation step of the algorithm and takes advantage of them to search among child nodes of the current state and pick the best one using an epsilon-greedy way instead of choosing them at random. Experimental results on synthetic and real datasets indicate the outperforming of the proposed strategy against some well-known and benchmark strategies like general MCTS, minimax algorithm with alpha-beta pruning, random nodes, nodes with maximum threshold and nodes with minimum threshold."},{"title":"POPGym: Benchmarking Partially Observable Reinforcement Learning.","link":"http://arxiv.org/abs/2303.01859","abstract":"Real world applications of Reinforcement Learning (RL) are often partially observable, thus requiring memory. Despite this, partial observability is still largely ignored by contemporary RL benchmarks and libraries. We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 15 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines -- the most in a single RL library. Existing partially observable benchmarks tend to fixate on 3D visual navigation, which is computationally expensive and only one type of POMDP. In contrast, POPGym environments are diverse, produce smaller observations, use less memory, and often converge within two hours of training on a consumer-grade GPU. We implement our high-level memory API and memory baselines on top of the popular RLlib framework, providing plug-and-play compatibility with various training algorithms, exploration strategies, and distributed training paradigms. Using POPGym, we execute the largest comparison across RL memory models to date. POPGym is available at https://github.com/proroklab/popgym."},{"title":"Rule-based Out-Of-Distribution Detection.","link":"http://arxiv.org/abs/2303.01860","abstract":"Out-of-distribution detection is one of the most critical issue in the deployment of machine learning. The data analyst must assure that data in operation should be compliant with the training phase as well as understand if the environment has changed in a way that autonomous decisions would not be safe anymore. The method of the paper is based on eXplainable Artificial Intelligence (XAI); it takes into account different metrics to identify any resemblance between in-distribution and out of, as seen by the XAI model. The approach is non-parametric and distributional assumption free. The validation over complex scenarios (predictive maintenance, vehicle platooning, covert channels in cybersecurity) corroborates both precision in detection and evaluation of training-operation conditions proximity."},{"title":"Bespoke: A Block-Level Neural Network Optimization Framework for Low-Cost Deployment.","link":"http://arxiv.org/abs/2303.01913","abstract":"As deep learning models become popular, there is a lot of need for deploying them to diverse device environments. Because it is costly to develop and optimize a neural network for every single environment, there is a line of research to search neural networks for multiple target environments efficiently. However, existing works for such a situation still suffer from requiring many GPUs and expensive costs. Motivated by this, we propose a novel neural network optimization framework named Bespoke for low-cost deployment. Our framework searches for a lightweight model by replacing parts of an original model with randomly selected alternatives, each of which comes from a pretrained neural network or the original model. In the practical sense, Bespoke has two significant merits. One is that it requires near zero cost for designing the search space of neural networks. The other merit is that it exploits the sub-networks of public pretrained neural networks, so the total cost is minimal compared to the existing works. We conduct experiments exploring Bespoke's the merits, and the results show that it finds efficient models for multiple targets with meager cost."},{"title":"FairShap: A Data Re-weighting Approach for Algorithmic Fairness based on Shapley Values.","link":"http://arxiv.org/abs/2303.01928","abstract":"In this paper, we propose FairShap, a novel and interpretable pre-processing (re-weighting) method for fair algorithmic decision-making through data valuation. FairShap is based on the Shapley Value, a well-known mathematical framework from game theory to achieve a fair allocation of resources. Our approach is easily interpretable, as it measures the contribution of each training data point to a predefined fairness metric. We empirically validate FairShap on several state-of-the-art datasets of different nature, with different training scenarios and models. The proposed approach outperforms other methods, yielding significantly fairer models with similar levels of accuracy. In addition, we illustrate FairShap's interpretability by means of histograms and latent space visualizations. We believe this work represents a promising direction in interpretable, model-agnostic approaches to algorithmic fairness."},{"title":"A toolkit of dilemmas: Beyond debiasing and fairness formulas for responsible AI/ML.","link":"http://arxiv.org/abs/2303.01930","abstract":"Approaches to fair and ethical AI have recently fell under the scrutiny of the emerging, chiefly qualitative, field of critical data studies, placing emphasis on the lack of sensitivity to context and complex social phenomena of such interventions. We employ some of these lessons to introduce a tripartite decision-making toolkit, informed by dilemmas encountered in the pursuit of responsible AI/ML. These are: (a) the opportunity dilemma between the availability of data shaping problem statements vs problem statements shaping data; (b) the trade-off between scalability and contextualizability (too much data versus too specific data); and (c) the epistemic positioning between the pragmatic technical objectivism and the reflexive relativism in acknowledging the social. This paper advocates for a situated reasoning and creative engagement with the dilemmas surrounding responsible algorithmic/data-driven systems, and going beyond the formulaic bias elimination and ethics operationalization narratives found in the fair-AI literature."},{"title":"Deep Neural Network Architecture Search for Accurate Visual Pose Estimation aboard Nano-UAVs.","link":"http://arxiv.org/abs/2303.01931","abstract":"Miniaturized autonomous unmanned aerial vehicles (UAVs) are an emerging and trending topic. With their form factor as big as the palm of one hand, they can reach spots otherwise inaccessible to bigger robots and safely operate in human surroundings. The simple electronics aboard such robots (sub-100mW) make them particularly cheap and attractive but pose significant challenges in enabling onboard sophisticated intelligence. In this work, we leverage a novel neural architecture search (NAS) technique to automatically identify several Pareto-optimal convolutional neural networks (CNNs) for a visual pose estimation task. Our work demonstrates how real-life and field-tested robotics applications can concretely leverage NAS technologies to automatically and efficiently optimize CNNs for the specific hardware constraints of small UAVs. We deploy several NAS-optimized CNNs and run them in closed-loop aboard a 27-g Crazyflie nano-UAV equipped with a parallel ultra-low power System-on-Chip. Our results improve the State-of-the-Art by reducing the in-field control error of 32% while achieving a real-time onboard inference-rate of ~10Hz@10mW and ~50Hz@90mW."},{"title":"CONTAIN: A Community-based Algorithm for Network Immunization.","link":"http://arxiv.org/abs/2303.01934","abstract":"Within the network analysis field, network immunization refers to the task of protecting a network from some arbitrary diffusion that tries to infect it. In this article, we consider the spread of harmful content in social networks, and we propose CONTAIN, a novel COmmuNiTy-based Algorithm for network ImmuNization. Our solution uses the network information to (1) detect harmful content spreaders, and (2) generate partitions and rank them for immunization using the subgraphs induced by each spreader, i.e., employing CONTAIN. The experimental results obtained on real-world datasets show that CONTAIN outperforms state-of-the-art solutions, i.e., NetShield and SparseShield, by immunizing the network in fewer iterations, thus, converging significantly faster than the state-of-the-art algorithms."},{"title":"Multi-Agent Adversarial Training Using Diffusion Learning.","link":"http://arxiv.org/abs/2303.01936","abstract":"This work focuses on adversarial learning over graphs. We propose a general adversarial training framework for multi-agent systems using diffusion learning. We analyze the convergence properties of the proposed scheme for convex optimization problems, and illustrate its enhanced robustness to adversarial attacks."},{"title":"Ultra-low Power Deep Learning-based Monocular Relative Localization Onboard Nano-quadrotors.","link":"http://arxiv.org/abs/2303.01940","abstract":"Precise relative localization is a crucial functional block for swarm robotics. This work presents a novel autonomous end-to-end system that addresses the monocular relative localization, through deep neural networks (DNNs), of two peer nano-drones, i.e., sub-40g of weight and sub-100mW processing power. To cope with the ultra-constrained nano-drone platform, we propose a vertically-integrated framework, from the dataset collection to the final in-field deployment, including dataset augmentation, quantization, and system optimizations. Experimental results show that our DNN can precisely localize a 10cm-size target nano-drone by employing only low-resolution monochrome images, up to ~2m distance. On a disjoint testing dataset our model yields a mean R2 score of 0.42 and a root mean square error of 18cm, which results in a mean in-field prediction error of 15cm and in a closed-loop control error of 17cm, over a ~60s-flight test. Ultimately, the proposed system improves the State-of-the-Art by showing long-endurance tracking performance (up to 2min continuous tracking), generalization capabilities being deployed in a never-seen-before environment, and requiring a minimal power consumption of 95mW for an onboard real-time inference-rate of 48Hz."},{"title":"Synthetic Data Generator for Adaptive Interventions in Global Health.","link":"http://arxiv.org/abs/2303.01954","abstract":"Artificial Intelligence and digital health have the potential to transform global health. However, having access to representative data to test and validate algorithms in realistic production environments is essential. We introduce HealthSyn, an open-source synthetic data generator of user behavior for testing reinforcement learning algorithms in the context of mobile health interventions. The generator utilizes Markov processes to generate diverse user actions, with individual user behavioral patterns that can change in reaction to personalized interventions (i.e., reminders, recommendations, and incentives). These actions are translated into actual logs using an ML-purposed data schema specific to the mobile health application functionality included with HealthKit, and open-source SDK. The logs can be fed to pipelines to obtain user metrics. The generated data, which is based on real-world behaviors and simulation techniques, can be used to develop, test, and evaluate, both ML algorithms in research and end-to-end operational RL-based intervention delivery frameworks."},{"title":"PointCert: Point Cloud Classification with Deterministic Certified Robustness Guarantees.","link":"http://arxiv.org/abs/2303.01959","abstract":"Point cloud classification is an essential component in many security-critical applications such as autonomous driving and augmented reality. However, point cloud classifiers are vulnerable to adversarially perturbed point clouds. Existing certified defenses against adversarial point clouds suffer from a key limitation: their certified robustness guarantees are probabilistic, i.e., they produce an incorrect certified robustness guarantee with some probability. In this work, we propose a general framework, namely PointCert, that can transform an arbitrary point cloud classifier to be certifiably robust against adversarial point clouds with deterministic guarantees. PointCert certifiably predicts the same label for a point cloud when the number of arbitrarily added, deleted, and/or modified points is less than a threshold. Moreover, we propose multiple methods to optimize the certified robustness guarantees of PointCert in three application scenarios. We systematically evaluate PointCert on ModelNet and ScanObjectNN benchmark datasets. Our results show that PointCert substantially outperforms state-of-the-art certified defenses even though their robustness guarantees are probabilistic."},{"title":"Less is More: Mitigate Spurious Correlations for Open-Domain Dialogue Response Generation Models by Causal Discovery.","link":"http://arxiv.org/abs/2303.01962","abstract":"In this paper, we conduct the first study on spurious correlations for open-domain response generation models based on a corpus CGDIALOG curated in our work. The cur rent models indeed suffer from spurious correlations and have a tendency of generating irrelevant and generic responses. Inspired by causal discovery algorithms, we propose a novel model-agnostic method for training and inference of response generation model using a conditional independence classifier. The classifier is trained by a constrained self-training method, coined CONSTRAIN, to overcome data scarcity. The experimental results based on both human and automatic evaluation show that our method significantly outperforms the competitive baselines in terms of relevance, informativeness, and fluency."},{"title":"Hybrid Approach for Solving Real-World Bin Packing Problem Instances Using Quantum Annealers.","link":"http://arxiv.org/abs/2303.01977","abstract":"Efficient packing of items into bins is a common daily task. Known as Bin Packing Problem, it has been intensively studied in the field of artificial intelligence, thanks to the wide interest from industry and logistics. Since decades, many variants have been proposed, with the three-dimensional Bin Packing Problem as the closest one to real-world use cases. We introduce a hybrid quantum-classical framework for solving real-world three-dimensional Bin Packing Problems (Q4RealBPP), considering different realistic characteristics, such as: i) package and bin dimensions, ii) overweight restrictions, iii) affinities among item categories and iv) preferences for item ordering. Q4RealBPP permits the solving of real-world oriented instances of 3dBPP, contemplating restrictions well appreciated by industrial and logistics sectors."},{"title":"Auto-weighted Multi-view Clustering for Large-scale Data.","link":"http://arxiv.org/abs/2303.01983","abstract":"Multi-view clustering has gained broad attention owing to its capacity to exploit complementary information across multiple data views. Although existing methods demonstrate delightful clustering performance, most of them are of high time complexity and cannot handle large-scale data. Matrix factorization-based models are a representative of solving this problem. However, they assume that the views share a dimension-fixed consensus coefficient matrix and view-specific base matrices, limiting their representability. Moreover, a series of large-scale algorithms that bear one or more hyperparameters are impractical in real-world applications. To address the two issues, we propose an auto-weighted multi-view clustering (AWMVC) algorithm. Specifically, AWMVC first learns coefficient matrices from corresponding base matrices of different dimensions, then fuses them to obtain an optimal consensus matrix. By mapping original features into distinctive low-dimensional spaces, we can attain more comprehensive knowledge, thus obtaining better clustering results. Moreover, we design a six-step alternative optimization algorithm proven to be convergent theoretically. Also, AWMVC shows excellent performance on various benchmark datasets compared with existing ones. The code of AWMVC is publicly available at https://github.com/wanxinhang/AAAI-2023-AWMVC."},{"title":"Spatiotemporal modeling of grip forces captures proficiency in manual robot control.","link":"http://arxiv.org/abs/2303.01995","abstract":"This paper builds on our previous work by exploiting Artificial Intelligence to predict individual grip force variability in manual robot control. Grip forces were recorded from various loci in the dominant and non dominant hands of individuals by means of wearable wireless sensor technology. Statistical analyses bring to the fore skill specific temporal variations in thousands of grip forces of a complete novice and a highly proficient expert in manual robot control. A brain inspired neural network model that uses the output metric of a Self Organizing Map with unsupervised winner take all learning was run on the sensor output from both hands of each user. The neural network metric expresses the difference between an input representation and its model representation at any given moment in time t and reliably captures the differences between novice and expert performance in terms of grip force variability.Functionally motivated spatiotemporal analysis of individual average grip forces, computed for time windows of constant size in the output of a restricted amount of task-relevant sensors in the dominant (preferred) hand, reveal finger-specific synergies reflecting robotic task skill. The analyses lead the way towards grip force monitoring in real time to permit tracking task skill evolution in trainees, or identify individual proficiency levels in human robot interaction in environmental contexts of high sensory uncertainty. Parsimonious Artificial Intelligence (AI) assistance will contribute to the outcome of new types of surgery, in particular single-port approaches such as NOTES (Natural Orifice Transluminal Endoscopic Surgery) and SILS (Single Incision Laparoscopic Surgery)."},{"title":"MLTEing Models: Negotiating, Evaluating, and Documenting Model and System Qualities.","link":"http://arxiv.org/abs/2303.01998","abstract":"Many organizations seek to ensure that machine learning (ML) and artificial intelligence (AI) systems work as intended in production but currently do not have a cohesive methodology in place to do so. To fill this gap, we propose MLTE (Machine Learning Test and Evaluation, colloquially referred to as \"melt\"), a framework and implementation to evaluate ML models and systems. The framework compiles state-of-the-art evaluation techniques into an organizational process for interdisciplinary teams, including model developers, software engineers, system owners, and other stakeholders. MLTE tooling supports this process by providing a domain-specific language that teams can use to express model requirements, an infrastructure to define, generate, and collect ML evaluation metrics, and the means to communicate results."},{"title":"Calibration of Quantum Decision Theory: Aversion to Large Losses and Predictability of Probabilistic Choices.","link":"http://arxiv.org/abs/2303.02028","abstract":"We present the first calibration of quantum decision theory (QDT) to a dataset of binary risky choice. We quantitatively account for the fraction of choice reversals between two repetitions of the experiment, using a probabilistic choice formulation in the simplest form without model assumption or adjustable parameters. The prediction of choice reversal is then refined by introducing heterogeneity between decision makers through their differentiation into two groups: ``majoritarian'' and ``contrarian'' (in proportion 3:1). This supports the first fundamental tenet of QDT, which models choice as an inherent probabilistic process, where the probability of a prospect can be expressed as the sum of its utility and attraction factors. We propose to parameterise the utility factor with a stochastic version of cumulative prospect theory (logit-CPT), and the attraction factor with a constant absolute risk aversion (CARA) function. For this dataset, and penalising the larger number of QDT parameters via the Wilks test of nested hypotheses, the QDT model is found to perform significantly better than logit-CPT at both the aggregate and individual levels, and for all considered fit criteria for the first experiment iteration and for predictions (second ``out-of-sample'' iteration). The distinctive QDT effect captured by the attraction factor is mostly appreciable (i.e., most relevant and strongest in amplitude) for prospects with big losses. Our quantitative analysis of the experimental results supports the existence of an intrinsic limit of predictability, which is associated with the inherent probabilistic nature of choice. The results of the paper can find applications both in the prediction of choice of human decision makers as well as for organizing the operation of artificial intelligence."},{"title":"Topic Modeling Based on Two-Step Flow Theory: Application to Tweets about Bitcoin.","link":"http://arxiv.org/abs/2303.02032","abstract":"Digital cryptocurrencies such as Bitcoin have exploded in recent years in both popularity and value. By their novelty, cryptocurrencies tend to be both volatile and highly speculative. The capricious nature of these coins is helped facilitated by social media networks such as Twitter. However, not everyone's opinion matters equally, with most posts garnering little to no attention. Additionally, the majority of tweets are retweeted from popular posts. We must determine whose opinion matters and the difference between influential and non-influential users. This study separates these two groups and analyzes the differences between them. It uses Hypertext-induced Topic Selection (HITS) algorithm, which segregates the dataset based on influence. Topic modeling is then employed to uncover differences in each group's speech types and what group may best represent the entire community. We found differences in language and interest between these two groups regarding Bitcoin and that the opinion leaders of Twitter are not aligned with the majority of users. There were 2559 opinion leaders (0.72% of users) who accounted for 80% of the authority and the majority (99.28%) users for the remaining 20% out of a total of 355,139 users."},{"title":"Linear CNNs Discover the Statistical Structure of the Dataset Using Only the Most Dominant Frequencies.","link":"http://arxiv.org/abs/2303.02034","abstract":"Our theoretical understanding of the inner workings of general convolutional neural networks (CNN) is limited. We here present a new stepping stone towards such understanding in the form of a theory of learning in linear CNNs. By analyzing the gradient descent equations, we discover that using convolutions leads to a mismatch between the dataset structure and the network structure. We show that linear CNNs discover the statistical structure of the dataset with non-linear, stage-like transitions, and that the speed of discovery changes depending on this structural mismatch. Moreover, we find that the mismatch lies at the heart of what we call the 'dominant frequency bias', where linear CNNs arrive at these discoveries using only the dominant frequencies of the different structural parts present in the dataset. Our findings can help explain several characteristics of general CNNs, such as their shortcut learning and their tendency to rely on texture instead of shape."},{"title":"Uncertainty Estimation by Fisher Information-based Evidential Deep Learning.","link":"http://arxiv.org/abs/2303.02045","abstract":"Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, \\textit{Fisher Information-based Evidential Deep Learning} ($\\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focus on the representation learning of uncertain classes. The generalization ability of our network is further improved by optimizing the PAC-Bayesian bound. As demonstrated empirically, our proposed method consistently outperforms traditional EDL-related algorithms in multiple uncertainty estimation tasks, especially in the more challenging few-shot classification settings."},{"title":"Graph-based Global Robot Localization Informing Situational Graphs with Architectural Graphs.","link":"http://arxiv.org/abs/2303.02076","abstract":"In this paper, we propose a solution for legged robot localization using architectural plans. Our specific contributions towards this goal are several. Firstly, we develop a method for converting the plan of a building into what we denote as an architectural graph (A-Graph). When the robot starts moving in an environment, we assume it has no knowledge about it, and it estimates an online situational graph representation (S-Graph) of its surroundings. We develop a novel graph-to-graph matching method, in order to relate the S-Graph estimated online from the robot sensors and the A-Graph extracted from the building plans. Note the challenge in this, as the S-Graph may show a partial view of the full A-Graph, their nodes are heterogeneous and their reference frames are different. After the matching, both graphs are aligned and merged, resulting in what we denote as an informed Situational Graph (iS-Graph), with which we achieve global robot localization and exploitation of prior knowledge from the building plans. Our experiments show that our pipeline shows a higher robustness and a significantly lower pose error than several LiDAR localization baselines."},{"title":"Eventual Discounting Temporal Logic Counterfactual Experience Replay.","link":"http://arxiv.org/abs/2303.02135","abstract":"Linear temporal logic (LTL) offers a simplified way of specifying tasks for policy optimization that may otherwise be difficult to describe with scalar reward functions. However, the standard RL framework can be too myopic to find maximally LTL satisfying policies. This paper makes two contributions. First, we develop a new value-function based proxy, using a technique we call eventual discounting, under which one can find policies that satisfy the LTL specification with highest achievable probability. Second, we develop a new experience replay method for generating off-policy data from on-policy rollouts via counterfactual reasoning on different ways of satisfying the LTL specification. Our experiments, conducted in both discrete and continuous state-action spaces, confirm the effectiveness of our counterfactual experience replay approach."},{"title":"Data Association Aware POMDP Planning with Hypothesis Pruning Performance Guarantees.","link":"http://arxiv.org/abs/2303.02139","abstract":"Autonomous agents that operate in the real world must often deal with partial observability, which is commonly modeled as partially observable Markov decision processes (POMDPs). However, traditional POMDP models rely on the assumption of complete knowledge of the observation source, known as fully observable data association. To address this limitation, we propose a planning algorithm that maintains multiple data association hypotheses, represented as a belief mixture, where each component corresponds to a different data association hypothesis. However, this method can lead to an exponential growth in the number of hypotheses, resulting in significant computational overhead. To overcome this challenge, we introduce a pruning-based approach for planning with ambiguous data associations. Our key contribution is to derive bounds between the value function based on the complete set of hypotheses and the value function based on a pruned-subset of the hypotheses, enabling us to establish a trade-off between computational efficiency and performance. We demonstrate how these bounds can both be used to certify any pruning heuristic in retrospect and propose a novel approach to determine which hypotheses to prune in order to ensure a predefined limit on the loss. We evaluate our approach in simulated environments and demonstrate its efficacy in handling multi-modal belief hypotheses with ambiguous data associations."},{"title":"Learning on heterogeneous graphs using high-order relations.","link":"http://arxiv.org/abs/2103.15532","abstract":"A heterogeneous graph consists of different vertices and edges types. Learning on heterogeneous graphs typically employs meta-paths to deal with the heterogeneity by reducing the graph to a homogeneous network, guide random walks or capture semantics. These methods are however sensitive to the choice of meta-paths, with suboptimal paths leading to poor performance. In this paper, we propose an approach for learning on heterogeneous graphs without using meta-paths. Specifically, we decompose a heterogeneous graph into different homogeneous relation-type graphs, which are then combined to create higher-order relation-type representations. These representations preserve the heterogeneity of edges and retain their edge directions while capturing the interaction of different vertex types multiple hops apart. This is then complemented with attention mechanisms to distinguish the importance of the relation-type based neighbors and the relation-types themselves. Experiments demonstrate that our model generally outperforms other state-of-the-art baselines in the vertex classification task on three commonly studied heterogeneous graph datasets."},{"title":"Sparse Bayesian Optimization.","link":"http://arxiv.org/abs/2203.01900","abstract":"Bayesian optimization (BO) is a powerful approach to sample-efficient optimization of black-box objective functions. However, the application of BO to areas such as recommendation systems often requires taking the interpretability and simplicity of the configurations into consideration, a setting that has not been previously studied in the BO literature. To make BO useful for this setting, we present several regularization-based approaches that allow us to discover sparse and more interpretable configurations. We propose a novel differentiable relaxation based on homotopy continuation that makes it possible to target sparsity by working directly with $L_0$ regularization. We identify failure modes for regularized BO and develop a hyperparameter-free method, sparsity exploring Bayesian optimization (SEBO) that seeks to simultaneously maximize a target objective and sparsity. SEBO and methods based on fixed regularization are evaluated on synthetic and real-world problems, and we show that we are able to efficiently optimize for sparsity."},{"title":"A Neuro-vector-symbolic Architecture for Solving Raven's Progressive Matrices.","link":"http://arxiv.org/abs/2203.04571","abstract":"Neither deep neural networks nor symbolic AI alone has approached the kind of intelligence expressed in humans. This is mainly because neural networks are not able to decompose joint representations to obtain distinct objects (the so-called binding problem), while symbolic AI suffers from exhaustive rule searches, among other problems. These two problems are still pronounced in neuro-symbolic AI which aims to combine the best of the two paradigms. Here, we show that the two problems can be addressed with our proposed neuro-vector-symbolic architecture (NVSA) by exploiting its powerful operators on high-dimensional distributed representations that serve as a common language between neural networks and symbolic AI. The efficacy of NVSA is demonstrated by solving the Raven's progressive matrices datasets. Compared to state-of-the-art deep neural network and neuro-symbolic approaches, end-to-end training of NVSA achieves a new record of 87.7% average accuracy in RAVEN, and 88.1% in I-RAVEN datasets. Moreover, compared to the symbolic reasoning within the neuro-symbolic approaches, the probabilistic reasoning of NVSA with less expensive operations on the distributed representations is two orders of magnitude faster. Our code is available at https://github.com/IBM/neuro-vector-symbolic-architectures."},{"title":"Strategy Complexity of Point Payoff, Mean Payoff and Total Payoff Objectives in Countable MDPs.","link":"http://arxiv.org/abs/2203.07079","abstract":"We study countably infinite Markov decision processes (MDPs) with real-valued transition rewards. Every infinite run induces the following sequences of payoffs: 1. Point payoff (the sequence of directly seen transition rewards), 2. Mean payoff (the sequence of the sums of all rewards so far, divided by the number of steps), and 3. Total payoff (the sequence of the sums of all rewards so far). For each payoff type, the objective is to maximize the probability that the $\\liminf$ is non-negative. We establish the complete picture of the strategy complexity of these objectives, i.e., how much memory is necessary and sufficient for $\\varepsilon$-optimal (resp. optimal) strategies. Some cases can be won with memoryless deterministic strategies, while others require a step counter, a reward counter, or both."},{"title":"Don't fear the unlabelled: safe semi-supervised learning via simple debiasing.","link":"http://arxiv.org/abs/2203.07512","abstract":"Semi-supervised learning (SSL) provides an effective means of leveraging unlabelled data to improve a model performance. Even though the domain has received a considerable amount of attention in the past years, most methods present the common drawback of lacking theoretical guarantees. Our starting point is to notice that the estimate of the risk that most discriminative SSL methods minimise is biased, even asymptotically. This bias impedes the use of standard statistical learning theory and can hurt empirical performance. We propose a simple way of removing the bias. Our debiasing approach is straightforward to implement and applicable to most deep SSL methods. We provide simple theoretical guarantees on the trustworthiness of these modified methods, without having to rely on the strong assumptions on the data distribution that SSL theory usually requires. In particular, we provide generalisation error bounds for the proposed methods. We evaluate debiased versions of different existing SSL methods, such as the Pseudo-label method and Fixmatch, and show that debiasing can compete with classic deep SSL techniques in various settings by providing better calibrated models. Additionally, we provide a theoretical explanation of the intuition of the popular SSL methods."},{"title":"Disentangling Abstraction from Statistical Pattern Matching in Human and Machine Learning.","link":"http://arxiv.org/abs/2204.01437","abstract":"The ability to acquire abstract knowledge is a hallmark of human intelligence and is believed by many to be one of the core differences between humans and neural network models. Agents can be endowed with an inductive bias towards abstraction through meta-learning, where they are trained on a distribution of tasks that share some abstract structure that can be learned and applied. However, because neural networks are hard to interpret, it can be difficult to tell whether agents have learned the underlying abstraction, or alternatively statistical patterns that are characteristic of that abstraction. In this work, we compare the performance of humans and agents in a meta-reinforcement learning paradigm in which tasks are generated from abstract rules. We define a novel methodology for building \"task metamers\" that closely match the statistics of the abstract tasks but use a different underlying generative process, and evaluate performance on both abstract and metamer tasks. We find that humans perform better at abstract tasks than metamer tasks whereas common neural network architectures typically perform worse on the abstract tasks than the matched metamers. This work provides a foundation for characterizing differences between humans and machine learning that can be used in future work towards developing machines with more human-like behavior."},{"title":"Geometric Graph Representation with Learnable Graph Structure and Adaptive AU Constraint for Micro-Expression Recognition.","link":"http://arxiv.org/abs/2205.00380","abstract":"Micro-expression recognition (MER) is valuable because micro-expressions (MEs) can reveal genuine emotions. Most works take image sequences as input and cannot effectively explore ME information because subtle ME-related motions are easily submerged in unrelated information. Instead, the facial landmark is a low-dimensional and compact modality, which achieves lower computational cost and potentially concentrates on ME-related movement features. However, the discriminability of facial landmarks for MER is unclear. Thus, this paper explores the contribution of facial landmarks and proposes a novel framework to efficiently recognize MEs. Firstly, a geometric two-stream graph network is constructed to aggregate the low-order and high-order geometric movement information from facial landmarks to obtain discriminative ME representation. Secondly, a self-learning fashion is introduced to automatically model the dynamic relationship between nodes even long-distance nodes. Furthermore, an adaptive action unit loss is proposed to reasonably build the strong correlation between landmarks, facial action units and MEs. Notably, this work provides a novel idea with much higher efficiency to promote MER, only utilizing graph-based geometric features. The experimental results demonstrate that the proposed method achieves competitive performance with a significantly reduced computational cost. Furthermore, facial landmarks significantly contribute to MER and are worth further study for high-efficient ME analysis."},{"title":"Human-AI Shared Control via Policy Dissection.","link":"http://arxiv.org/abs/2206.00152","abstract":"Human-AI shared control allows human to interact and collaborate with AI to accomplish control tasks in complex environments. Previous Reinforcement Learning (RL) methods attempt the goal-conditioned design to achieve human-controllable policies at the cost of redesigning the reward function and training paradigm. Inspired by the neuroscience approach to investigate the motor cortex in primates, we develop a simple yet effective frequency-based approach called \\textit{Policy Dissection} to align the intermediate representation of the learned neural controller with the kinematic attributes of the agent behavior. Without modifying the neural controller or retraining the model, the proposed approach can convert a given RL-trained policy into a human-interactive policy. We evaluate the proposed approach on the RL tasks of autonomous driving and locomotion. The experiments show that human-AI shared control achieved by Policy Dissection in driving task can substantially improve the performance and safety in unseen traffic scenes. With human in the loop, the locomotion robots also exhibit versatile controllable motion skills even though they are only trained to move forward. Our results suggest the promising direction of implementing human-AI shared autonomy through interpreting the learned representation of the autonomous agents. Demo video and code will be made available at https://metadriverse.github.io/policydissect."},{"title":"Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning.","link":"http://arxiv.org/abs/2206.01342","abstract":"While the empirical success of self-supervised learning (SSL) heavily relies on the usage of deep nonlinear models, existing theoretical works on SSL understanding still focus on linear ones. In this paper, we study the role of nonlinearity in the training dynamics of contrastive learning (CL) on one and two-layer nonlinear networks with homogeneous activation $h(x) = h'(x)x$. We have two major theoretical discoveries. First, the presence of nonlinearity can lead to many local optima even in 1-layer setting, each corresponding to certain patterns from the data distribution, while with linear activation, only one major pattern can be learned. This suggests that models with lots of parameters can be regarded as a \\emph{brute-force} way to find these local optima induced by nonlinearity. Second, in the 2-layer case, linear activation is proven not capable of learning specialized weights into diverse patterns, demonstrating the importance of nonlinearity. In addition, for 2-layer setting, we also discover \\emph{global modulation}: those local patterns discriminative from the perspective of global-level patterns are prioritized to learn, further characterizing the learning process. Simulation verifies our theoretical findings."},{"title":"Time-aware Dynamic Graph Embedding for Asynchronous Structural Evolution.","link":"http://arxiv.org/abs/2207.00594","abstract":"Dynamic graphs refer to graphs whose structure dynamically changes over time. Despite the benefits of learning vertex representations (i.e., embeddings) for dynamic graphs, existing works merely view a dynamic graph as a sequence of changes within the vertex connections, neglecting the crucial asynchronous nature of such dynamics where the evolution of each local structure starts at different times and lasts for various durations. To maintain asynchronous structural evolutions within the graph, we innovatively formulate dynamic graphs as temporal edge sequences associated with joining time of vertices (ToV) and timespan of edges (ToE). Then, a time-aware Transformer is proposed to embed vertices' dynamic connections and ToEs into the learned vertex representations. Meanwhile, we treat each edge sequence as a whole and embed its ToV of the first vertex to further encode the time-sensitive information. Extensive evaluations on several datasets show that our approach outperforms the state-of-the-art in a wide range of graph mining tasks. At the same time, it is very efficient and scalable for embedding large-scale dynamic graphs."},{"title":"Verifying the Union of Manifolds Hypothesis for Image Data.","link":"http://arxiv.org/abs/2207.02862","abstract":"Deep learning has had tremendous success at learning low-dimensional representations of high-dimensional data. This success would be impossible if there was no hidden low-dimensional structure in data of interest; this existence is posited by the manifold hypothesis, which states that the data lies on an unknown manifold of low intrinsic dimension. In this paper, we argue that this hypothesis does not properly capture the low-dimensional structure typically present in image data. Assuming that data lies on a single manifold implies intrinsic dimension is identical across the entire data space, and does not allow for subregions of this space to have a different number of factors of variation. To address this deficiency, we consider the union of manifolds hypothesis, which states that data lies on a disjoint union of manifolds of varying intrinsic dimensions. We empirically verify this hypothesis on commonly-used image datasets, finding that indeed, observed data lies on a disconnected set and that intrinsic dimension is not constant. We also provide insights into the implications of the union of manifolds hypothesis in deep learning, both supervised and unsupervised, showing that designing models with an inductive bias for this structure improves performance across classification and generative modelling tasks. Our code is available at https://github.com/layer6ai-labs/UoMH."},{"title":"Trainability Preserving Neural Pruning.","link":"http://arxiv.org/abs/2207.12534","abstract":"Many recent works have shown trainability plays a central role in neural network pruning -- unattended broken trainability can lead to severe under-performance and unintentionally amplify the effect of retraining learning rate, resulting in biased (or even misinterpreted) benchmark results. This paper introduces trainability preserving pruning (TPP), a scalable method to preserve network trainability against pruning, aiming for improved pruning performance and being more robust to retraining hyper-parameters (e.g., learning rate). Specifically, we propose to penalize the gram matrix of convolutional filters to decorrelate the pruned filters from the retained filters. In addition to the convolutional layers, per the spirit of preserving the trainability of the whole network, we also propose to regularize the batch normalization parameters (scale and bias). Empirical studies on linear MLP networks show that TPP can perform on par with the oracle trainability recovery scheme. On nonlinear ConvNets (ResNet56/VGG19) on CIFAR10/100, TPP outperforms the other counterpart approaches by an obvious margin. Moreover, results on ImageNet-1K with ResNets suggest that TPP consistently performs more favorably against other top-performing structured pruning approaches. Code: https://github.com/MingSun-Tse/TPP."},{"title":"Language Models Can Teach Themselves to Program Better.","link":"http://arxiv.org/abs/2207.14502","abstract":"Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM's performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model 'improves itself' using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al., 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance."},{"title":"FairGBM: Gradient Boosting with Fairness Constraints.","link":"http://arxiv.org/abs/2209.07850","abstract":"Tabular data is prevalent in many high-stakes domains, such as financial services or public policy. Gradient Boosted Decision Trees (GBDT) are popular in these settings due to their scalability, performance, and low training cost. While fairness in these domains is a foremost concern, existing in-processing Fair ML methods are either incompatible with GBDT, or incur in significant performance losses while taking considerably longer to train. We present FairGBM, a dual ascent learning framework for training GBDT under fairness constraints, with little to no impact on predictive performance when compared to unconstrained GBDT. Since observational fairness metrics are non-differentiable, we propose smooth convex error rate proxies for common fairness criteria, enabling gradient-based optimization using a ``proxy-Lagrangian'' formulation. Our implementation shows an order of magnitude speedup in training time relative to related work, a pivotal aspect to foster the widespread adoption of FairGBM by real-world practitioners."},{"title":"Machine Learning for Stress Monitoring from Wearable Devices: A Systematic Literature Review.","link":"http://arxiv.org/abs/2209.15137","abstract":"Introduction. The stress response has both subjective, psychological and objectively measurable, biological components. Both of them can be expressed differently from person to person, complicating the development of a generic stress measurement model. This is further compounded by the lack of large, labeled datasets that can be utilized to build machine learning models for accurately detecting periods and levels of stress. The aim of this review is to provide an overview of the current state of stress detection and monitoring using wearable devices, and where applicable, machine learning techniques utilized.  Methods. This study reviewed published works contributing and/or using datasets designed for detecting stress and their associated machine learning methods, with a systematic review and meta-analysis of those that utilized wearable sensor data as stress biomarkers. The electronic databases of Google Scholar, Crossref, DOAJ and PubMed were searched for relevant articles and a total of 24 articles were identified and included in the final analysis. The reviewed works were synthesized into three categories of publicly available stress datasets, machine learning, and future research directions.  Results. A wide variety of study-specific test and measurement protocols were noted in the literature. A number of public datasets were identified that are labeled for stress detection. In addition, we discuss that previous works show shortcomings in areas such as their labeling protocols, lack of statistical power, validity of stress biomarkers, and generalization ability.  Conclusion. Generalization of existing machine learning models still require further study, and research in this area will continue to provide improvements as newer and more substantial datasets become available for study."},{"title":"Learning Perception-Aware Agile Flight in Cluttered Environments.","link":"http://arxiv.org/abs/2210.01841","abstract":"Recently, neural control policies have outperformed existing model-based planning-and-control methods for autonomously navigating quadrotors through cluttered environments in minimum time. However, they are not perception aware, a crucial requirement in vision-based navigation due to the camera's limited field of view and the underactuated nature of a quadrotor. We propose a learning-based system that achieves perception-aware, agile flight in cluttered environments. Our method combines imitation learning with reinforcement learning (RL) by leveraging a privileged learning-by-cheating framework. Using RL, we first train a perception-aware teacher policy with full-state information to fly in minimum time through cluttered environments. Then, we use imitation learning to distill its knowledge into a vision-based student policy that only perceives the environment via a camera. Our approach tightly couples perception and control, showing a significant advantage in computation speed (10 times faster) and success rate. We demonstrate the closed-loop control performance using hardware-in-the-loop simulation."},{"title":"Sampling-based inference for large linear models, with application to linearised Laplace.","link":"http://arxiv.org/abs/2210.04994","abstract":"Large-scale linear models are ubiquitous throughout machine learning, with contemporary application as surrogate models for neural network uncertainty quantification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method's application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parameters, 100 output dimensions x 50k datapoints) and with a U-Net on a high-resolution tomographic reconstruction task (2M parameters, 251k output dimensions)."},{"title":"Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring.","link":"http://arxiv.org/abs/2211.07138","abstract":"Federated learning (FL) allows multiple participants to collaboratively build deep learning (DL) models without directly sharing data. Consequently, the issue of copyright protection in FL becomes important since unreliable participants may gain access to the jointly trained model. Application of homomorphic encryption (HE) in secure FL framework prevents the central server from accessing plaintext models. Thus, it is no longer feasible to embed the watermark at the central server using existing watermarking schemes. In this paper, we propose a novel client-side FL watermarking scheme to tackle the copyright protection issue in secure FL with HE. To our best knowledge, it is the first scheme to embed the watermark to models under the Secure FL environment. We design a black-box watermarking scheme based on client-side backdooring to embed a pre-designed trigger set into an FL model by a gradient-enhanced embedding method. Additionally, we propose a trigger set construction mechanism to ensure the watermark cannot be forged. Experimental results demonstrate that our proposed scheme delivers outstanding protection performance and robustness against various watermark removal attacks and ambiguity attack."},{"title":"Inclusive Artificial Intelligence.","link":"http://arxiv.org/abs/2212.12633","abstract":"Prevailing methods for assessing and comparing generative AIs incentivize responses that serve a hypothetical representative individual. Evaluating models in these terms presumes homogeneous preferences across the population and engenders selection of agglomerative AIs, which fail to represent the diverse range of interests across individuals. We propose an alternative evaluation method that instead prioritizes inclusive AIs, which provably retain the requisite knowledge not only for subsequent response customization to particular segments of the population but also for utility-maximizing decisions."},{"title":"Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems.","link":"http://arxiv.org/abs/2301.04090","abstract":"Networked discrete dynamical systems are often used to model the spread of contagions and decision-making by agents in coordination games. Fixed points of such dynamical systems represent configurations to which the system converges. In the dissemination of undesirable contagions (such as rumors and misinformation), convergence to fixed points with a small number of affected nodes is a desirable goal. Motivated by such considerations, we formulate a novel optimization problem of finding a nontrivial fixed point of the system with the minimum number of affected nodes. We establish that, unless P = NP, there is no polynomial time algorithm for approximating a solution to this problem to within the factor n^1-\\epsilon for any constant epsilon &gt; 0. To cope with this computational intractability, we identify several special cases for which the problem can be solved efficiently. Further, we introduce an integer linear program to address the problem for networks of reasonable sizes. For solving the problem on larger networks, we propose a general heuristic framework along with greedy selection methods. Extensive experimental results on real-world networks demonstrate the effectiveness of the proposed heuristics."},{"title":"Imitating Human Behaviour with Diffusion Models.","link":"http://arxiv.org/abs/2301.10677","abstract":"Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment."},{"title":"Learning from Multiple Independent Advisors in Multi-agent Reinforcement Learning.","link":"http://arxiv.org/abs/2301.11153","abstract":"Multi-agent reinforcement learning typically suffers from the problem of sample inefficiency, where learning suitable policies involves the use of many data samples. Learning from external demonstrators is a possible solution that mitigates this problem. However, most prior approaches in this area assume the presence of a single demonstrator. Leveraging multiple knowledge sources (i.e., advisors) with expertise in distinct aspects of the environment could substantially speed up learning in complex environments. This paper considers the problem of simultaneously learning from multiple independent advisors in multi-agent reinforcement learning. The approach leverages a two-level Q-learning architecture, and extends this framework from single-agent to multi-agent settings. We provide principled algorithms that incorporate a set of advisors by both evaluating the advisors at each state and subsequently using the advisors to guide action selection. We also provide theoretical convergence and sample complexity guarantees. Experimentally, we validate our approach in three different test-beds and show that our algorithms give better performances than baselines, can effectively integrate the combined expertise of different advisors, and learn to ignore bad advice."},{"title":"Towards Adversarial Realism and Robust Learning for IoT Intrusion Detection and Classification.","link":"http://arxiv.org/abs/2301.13122","abstract":"The Internet of Things (IoT) faces tremendous security challenges. Machine learning models can be used to tackle the growing number of cyber-attack variations targeting IoT systems, but the increasing threat posed by adversarial attacks restates the need for reliable defense strategies. This work describes the types of constraints required for a realistic adversarial cyber-attack example and proposes a methodology for a trustworthy adversarial robustness analysis with a realistic adversarial evasion attack vector. The proposed methodology was used to evaluate three supervised algorithms, Random Forest (RF), Extreme Gradient Boosting (XGB), and Light Gradient Boosting Machine (LGBM), and one unsupervised algorithm, Isolation Forest (IFOR). Constrained adversarial examples were generated with the Adaptative Perturbation Pattern Method (A2PM), and evasion attacks were performed against models created with regular and adversarial training. Even though RF was the least affected in binary classification, XGB consistently achieved the highest accuracy in multi-class classification. The obtained results evidence the inherent susceptibility of tree-based algorithms and ensembles to adversarial evasion attacks and demonstrates the benefits of adversarial training and a security by design approach for a more robust IoT network intrusion detection and cyber-attack classification."},{"title":"PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets.","link":"http://arxiv.org/abs/2302.00586","abstract":"The financial markets, which involve more than $90 trillion market capitals, attract the attention of innumerable investors around the world. Recently, reinforcement learning in financial markets (FinRL) has emerged as a promising direction to train agents for making profitable investment decisions. However, the evaluation of most FinRL methods only focuses on profit-related measures and ignores many critical axes, which are far from satisfactory for financial practitioners to deploy these methods into real-world financial markets. Therefore, we introduce PRUDEX-Compass, which has 6 axes, i.e., Profitability, Risk-control, Universality, Diversity, rEliability, and eXplainability, with a total of 17 measures for a systematic evaluation. Specifically, i) we propose AlphaMix+ as a strong FinRL baseline, which leverages mixture-of-experts (MoE) and risk-sensitive approaches to make diversified risk-aware investment decisions, ii) we evaluate 8 FinRL methods in 4 long-term real-world datasets of influential financial markets to demonstrate the usage of our PRUDEX-Compass, iii) PRUDEX-Compass together with 4 real-world datasets, standard implementation of 8 FinRL methods and a portfolio management environment is released as public resources to facilitate the design and comparison of new FinRL methods. We hope that PRUDEX-Compass can not only shed light on future FinRL research to prevent untrustworthy results from stagnating FinRL into successful industry deployment but also provide a new challenging algorithm evaluation scenario for the reinforcement learning (RL) community."},{"title":"Continual Learning of Language Models.","link":"http://arxiv.org/abs/2302.03241","abstract":"Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual learning of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method."},{"title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation.","link":"http://arxiv.org/abs/2302.07845","abstract":"Translating natural language into Bash Commands is an emerging research field that has gained attention in recent years. Most efforts have focused on producing more accurate translation models. To the best of our knowledge, only two datasets are available, with one based on the other. Both datasets involve scraping through known data sources (through platforms like stack overflow, crowdsourcing, etc.) and hiring experts to validate and correct either the English text or Bash Commands. This paper provides two contributions to research on synthesizing Bash Commands from scratch. First, we describe a state-of-the-art translation model used to generate Bash Commands from the corresponding English text. Second, we introduce a new NL2CMD dataset that is automatically generated, involves minimal human intervention, and is over six times larger than prior datasets. Since the generation pipeline does not rely on existing Bash Commands, the distribution and types of commands can be custom adjusted. We evaluate the performance of ChatGPT on this task and discuss the potential of using it as a data generator. Our empirical results show how the scale and diversity of our dataset can offer unique opportunities for semantic parsing researchers."},{"title":"A Planning-Based Explainable Collaborative Dialogue System.","link":"http://arxiv.org/abs/2302.09646","abstract":"Eva is a multimodal conversational system that helps users to accomplish their domain goals through collaborative dialogue. The system does this by inferring users' intentions and plans to achieve those goals, detects whether obstacles are present, finds plans to overcome them or to achieve higher-level goals, and plans its actions, including speech acts,to help users accomplish those goals. In doing so, the system maintains and reasons with its own beliefs, goals and intentions, and explicitly reasons about those of its user. Belief reasoning is accomplished with a modal Horn-clause meta-interpreter. The planning and reasoning subsystems obey the principles of persistent goals and intentions, including the formation and decomposition of intentions to perform complex actions, as well as the conditions under which they can be given up. In virtue of its planning process, the system treats its speech acts just like its other actions -- physical acts affect physical states, digital acts affect digital states, and speech acts affect mental and social states. This general approach enables Eva to plan a variety of speech acts including requests, informs, questions, confirmations, recommendations, offers, acceptances, greetings, and emotive expressions. Each of these has a formally specified semantics which is used during the planning and reasoning processes. Because it can keep track of different users' mental states, it can engage in multi-party dialogues. Importantly, Eva can explain its utterances because it has created a plan standing behind each of them. Finally, Eva employs multimodal input and output, driving an avatar that can perceive and employ facial and head movements along with emotive speech acts."},{"title":"On The Coherence of Quantitative Evaluation of Visual Explanations.","link":"http://arxiv.org/abs/2302.10764","abstract":"Recent years have shown an increased development of methods for justifying the predictions of neural networks through visual explanations. These explanations usually take the form of heatmaps which assign a saliency (or relevance) value to each pixel of the input image that expresses how relevant the pixel is for the prediction of a label.  Complementing this development, evaluation methods have been proposed to assess the \"goodness\" of such explanations. On the one hand, some of these methods rely on synthetic datasets. However, this introduces the weakness of having limited guarantees regarding their applicability on more realistic settings. On the other hand, some methods rely on metrics for objective evaluation. However the level to which some of these evaluation methods perform with respect to each other is uncertain.  Taking this into account, we conduct a comprehensive study on a subset of the ImageNet-1k validation set where we evaluate a number of different commonly-used explanation methods following a set of evaluation methods. We complement our study with sanity checks on the studied evaluation methods as a means to investigate their reliability and the impact of characteristics of the explanations on the evaluation methods.  Results of our study suggest that there is a lack of coherency on the grading provided by some of the considered evaluation methods. Moreover, we have identified some characteristics of the explanations, e.g. sparsity, which can have a significant effect on the performance."},{"title":"Language Models are Few-shot Learners for Prognostic Prediction.","link":"http://arxiv.org/abs/2302.12692","abstract":"Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases."},{"title":"Automated Task-Time Interventions to Improve Teamwork using Imitation Learning.","link":"http://arxiv.org/abs/2303.00413","abstract":"Effective human-human and human-autonomy teamwork is critical but often challenging to perfect. The challenge is particularly relevant in time-critical domains, such as healthcare and disaster response, where the time pressures can make coordination increasingly difficult to achieve and the consequences of imperfect coordination can be severe. To improve teamwork in these and other domains, we present TIC: an automated intervention approach for improving coordination between team members. Using BTIL, a multi-agent imitation learning algorithm, our approach first learns a generative model of team behavior from past task execution data. Next, it utilizes the learned generative model and team's task objective (shared reward) to algorithmically generate execution-time interventions. We evaluate our approach in synthetic multi-agent teaming scenarios, where team members make decentralized decisions without full observability of the environment. The experiments demonstrate that the automated interventions can successfully improve team performance and shed light on the design of autonomous agents for improving teamwork."},{"title":"Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision.","link":"http://arxiv.org/abs/2303.00462","abstract":"This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our approach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implicitly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learning problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experiments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal supervised learning to infer more accurate 4D radar scene flow. We also show its usefulness to two subtasks - motion segmentation and ego-motion estimation. Our source code will be available on https://github.com/Toytiny/CMFlow."},{"title":"Attention-based Graph Convolution Fusing Latent Structures and Multiple Features for Graph Neural Networks.","link":"http://arxiv.org/abs/2303.00944","abstract":"We present an attention-based spatial graph convolution (AGC) for graph neural networks (GNNs). Existing AGCs focus on only using node-wise features and utilizing one type of attention function when calculating attention weights. Instead, we propose two methods to improve the representational power of AGCs by utilizing 1) structural information in a high-dimensional space and 2) multiple attention functions when calculating their weights. The first method computes a local structure representation of a graph in a high-dimensional space. The second method utilizes multiple attention functions simultaneously in one AGC. Both approaches can be combined. We also propose a GNN for the classification of point clouds and that for the prediction of point labels in a point cloud based on the proposed AGC. According to experiments, the proposed GNNs perform better than existing methods. Our codes open at https://github.com/liyang-tuat/SFAGC."},{"title":"AI and the FCI: Can ChatGPT Project an Understanding of Introductory Physics?.","link":"http://arxiv.org/abs/2303.01067","abstract":"ChatGPT is a groundbreaking ``chatbot\"--an AI interface built on a large language model that was trained on an enormous corpus of human text to emulate human conversation. Beyond its ability to shoot the breeze in a plausible way, it has attracted attention for its ability to competently answer questions from the bar exam and from MBA coursework, and to provide useful assistance in writing computer code. These apparent abilities have prompted discussion of ChatGPT as both a threat to the integrity of higher education and conversely as a powerful teaching tool. In this work we present a preliminary analysis of how ChatGPT fares in the field of first-semester university physics, using primarily the Force Concept Inventory (FCI) to assess whether it can give correct responses to conceptual physics questions about kinematics and Newtonian dynamics. We demonstrate that, by some measures, ChatGPT can match or exceed the median performance of a university student who has completed one semester of college physics, though its performance is notably uneven and the results are nuanced. We conclude with a discussion of these results in light of four questions that motivated this work: what does ChatGPT's performance tell us about the nature of conceptual assessment tools like the FCI? How might the availability of ChatGPT as a resource for students? Can ChatGPT be used as an in-class teaching tool for physics instruction? And can it be used as an out-of-classroom aid to those engaged in physics pedagogy?"}]},{"name":"Plant Biology","feed":[{"title":"Optimization of somatic embryogenesis in Euterpe edulis Martius using auxin analogs and atomic force microscopy","link":"http://biorxiv.org/cgi/content/short/2023.03.04.531114v1?rss=1","abstract":"Euterpe edulis Martius is an endangered species of the Atlantic Forest, whose fruits have high antioxidant potential, and propagated exclusively by seeds. The present study assessed the ability of different auxin inducers and picloram analogs to trigger somatic embryogenesis in E. edulis. Immature seeds were harvested, and their zygotic embryos were excised and grown in MS culture medium supplemented with 2,4-D dichlorophenoxyacetic acid (2,4-D) or picloram at 150, 300, 450, 600 M. The activity of picloram analogs triclopyr and clopyralid was evaluated in semisolid MS medium. At maturation and germination, picloram-derived calli and somatic embryos isolated from triclopyr-grown cultures were first transferred to pre-maturation medium and, after 30 days, to basal MS or MS medium supplemented with either 5 M abscisic acid or 0.53 M 1-naphthaleneacetic acid plus 12.3 M 2-isopentenyladenine. Finally, somatic embryos with root protrusions were transferred to MS medium devoid of sucrose for 30 days and then acclimatized ex vitro. Scanning, transmission, and atomic force microscopy revealed that picloram was superior to 2,4-D but less effective than triclopyr (100 M) in inducing embryogenesis. Maturation and germination of somatic embryos in E. edulis can be maximized by 5 M abscisic acid, and selecting calli via atomic force microscopy."},{"title":"A transcriptional activator effector of Ustilago maydis regulates hyperplasia in maize during pathogen-induced tumor formation","link":"http://biorxiv.org/cgi/content/short/2023.03.06.531288v1?rss=1","abstract":"Ustilago maydis causes common smut in maize, which is characterized by tumor formation in aerial parts of the host. Tumors result from the de novo cell division of highly developed bundle sheath and subsequent cell enlargement. However, the molecular mechanisms underlying tumorigenesis are still largely unknown. Here, we characterize the U. maydis effector Sts2 (Small tumor on seedlings 2), which promotes the division of hyperplasia tumor cells. Upon infection, Sts2 is translocated into the maize cell nucleus, where it acts as a transcriptional activator, and the transactivation activity is crucial for its virulence function. Sts2 interacts with ZmNECAP1, a yet undescribed plant transcriptional activator, and it activates the expression of several leaf developmental regulators to potentiate tumor formation. Contrary, fusion of a suppressive SRDX-motif to Sts2 causes dominant negative inhibition of tumor formation, underpinning the central role of Sts2 for tumorigenesis. Our results not only disclose the virulence mechanism of a tumorigenic effector, but also reveal the essential role of leaf developmental regulators in pathogen-induced tumor formation."},{"title":"Genomic resources for a historical collection of cultivated two-row European spring barley genotypes.","link":"http://biorxiv.org/cgi/content/short/2023.03.06.531259v1?rss=1","abstract":"Barley genomic resources are increasing rapidly, with the publication of a barley pangenome as one of the latest developments. Two-row spring barley cultivars are intensely studied as they are the source of high-quality grain for malting and distilling. Here we provide data from a European two-row spring barley population containing 209 different genotypes registered for the UK market between 1830 to 2014. The dataset encompasses RNA-sequencing data from six different tissues across a range of barley developmental stages, phenotypic datasets from two consecutive years of field-grown trials in the United Kingdom, Germany and the USA; and whole genome shotgun sequencing from all cultivars, which was used to complement the RNA-sequencing data for variant calling. The outcomes are a filtered SNP marker file, a phenotypic database and a large gene expression dataset providing a comprehensive resource which allows for downstream analyses like genome wide association studies or expression associations."},{"title":"Leaf hydration status under drought is predominantly linked to stomatal regulation and leaf roll but not osmotic adjustment in Canadian hard red spring wheat (Triticum aestivum) cultivars","link":"http://biorxiv.org/cgi/content/short/2023.03.05.531113v1?rss=1","abstract":"For wheat (Triticum aestivum), sustained crop yield at limited soil water has been linked to osmotic adjustment (OA) as one of the main drivers to minimize drought-induced reductions in leaf hydration status and growth. Canada Western Red Spring (CWRS) cultivars are typically grown in rainfed areas in northern regions with milder climates, but ongoing climate change has increased the frequency and intensity of drought event questioning how successful they are in tolerating drought. The extent of OA and its relation to stomatal behavior, leaf roll, and kernel development under periods of drought remain elusive for CWRS. For several commercially used Canadian cultivars (Superb, Stettler, AAC Viewfield), OA was not found to be a mechanism contributing to drought tolerance. In contrast, we found that sustained kernel weight during periods of relatively low soil water content was linked to tight stomatal behavior (i.e., efficient transition from onset to full stomatal closure) and early leaf roll (i.e., reductions in flag leaf width). Moreover, leaf hydration status ({Theta}RWC) marked the onset of drought-induced losses in kernel weight in all three cultivars. In conclusion, HRSW lacks OA but leaf stomatal behavior and leaf rolling aid in securing leaf hydration status and kernel weight under drought."},{"title":"Drought and recovery in barley: key gene networks and retrotransposon response.","link":"http://biorxiv.org/cgi/content/short/2023.03.05.531133v1?rss=1","abstract":"During drought, plants close their stomata at a critical soil water content (SWC), together with diverse physiological, developmental, and biochemical responses. Using precision-phenotyping lysimeters, we imposed pre-flowering drought on four barley varieties (Arvo, Golden Promise, Hankkija 673 and Morex) and followed their physiological responses. For Golden Promise, we carried out RNA-seq on leaf transcripts before and during drought, and during recovery, also examining retrotransposon BARE1 expression. Transcriptional data were subjected to network analysis. The varieties differed by their critical SWC, Hankkija 673 responding at the highest and Golden Promise at the lowest. Pathways connected to drought and salinity response were strongly upregulated during drought; pathways connected to growth and development were strongly downregulated. During recovery, growth and development pathways were upregulated; altogether 117 networked genes involved in ubiquitin-mediated autophagy were downregulated. The differential response to SWC suggests adaptation to distinct rainfall patterns. We identified several strongly differentially expressed genes not earlier associated with drought response in barley. BARE1 transcription is strongly transcriptionally upregulated by drought and downregulated during recovery unequally between the investigated cultivars. The downregulation of networked autophagy genes suggests a role for autophagy in drought response; its importance to resilience should be further investigated."},{"title":"Multi-omic dissection of ancestral heat stress memory responses in Brachypodium distachyon","link":"http://biorxiv.org/cgi/content/short/2023.03.04.531132v1?rss=1","abstract":"Stressful environmental conditions, including heat stress (HS), are a major limiting factor in crop yield. Understanding the molecular mechanisms of plant stress memory and resilience is important for engineering more resistant plants and improving crop yield. To study how the different gene regulatory layers change upon repeated HS and how these layers are interconnected, we performed a dense temporal atlas of gene expression, alternative splicing, small and long noncoding RNAs, and DNA methylation in Brachypodium distachyon. Results show that a second HS induces changes in coding and noncoding RNA expression and alternative splicing and that DNA demethylation is responsible for mediating differential gene expression. We identified a long noncoding RNA regulatory network and provided evidence that lncRNAs positively regulate gene expression, while miRNAs are implicated in alternative splicing events. We reconstructed the ancestral heat memory network of flowering plants by comparing the dynamic responses of Arabidopsis thaliana and Brachypodium distachyon. These findings enhance our understanding of the complex inter-layer cross-talk governing HS resilience and memory and identify novel genes essential for these processes."},{"title":"Nodule-specific Cu+-chaperone NCC1 is required for symbiotic nitrogen fixation in Medicago truncatula root nodules","link":"http://biorxiv.org/cgi/content/short/2023.03.05.531139v1?rss=1","abstract":"Cu+-chaperones are a diverse group of proteins that allocate Cu+ to specific copper-proteins, creating different copper pools targeted to specific physiological processes. Symbiotic nitrogen fixation carried out in legume root nodules requires relatively large amounts of copper, for which targeted copper deliver systems would be required. MtNCC1 is a nodule-specific Cu+-chaperone encoded in the Medicago truncatula genome, with a N-terminus Atx1-like domain that can bind Cu+ with picomolar affinities. This gene is expressed primarily from the late infection zone to the early fixation zone, and is located in the cytosol, associated to plasma and symbiosome membranes, and within nuclei. Consistent with its key role in nitrogen fixation, ncc1 mutants have a severe reduction of nitrogenase activity, and a 50% reduction in copper-dependent cytochrome c oxidase activity. A subset of the copper-proteome is also affected in the mutant nodules. Many of these proteins can be pulled-down when using a Cu+-loaded N-terminal MtNCC1 moiety as a bait, indicating a role in nodule copper homeostasis and in copper-dependent physiological processes. Overall, these data suggest a pleiotropic role of MtNCC1 in copper delivery for symbiotic nitrogen fixation."},{"title":"Arabidopsis thaliana BBX14 is a target of GLK1 and involved in high-light acclimation, photomorphogenesis and GUN-type retrograde signaling","link":"http://biorxiv.org/cgi/content/short/2023.03.03.530939v1?rss=1","abstract":"Development of photosynthetically competent seedlings requires both light and retrograde biogenic signaling pathways. The transcription factor GLK1 functions at the interface between these pathways, and receives input from the biogenic-signaling integrator GUN1. BBX14 was previously identified, together with GLK1, in a core module that mediates the response to high light levels and biogenic signaling. To gain insight into the function of BBX14, we generated BBX14 overexpressors and CRISPR/Cas-mediated bbx14 mutant plants, conducted high-light, RT-qPCR and ChIP-Seq experiments, measured photosynthetic parameters, chlorophyll contents and growth rates, and analyzed alterations in transcriptomics. We found that, although overexpression of BBX14 is deleterious under normal growth conditions, BBX14 is needed to acclimate plants to high light stress. BBX14 is a direct target of GLK1, and RNA-Seq analysis suggests that BBX14 is involved in the circadian clock. Knockout of BBX14 results in a long-hypocotyl phenotype that depends on a retrograde signal, and BBX14 expression during biogenic signaling requires GUN1. Finally, we clarify the role of BBX14 in GUN-type biogenic signaling. We conclude that BBX14 is an integrator of photomorphogenetic and biogenic signals, and suggest that BBX14 is a nuclear target of retrograde signals downstream of the GUN1/GLK1 module."},{"title":"Insights into the function of the chloroplastic ribosome-associated GTPase HflX in Arabidopsis thaliana.","link":"http://biorxiv.org/cgi/content/short/2023.03.03.530967v1?rss=1","abstract":"Ribosome-associated GTPases are conserved enzymes that participate in ribosome biogenesis and ribosome function. In bacteria, recent studies have identified HflX as a ribosome-associated GTPase that is involved in both ribosome biogenesis and recycling under stress conditions. Plants possess a chloroplastic HflX homolog, but its function remains unknown. Here, we characterised the role of HflX in the plant Arabidopsis thaliana. Our findings demonstrate that HflX does not have a detectable role in plant growth and development, nor does it play a distinct role in acclimation to several different stresses, including heat, manganese, cold, and salt stress. However, we found that HflX is required for plant resistance to chloroplast translational stress mediated by the antibiotic lincomycin. Our results suggest that HflX is a chloroplast ribosome-associated protein that may play a role in the surveillance of translation. These findings provide new insight into the function of HflX as a ribosome-associated GTPase in plants and highlight the importance of investigating conserved proteins in different organisms to gain a comprehensive understanding of their biological roles."},{"title":"PIF4 promotes water use efficiency during fluctuating light and drought resistance in rice","link":"http://biorxiv.org/cgi/content/short/2023.03.02.530909v1?rss=1","abstract":"Molecular mechanism of intrinsic water use efficiency (iWUE) during fluctuating light (FL) was rarely understood. In this study, we investigated five parameters of iWUE under FL in 200 Minicore rice accessions. Among them, a novel trait, WUEFL (averaged iWUE during FL) has highest SNP heritability in these parameters. GWAS identifies six candidate genes, and PIF4 is highly expressed in high iWUEFL rice subgroup. Nine SNPs were significantly associated with iWUEFL, and v3 SNP located at -1,075 bp of PIF4 promoter shows highest sensitives to light. Deletion of v3 in a rice cultivar, WYG7 (PIF4v3m) leads to ~20% reduction in iWUEFL, and overexpressing PIF4 causes 25% increase in iWUEFL under DS. There are 85% reduction in adenosine 3',5'-diphosphate (PAP) amounts together with 73% increase in SAL1 gene abundance in PIF4v3m than WYG7. PIF4 transcriptionally repress and activate SAL1 and NHX1, respectively, through binding to G-box motifs of the two genes, which leads to 16% reduction and 5% increase in iWUEFL in co-overexpression rice lines of PIF4-SAL1 and PIF4-NHX1, respectively, relative to PIF4-OE under DS. We proposed that PIF4 promotes iWUEFL and stomatal adjustment via targeting the G-box motif of SAL1 and NHX1 genes during FL, eventually facilitating to drought resistance."},{"title":"ERF1 inhibits lateral root emergence by promoting local auxin accumulation with altered distribution and repressing ARF7 expression","link":"http://biorxiv.org/cgi/content/short/2023.03.02.530895v1?rss=1","abstract":"Lateral roots (LRs) are crucial for plants to sense environmental signals in addition to water and nutrient absorption. Auxin is key for LR formation, but the underlying mechanisms are not fully understood. Here we report that Arabidopsis ERF1 inhibits LR emergence by promoting local auxin accumulation with altered distribution and regulating auxin signaling. Loss of ERF1 increases LR density compared with the wild type, whereas ERF1 overexpression causes the opposite phenotype. ERF1 enhances auxin transport by upregulating PIN1 and AUX1, resulting in excessive auxin accumulation in the endodermal, cortical, and epidermal cells surrounding LR primordia. Furthermore, ERF1 represses ARF7 transcription, consequently affecting the expression of cell wall remodeling genes that facilitate LR emergence. Together, our study reveals that ERF1 integrates environmental signals to promote local auxin accumulation with altered distribution and repress ARF7, consequently inhibiting LR emergence in adaptation to fluctuating environments.  HighlightsO_LIERF1 functions as a negative regulator of lateral root emergence C_LIO_LIERF1 enhances rootward and shootward auxin transport by directly upregulating the expression of PIN1 and AUX1, resulting in high local auxin accumulation and abnormal auxin distribution in the endodermal, cortical, and epidermal cells overlying lateral root primordia C_LIO_LIERF1 represses the transcription of ARF7 and cell wall remodeling genes in lateral root emergence C_LI"},{"title":"A critical role of a eubiotic microbiota in gating proper immunocompetence in Arabidopsis","link":"http://biorxiv.org/cgi/content/short/2023.03.02.527037v1?rss=1","abstract":"Although many studies have shown that microbes can ectopically stimulate or suppress plant immune responses, the fundamental question of whether the entire preexisting microbiota is indeed required for proper development of plant immune response remains unanswered. Using a recently developed peat-based gnotobiotic plant growth system we found that Arabidopsis grown in the absence of a natural microbiota lacked age-dependent maturation of plant immune response and were defective in several aspects of pattern-triggered immunity. Axenic plants exhibited hypersusceptibility to infection by the foliar bacterial pathogen Pseudomonas syringae pv. tomato DC3000. Microbiota-mediated immunocompetence was suppressed by rich nutrient conditions, indicating a tripartite interaction between the host, microbiota, and abiotic environment. A synthetic microbiota composed of 48 culturable bacterial strains from the leaf endosphere of healthy Arabidopsis plants was able to substantially restore immunocompetence similar to plants inoculated with a soil-derived community. In contrast, a 52-member dysbiotic synthetic leaf microbiota overstimulated the immune transcriptome. Together, these results provide evidence for a causal role of a eubiotic microbiota in gating proper immunocompetence and age-dependent immunity in plants."},{"title":"Cell specialization and coordination in Arabidopsis leaves upon pathogenic attack revealed by scRNA-seq.","link":"http://biorxiv.org/cgi/content/short/2023.03.02.530814v1?rss=1","abstract":"Plant defense responses involve several biological processes that allow plants to fight against pathogenic attacks. How these different processes are orchestrated within organs and depend on specific cell types is poorly known. Here, using scRNA-seq technology on three independent biological replicates, we identified 10 distinct cell populations in wild-type Arabidopsis leaves inoculated with the bacterial pathogen Pseudomonas syringae DC3000. Among those, we retrieved major cell types of the leaves (mesophyll, guard, epidermal, companion and vascular S cells) to which we could associate characteristic transcriptional reprogramming and regulators, thereby specifying different cell-type responses to the pathogen. Further analyses of transcriptional dynamics, based on inference of cell trajectories, indicated that the different cell types, in addition to their characteristic defense responses, can also share similar modules of gene reprogramming, allowing for instance vascular S cells, epidermal cells and mesophyll cells to converge towards an identical cell fate, mostly characterized by lignification and detoxification functions. Moreover, it appeared that the defense responses of these three cell types can evolve along a second separate path. As this divergence does not correspond to the differentiation between immune and susceptible cells, we speculate that this might reflect the discrimination between cell-autonomous and non-cell-autonomous responses. Altogether our data provide an upgraded framework to describe, explore and explain the specialization and the coordination of plant cell responses upon pathogenic challenge."},{"title":"The selective estrogen receptor modulator clomiphene inhibits sterol biosynthesis in Arabidopsis thaliana","link":"http://biorxiv.org/cgi/content/short/2023.03.02.530820v1?rss=1","abstract":"Sterols are produced via complex, multistep biosynthetic pathways involving similar enzymatic conversions in plants, animals and fungi, yielding a variety of sterol metabolites with slightly different chemical properties to exert diverse and specific functions. The role of plant sterols has been studied in the context of cell biological processes, signaling and overall plant development, mainly based on mutants. Due to their essential nature, genetic interference with their function causes pleiotropic developmental defects. An important alternative is to use a pharmacological approach. However, the current toolset for manipulating sterol biosynthesis in plants remains limited. Here, we probed a collection of inhibitors of mammalian cholesterol biosynthesis to identify new inhibitors of plant sterol biosynthesis. We provide evidence that imidazole-type fungicides, bifonazole, clotrimazole and econazole inhibit the obtusifoliol 14-demethylase CYP51, that is highly conserved among eukaryotes. Surprisingly, we found that the selective estrogen receptor modulator, clomiphene, inhibits sterol biosynthesis, in part by inhibiting the plant-specific cyclopropyl-cycloisomerase CPI1. These results demonstrate that rescreening of the animal sterol biosynthesis pharmacology is an easy approach for identifying novel inhibitors of plant sterol biosynthesis. Such molecules can be used as entry points for the development of plant-specific inhibitors of sterol biosynthesis that can be used in agriculture."},{"title":"metaGE: Investigating Genotype-by-Environment interactions through meta-analysis","link":"http://biorxiv.org/cgi/content/short/2023.03.01.530237v1?rss=1","abstract":"Dissecting the genetic components of Genotype-by-Environment interactions is of key importance in the context of increasing instability and plant competition due to climate change and phytosanitary treatment limitations. It is widely addressed in plants using Multi-Environment Trials (MET), in which statistical modelling for genome-wide association studies (GWAS) is promising but significantly more complex than for single-environment studies. In this context, we introduce metaGE, a flexible and computationally efficient meta-analysis approach for the joint analysis of any MET GWAS experiment. To cope with the specific requirements of the MET context, metaGE accounts for both the heterogeneity of QTL effects across environments and the correlation between GWAS summary statistics acquired on the same or related set(s) of genotypes. Compared to previous GWAS in 3 plant species and a multi-parent population, metaGE identified known and new QTLs. It provided valuable insight into the genetic architecture of several complex traits and the variation of QTL effects conditional to environmental conditions."},{"title":"OsNLP3/4-OsRFL module regulates nitrogen-promoted panicle architecture in rice","link":"http://biorxiv.org/cgi/content/short/2023.02.28.530553v1?rss=1","abstract":"Rice panicles, a major component of yield, are regulated by phytohormones and nutrients. How mineral nutrients promote panicle architecture remains largely unknown. Here, we report that NIN-LIKE PROTEIN3 and 4 (OsNLP3/4) are crucial positive regulators of rice panicle architecture in response to nitrogen (N). Loss-of- function mutants of either OsNLP3 or OsNLP4 produced smaller panicles with reduced primary and secondary branches and fewer grains compared with wild type, whereas their overexpression plants showed the opposite phenotypes. Notably, the OsNLP3/4-regulated panicle architecture was positively correlated with N availability. OsNLP3/4 directly bind to the promoter of OsRFL and activate its expression to promote inflorescence meristem development. Furthermore, OsRFL activates OsMOC1 expression by binding to its promoter. Our findings reveal the novel N- responsive OsNLP3/4-OsRFL-OsMOC1 module that integrates N availability to regulate panicle architecture, shedding light on how nutrient signals regulate panicle architecture and providing candidate targets for the improvement of crop yield."},{"title":"Combining high-resolution imaging, deep learning, and dynamic modelling to separate disease and senescence in wheat canopies","link":"http://biorxiv.org/cgi/content/short/2023.03.01.530609v1?rss=1","abstract":"Maintenance of sufficient healthy green leaf area after anthesis is key to ensuring an adequate assimilate supply for grain filling. Tightly regulated age-related physiological senescence and various biotic and abiotic stressors drive overall greenness decay dynamics under field conditions. Besides direct effects on green leaf area in terms of leaf damage, stressors often anticipate or accelerate physiological senescence, which may multiply their negative impact on grain filling. Here, we present an image processing methodology that enables the monitoring of chlorosis and necrosis separately for ears and shoots (stems + leaves) based on deep learning models for semantic segmentation and color properties of vegetation. A vegetation segmentation model was trained using semi-synthetic training data generated using image composition and generative adversarial neural networks, which greatly reduced the risk of annotation uncertainties and annotation effort. Application of the models to image time-series revealed temporal patterns of greenness decay as well as the relative contributions of chlorosis and necrosis. Image-based estimation of greenness decay dynamics was highly correlated with scoring-based estimations (r {approx} 0.9). Contrasting patterns were observed for plots with different levels of foliar diseases, particularly septoria tritici blotch. Our results suggest that tracking the chlorotic and necrotic fractions separately may enable (i) a separate quantification of the contribution of biotic stress and physiological senescence on overall green leaf area dynamics and (ii) investigation of the elusive interaction between biotic stress and physiological senescence. The potentially high-throughput nature of our methodology paves the way to conducting genetic studies of disease resistance and tolerance."},{"title":"A lack of population structure characterizes the invasive Lonicera japonica in West Virginia and across eastern North America","link":"http://biorxiv.org/cgi/content/short/2023.03.01.530604v1?rss=1","abstract":"Invasive plant species cause massive ecosystem damage globally, yet represent powerful case studies in population genetics and rapid adaptation to new habitats. The availability of digitized herbarium collections data, and the ubiquity of invasive species across the landscape make them highly accessible for studies of invasion history and population dynamics associated with their introduction, establishment, spread, and ecological interactions. Here we focus on Lonicera japonica, one of the most damaging invasive vine species in North America. We leveraged digitized collections data and contemporary field collections to reconstruct the invasion history and characterize patterns of genomic variation in the eastern USA, using a straightforward method for generating nucleotide polymorphism data and a recently published, chromosome-level genome for the species. We found an overall lack of population structure among sites in northern West Virginia, USA, as well as across sites in the central and eastern USA. Heterozygosity and population differentiation were both low based on Fst, analysis of molecular variance, principal components analysis, and cluster-based analyses. We also found evidence of high inbreeding coefficients and significant linkage disequilibrium, in line with the ability of this otherwise outcrossing, perennial species to propagate vegetatively. Our findings corroborate earlier studies based on allozyme data, and suggest that intentional, human-assisted spread explains the lack of population structure, as this species was planted for erosion control and as an ornamental, escaping cultivation repeatedly across the USA. Finally, we discuss how plant invasion genomics can be incorporated into experiential undergraduate education as a way to integrate teaching and research."},{"title":"Efficient transgene-free genome editing in plants in the T0 generation based on a co-editing strategy","link":"http://biorxiv.org/cgi/content/short/2023.03.02.530790v1?rss=1","abstract":"Transgene-free genome editing of plants in the T0 generation is highly desirable but challenging, especially in perennials and vegetatively propagated plants. Here, we investigated the co-editing strategy for generating transgene-free, gene-edited plants via Agrobacterium-mediated transient expression of cytosine base editor (CBE)/gRNA-Cas12a/crRNA-GFP in planta. Specifically, CBE/gRNA was used to base edit the ALS gene to confer resistance to herbicide chlorsulfuron as a selection marker, which has no negative effects on plant phenotypes; Cas12a/crRNA was used for editing genes(s) of interest; GFP was used for selecting transgene-free transformants. Using this approach, transgene-free genome-edited plants were efficiently generated for various genes (either individual or multiplex) in tomato, tobacco, potato, and citrus in the T0 generation. The biallelic/homozygous transgene-free mutation rates for target genes among herbicide-resistant transformants ranged from 8% to 50%. Whole genome sequencing further confirmed transgene-free and absence of off-target mutations in the edited plants. The co-editing strategy is efficient for generating transgene-free, genome-edited plants in the T0 generation, thus being a potent tool for plant genetic improvement."},{"title":"Canonical and ETT-mediated auxin signalling pathways synergistically promote gynoecium development in Arabidopsis thaliana","link":"http://biorxiv.org/cgi/content/short/2023.03.02.530771v1?rss=1","abstract":"Auxin is a phytohormone involved in a wide range of developmental processes. Whilst auxin-mediated signalling is now known to occur through a range of pathways, the transduction of auxin signalling within the carpels has mainly been studied in the context of the ETTIN (ETT) mediated pathway. Several lines of evidence suggest a role for the canonical TIR1/AFB2-mediated pathway in carpel development. However, the relationship between canonical and ETT-mediated mechanisms has not been investigated. Here, tir1-1 afb2-3 ett-3 triple mutants were generated in Arabidopsis, and phenotypic analysis showed synergism between the canonical and ETT-mediated pathways in gynoecium development. Furthermore, ETT shares partial redundancy with ARF4, a close paralog which interacts with the canonical pathway AUX/IAAs. However, the role of ARF4 in the regulation of auxin-responsive gene expression has not been studied in the gynoecium. Here comparative transcriptomics were utilized to assess the roles of ETT and ARF4 in gynoecium development, and the mis-regulation of the auxin response in ett-3 arf4-2 mutants. The data suggest that the auxin independent mis-regulation of YABBY2, YABBY5, and CRABSCLAW contributes to the abaxial-adaxial identity defects observed in gynoecia and leaves of the double mutant. Auxin sensitive transcriptomics analyses suggest that ETT and ARF4 maintain the auxin insensitivity of both distinct and overlapping targets, many of which become upregulated by auxin in their absence. This is consistent with previously proposed models of A-B ARF antagonism, and suggests that the maintenance of auxin insensitivity at target loci may play an equally important role as the auxin-sensitive induction of genes for a range of developmental processes."},{"title":"Biosynthesis of plant papanridins -A group of novel oligomeric flavonoids","link":"http://biorxiv.org/cgi/content/short/2023.03.01.530648v1?rss=1","abstract":"Discovery of novel flavonoids and their biosynthesis are fundamental to understand their roles in plants and benefits to human and animal health. Herein, we report a new polymerization pathway of a group of novel oligomeric flavonoids in plants. We have engineered red cells for discovering genes of interest involved in the flavonoid pathway and identified a gene that encodes a novel flavanol polymerase (FP) localized in the central vacuole. FP catalyzes the polymerization of flavanols, such as epicatechin and catechin, to produce yellowish dimers or oligomers. Structural elucidation show that these compounds are featured with a novel oligomeric flaven-flavan (FF) skeleton linked by interflavan-flaven and interflaven bonds, which are different from proanthocyanidins and dehydrodicatechins. Detailed chemical and physical characterizations further demonstrate that FFs are novel flavonoids. Mechanistic investigations show that FP polymerizes flavan-3-ols and flav-2-en-3-ol carbocation to form dimeric or oligomeric flaven-4[-&gt;]8-flavans, termed as papanridins. Data from transgenic, mutation, metabolic profiling, and phylogenetic analyses demonstrate that the biosynthesis of papanridins is prevalent in cacao, grape, blue berry, corn, rice, Arabidopsis and others in the plant kingdom. Given that these findings are the first report, many questions remain for answers. For instance, what are roles of papanridins in plants and what benefits do they have for human and animal health? We anticipate that these findings will promote investigations across plant, nutritional, and animal sciences to understand papanridins in plants and food products.  TeaserPlant flavanol polymerase catalyzes the biosynthesis of novel oligomeric flavonoids in the plant kingdom."},{"title":"Balanced nitrogen-iron nutrition boosts grain yield and nitrogen use efficiency in rice and wheat","link":"http://biorxiv.org/cgi/content/short/2023.02.28.530550v1?rss=1","abstract":"Nutrients must be balanced for optimal plant growth. However, the potential significance of balanced nitrogen-iron (N-Fe) for improving crop yield and nitrogen use efficiency (NUE) has never been addressed. Here, we show that balanced N-Fe substantially increases tiller number and boosts yield and NUE in rice and wheat. NIN-like protein 4 plays a pivotal role in maintaining the N-Fe balance by coordinately regulating the expression of multiple genes in N and Fe metabolism and signaling. Moreover, we show that foliar spraying of balanced N-Fe at the tillering stage can effectively increase rice tillers, yield and NUE in the field, which can be reproduced with wheat. Our findings provide guidelines for innovative fertilization to reduce N fertilizer input and promote yield, thus benefitting environment-friendly and sustainable agriculture worldwide."},{"title":"Pleiotropic Regulatory Locus1 maintains actin microfilament integrity and concomitant cellular homeostasis facilitating root development in Arabidopsis","link":"http://biorxiv.org/cgi/content/short/2023.02.28.530538v1?rss=1","abstract":"Cell functions are based on integrity of actin filaments. The Actin cytoskeleton is typically the target but also the source of signals. An evolutionarily conserved WD-40 protein PRL1 (Pleiotropic Regulatory Locus1) in Arabidopsis was investigated with multilayer functions in development, innate immunity, alternative splicing activation, transcription regulation, genome maintenance, ubiquitination-based protein turnover et al., but the underlying mechanisms are undefined. Here, we show PRL1 maintains actin integrity and concomitant cellular homeostasis. To explore causes for developmental root defect, we found depolymerization of cortical actin cytoskeleton and ROS imbalance in prl1 mutant. Further, we revealed that actin de-polymerization was the fundamental cause and dominant to ROS imbalance (H2O2 and O2{middle dot}-) for retarded root of prl1; NAC085 was up-regulated by and cooperated with actin depolymerization to mediate to stele cell death. Moreover, we revealed stress-related differentially expressed genes and alternative splicing defects were mutually independent and were responses to actin depolymerization in prl1. Our work ravels out cause-effect relationships between actin configuration and downstream hierarchical signals and explores underlying mechanism for functions of PRL1."},{"title":"PRL1 negatively regulates Rho GTPase-independent and -dependent signaling pathways maintaining actin microfilament dynamic for pavement cell morphogenesis","link":"http://biorxiv.org/cgi/content/short/2023.02.28.530536v1?rss=1","abstract":"Actin dynamic is critical for cell morphogenesis in plants, but the signaling mechanisms underlying its regulation are not well understood. Here we found PRL1 (Pleiotropic Regulatory Locus1) modulates leaf pavement cell (PC) morphogenesis in Arabidopsis by maintaining the dynamic homeostasis of actin microfilaments (MF). Our previous studies indicated PC shape formation was mediated by the counteracting ROP2 and ROP6 signaling pathways that promote the organization of cortical MF and microtubules (MT), respectively. Our genetic screen for ROP6 enhancers identified prl1 alleles. Genetic analysis suggested that prl1 acted synergistically with ROP2 and ROP6 in regulation of PC morphogenesis. We further found that the activities of ROP2 and ROP6 were increased and decreased in prl1 mutants, respectively. Interestingly prl1 was found to prefer to depolymerize MF independent of ROP2 and ROP6. Stress (high salinity and low temperature) induced similar changes of ROP activities as do prl1 mutations. Together our findings provided evidence that PRL1 governed two signaling pathways that counteractively maintain actin dynamics and resultant cell morphogenesis."},{"title":"OsNLP4-OsD3 module integrates nitrogen-iron nutrient signals to promote rice tillering by repressing strigolactone signaling","link":"http://biorxiv.org/cgi/content/short/2023.02.28.530551v1?rss=1","abstract":"Rice tillers are a major yield component regulated by phytohormones and nutrients. How nutrients interact with phytohormones to control tillering remains largely elusive. Here, we report a novel mechanism by which the transcription factor NIN-like protein 4 (OsNLP4) integrates nitrogen (N)-iron (Fe) nutrient signals to promote tillering by repressing OsD3 in strigolactone (SL) signaling. We show that the N-Fe balance modulates OsNLP4 nuclear accumulation, which is increased by Fe through H2O2 reduction. Furthermore, OsNLP4 upregulates multiple H2O2 scavenging genes, providing a positive regulatory loop for OsNLP4 nuclear accumulation. Our findings uncover a fundamental mechanism by which the OsNLP4-OsD3 module integrates N-Fe nutrient signals to downregulate SL signaling and thereby promote rice tillering and yield, thus facilitating sustainable agriculture worldwide."},{"title":"Importance of genetic architecture in marker selection decisions for genomic prediction","link":"http://biorxiv.org/cgi/content/short/2023.02.28.530521v1?rss=1","abstract":"Breeders commonly use genetic markers to predict the performance of untested individuals as a way to improve the efficiency of breeding programs. These genomic prediction models have almost exclusively used single nucleotide polymorphisms (SNPs) as their source of genetic information, even though other types of markers exist, such as structural variants (SVs). Given that SVs are associated with environmental adaptation and not all of them are in linkage disequilibrium to SNPs, SVs have the potential to bring additional information to multi-environment prediction models that are not captured by SNPs alone. Here, we evaluated different marker types (SNPs and/or SVs) on prediction accuracy across a range of genetic architectures for simulated traits across multiple environments. Our results show that SVs can improve prediction accuracy by up to 19%, but it is highly dependent on the genetic architecture of the trait. Differences in prediction accuracy across marker types were more pronounced for traits with high heritability, high number of QTLs, and SVs as causative variants. In these scenarios, using SV markers resulted in better prediction accuracies than SNP markers, especially when predicting untested genotypes across environments, likely due to more predictors being in linkage disequilibrium with causative variants. The simulations revealed little impact of different effect sizes between SNPs and SVs as causative variants on prediction accuracy. This study demonstrates the importance of knowing the genetic architecture of a trait in deciding what markers and marker types to use in large scale genomic prediction modeling in a breeding program.  Key messageWe demonstrate potential for improved multi-environment genomic prediction accuracy using structural variant markers. However, the degree of observed improvement is highly dependent on the genetic architecture of the trait."},{"title":"Agronomic treatments combined with embryo rescue for rapid generation advancement in tomato speed breeding","link":"http://biorxiv.org/cgi/content/short/2023.02.28.530438v1?rss=1","abstract":"Unlike other major crops, little research has been performed on tomato for reducing generation time for speed breeding. We evaluated several agronomic treatments for reducing the generation time of tomato in the M82 (determinate) and Moneymaker (indeterminate) varieties and evaluated the best combination in conjunction with embryo rescue. In a first experiment under the autumn cycle, five container sizes, from 0.2 1 (XS) to 6 1 (XL), were evaluated. We found that plants from the XL containers exhibited better development and required less time from sowing to anthesis (DSA) and for anthesis to fruit ripening (DAR). In a second experiment, using XL containers in the autumn-winter cycle, we evaluated cold priming at the cotyledonary stage, water stress, P supplementation, and K supplementation on generation time. We found that, compared to the control, cold priming significantly reduced the number of leaves and plant height to first inflorescence as well as DSA (2.7 d), while K supplementation reduced DAR (8.8 d). No effects of these treatments were observed for other growth of physiological traits. In a third experiment with XL containers in the spring-summer cycle, the combination of cold priming plus K supplementation was tested, confirming the significant effect of the combination on generation time (2.9 d for DSA and 3.9 d for DAR). Embryo rescue during the cell expansion cycle (average of 22.0 d and 23.3 d after anthesis for M82 and Moneymaker, respectively) allowed shortening the generation time by 8.7 d in M82 and 11.6 d in Moneymaker compared to the in planta fruit ripening. The combination of agronomic treatments with embryo rescue can make an effective contribution to increase the number of generations per year for speed breeding in tomato from the current three to four."},{"title":"Aging seeds of weedy broomrapes and witchweeds lose sensitivity to strigolactones as DNA demethylates","link":"http://biorxiv.org/cgi/content/short/2023.02.27.530112v1?rss=1","abstract":"Broomrapes (Phelipanche and Orobanche spp.) and witchweeds (Striga and Alectra spp.) are obligate root parasitic weeds responsible for major crop yield losses worldwide. Their success in agricultural landscapes is attributable to their ability to produce thousands of long-lived minute seeds that coordinate their germination with the presence of nearby hosts by perceiving host-derived strigolactones. Nevertheless, the processes underlying the alleged decade(s)-long persistence in the field are understudied. Using an accelerated seed aging method coupled to germination and ELISA bioassays, we report that the loss of seed viability and germinability along seed aging is accompanied by a decrease in both strigolactone sensitivity and global DNA methylation. Our results also suggest that seeds of broomrapes are longer-lived than those of witchweeds. Overall, this study deems to initiate further research into how epigenetic mechanisms contribute to alterations in seed viability in parasitic weeds, and how seed aging influence seed responses to their environment."},{"title":"Cautionary note on ribonuclease activity of recombinant PR-10 proteins","link":"http://biorxiv.org/cgi/content/short/2023.02.27.529914v1?rss=1","abstract":"We studied the biochemical properties of three splicing isoforms of PR-10 from rubber tree (Hevea brasiliensis) and found that purified recombinant HbPR10 can cause RNA degradation in vitro, a well-known activity described for many PR-10 proteins. This ribonuclease activity was observed for all three HbPR10 splicing isoforms and is abolished by boiling. However, inclusion of a negative control proteins revealed that ribonuclease activity rather originates from RNases that are copurified from E. coli, which are overlooked by traditionally used controls such as heat inactivation, RNase inhibitors and negative control proteins obtained with different procedures. The crucial control proteins are missing for at least nine reports on ribonuclease activity in PR-10 proteins published by different laboratories worldwide, indicating that proper controls are frequently overlooked in ribonuclease assays. The raised cautionary note applies to several PR-10 proteins with proclaimed ribonuclease activities and call for the use of different assays and mutant PR-10 proteins as control."},{"title":"Comparative analysis of SQUAMOSA PROMOTER BINDING PROTEIN-LIKE (SPL) gene family between bryophytes and seed plants","link":"http://biorxiv.org/cgi/content/short/2023.02.27.530190v1?rss=1","abstract":"SQUAMOSA-PROMOTER BINDING PROTEIN-LIKE (SPL) genes encode plant-specific transcription factors which have been found to be conserved in green plants lineage. SPL proteins are important regulators of diverse plant developmental processes in bryophytes and vascular plants. In our study, we took advantage of available genome sequences of representatives of each bryophyte clade to investigate the relationships of SPL genes between bryophytes and model angiosperm Arabidopsis thaliana. We have identified four SPL genes in each of the two hornworts species, Anthoceros agrestis and Anthoceros punctatus, what is similar to the set of SPL genes present in the liverwort Marchantia polymorpha. Thus, the analyzed hornworts and liverwort genomes encode a minimal set of SPL genes in comparison to other land plants that may resemble an archetype of SPL genes present in the ancestor of land plants. The phylogenetic analysis revealed the presence of four SPL groups. Comparative gene structure analysis showed that SPLs share similar exon-intron organization within the same phylogenetic group with some exceptions in hornworts. While we have identified conserved protein motifs between bryophytes and Arabidopsis in three out of four phylogenetic groups, the motif content differed explicitly in the fourth group. Since current understanding of SPL genes mostly arises from seed plants, the presented comparative and phylogenetic analysis will provide better understanding of SPL gene family from the representatives of the oldest living land plants."}]},{"name":"Economics","feed":[{"title":"Time-inconsistent contract theory.","link":"http://arxiv.org/abs/2303.01601","abstract":"This paper investigates the moral hazard problem in finite horizon with both continuous and lump-sum payments, involving a time-inconsistent sophisticated agent and a standard utility maximiser principal. Building upon the so-called dynamic programming approach in Cvitani\\'c, Possama\\\"i, and Touzi [18] and the recently available results in Hern\\'andez and Possama\\\"i [43], we present a methodology that covers the previous contracting problem. Our main contribution consists in a characterisation of the moral hazard problem faced by the principal. In particular, it shows that under relatively mild technical conditions on the data of the problem, the supremum of the principal's expected utility over a smaller restricted family of contracts is equal to the supremum over all feasible contracts. Nevertheless, this characterisation yields, as far as we know, a novel class of control problems that involve the control of a forward Volterra equation via Volterra-type controls, and infinite-dimensional stochastic target constraints. Despite the inherent challenges associated to such a problem, we study the solution under three different specifications of utility functions for both the agent and the principal, and draw qualitative implications from the form of the optimal contract. The general case remains the subject of future research."},{"title":"Revisiting the effect of search frictions on market concentration.","link":"http://arxiv.org/abs/2303.01824","abstract":"Search frictions can impede the formation of optimal matches between consumer and supplier, or employee and employer, and lead to inefficiencies. This paper revisits the effect of search frictions on the firm size distribution when challenging two common but strong assumptions: that all agents share the same ranking of firms, and that agents meet all firms, whether small or large, at the same rate. We build a random search model in which we relax those two assumptions and show that the intensity of search frictions has a non monotonic effect on market concentration. An increase in friction intensity increases market concentration up to a certain threshold of frictions, that depends on the slope of the meeting rate with respect to firm size. We leverage unique French customs data to estimate this slope. First, we find that in a range of plausible scenarios, search frictions intensity increases market concentration. Second, we show that slopes have increased over time, which unambiguously increases market concentration in our model. Overall, we shed light on the importance of the structure of frictions, rather than their intensity, to understand market concentration."},{"title":"Constructing High Frequency Economic Indicators by Imputation.","link":"http://arxiv.org/abs/2303.01863","abstract":"Monthly and weekly economic indicators are often taken to be the largest common factor estimated from high and low frequency data, either separately or jointly. To incorporate mixed frequency information without directly modeling them, we target a low frequency diffusion index that is already available, and treat high frequency values as missing. We impute these values using multiple factors estimated from the high frequency data. In the empirical examples considered, static matrix completion that does not account for serial correlation in the idiosyncratic errors yields imprecise estimates of the missing values irrespective of how the factors are estimated. Single equation and systems-based dynamic procedures yield imputed values that are closer to the observed ones. This is the case in the counterfactual exercise that imputes the monthly values of consumer sentiment series before 1978 when the data was released only on a quarterly basis. This is also the case for a weekly version of the CFNAI index of economic activity that is imputed using seasonally unadjusted data. The imputed series reveals episodes of increased variability of weekly economic information that are masked by the monthly data, notably around the 2014-15 collapse in oil prices."},{"title":"Fast Forecasting of Unstable Data Streams for On-Demand Service Platforms.","link":"http://arxiv.org/abs/2303.01887","abstract":"On-demand service platforms face a challenging problem of forecasting a large collection of high-frequency regional demand data streams that exhibit instabilities. This paper develops a novel forecast framework that is fast and scalable, and automatically assesses changing environments without human intervention. We empirically test our framework on a large-scale demand data set from a leading on-demand delivery platform in Europe, and find strong performance gains from using our framework against several industry benchmarks, across all geographical regions, loss functions, and both pre- and post-Covid periods. We translate forecast gains to economic impacts for this on-demand service platform by computing financial gains and reductions in computing costs."},{"title":"Finding the Optimal Currency Composition of Foreign Exchange Reserves with a Quantum Computer.","link":"http://arxiv.org/abs/2303.01909","abstract":"Portfolio optimization is an inseparable part of strategic asset allocation at the Czech National Bank. Quantum computing is a new technology offering algorithms for that problem. The capabilities and limitations of quantum computers with regard to portfolio optimization should therefore be investigated. In this paper, we focus on applications of quantum algorithms to dynamic portfolio optimization based on the Markowitz model. In particular, we compare algorithms for universal gate-based quantum computers (the QAOA, the VQE and Grover adaptive search), single-purpose quantum annealers, the classical exact branch and bound solver and classical heuristic algorithms (simulated annealing and genetic optimization). To run the quantum algorithms we use the IBM Quantum\\textsuperscript{TM} gate-based quantum computer. We also employ the quantum annealer offered by D-Wave. We demonstrate portfolio optimization on finding the optimal currency composition of the CNB's FX reserves. A secondary goal of the paper is to provide staff of central banks and other financial market regulators with literature on quantum optimization algorithms, because financial firms are active in finding possible applications of quantum computing."},{"title":"Trusting: Alone and together.","link":"http://arxiv.org/abs/2303.01921","abstract":"We study the problem of an agent continuously faced with the decision of placing or not placing trust in an institution. The agent makes use of Bayesian learning in order to estimate the institution's true trustworthiness and makes the decision to place trust based on myopic rationality. Using elements from random walk theory, we explicitly derive the probability that such an agent ceases placing trust at some point in the relationship, as well as the expected time spent placing trust conditioned on their discontinuation thereof.  We then continue by modelling two truster agents, each in their own relationship to the institution. We consider two natural models of communication between them. In the first (``observable rewards'') agents disclose their experiences with the institution with one another, while in the second (``observable actions'') agents merely witness the actions of their neighbour, i.e., placing or not placing trust. Under the same assumptions as in the single agent case, we describe the evolution of the beliefs of agents under these two different communication models. Both the probability of ceasing to place trust and the expected time in the system elude explicit expressions, despite there being only two agents. We therefore conduct a simulation study in order to compare the effect of the different kinds of communication on the trust dynamics.  We find that a pair of agents in both communication models has a greater chance of learning the true trustworthiness of an institution than a single agent. Communication between agents promotes the formation of long term trust with a trustworthy institution as well as the timely exit from a trust relationship with an untrustworthy institution. Contrary to what one might expect, we find that having less information (observing each other's actions instead of experiences) can sometimes be beneficial to the agents."},{"title":"The barriers to sustainable risk transfer in the cyber-insurance market.","link":"http://arxiv.org/abs/2303.02061","abstract":"Smooth risk transfer is a key condition for the development of an insurance market that is well-functioning and sustainable. The constantly evolving nature of cyber-threats and lack of public data sharing means the economic conditions required for quoted cyber-insurance premiums to be considered efficient are highly unlikely to be met. This paper develops Monte Carlo simulations of an artificial cyber-insurance market and compares the efficient and inefficient outcomes based on the informational setup between the market participants. The existence of diverse loss distributions is justified by the dynamic nature of cyber-threats and the absence of any reliable and centralised incident reporting. We show that the limited involvement of reinsurers when loss expectations are not shared leads to increased premia and lower overall capacity. This suggests that the sustainability of the cyber-insurance market requires both better data sharing and external sources of risk tolerant capital."},{"title":"Partially Linear Models under Data Combination.","link":"http://arxiv.org/abs/2204.05175","abstract":"We study partially linear models when the outcome of interest and some of the covariates are observed in two different datasets that cannot be linked. This type of data combination problem arises very frequently in empirical microeconomics. Using recent tools from optimal transport theory, we derive a constructive characterization of the sharp identified set. We then build on this result and develop a novel inference method that exploits the specific geometric properties of the identified set. Our method exhibits good performances in finite samples, while remaining very tractable. We apply our approach to study intergenerational income mobility over the period 1850-1930 in the United States. Our method allows us to relax the exclusion restrictions used in earlier work, while delivering confidence regions that are informative."},{"title":"Inference for Cluster Randomized Experiments with Non-ignorable Cluster Sizes.","link":"http://arxiv.org/abs/2204.08356","abstract":"This paper considers the problem of inference in cluster randomized experiments when cluster sizes are non-ignorable. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by non-ignorable cluster sizes we mean that \"large'' clusters and \"small'' clusters may be heterogeneous, and, in particular, the effects of the treatment may vary across clusters of differing sizes. In order to permit this sort of flexibility, we consider a sampling framework in which cluster sizes themselves are random. In this way, our analysis departs from earlier analyses of cluster randomized experiments in which cluster sizes are treated as non-random. We distinguish between two different parameters of interest: the equally-weighted cluster-level average treatment effect, and the size-weighted cluster-level average treatment effect. For each parameter, we provide methods for inference in an asymptotic framework where the number of clusters tends to infinity and treatment is assigned using a covariate-adaptive stratified randomization procedure. We additionally permit the experimenter to sample only a subset of the units within each cluster rather than the entire cluster and demonstrate the implications of such sampling for some commonly used estimators. A small simulation study and empirical demonstration show the practical relevance of our theoretical results."},{"title":"Asymptotic Properties of Endogeneity Corrections Using Nonlinear Transformations.","link":"http://arxiv.org/abs/2207.09246","abstract":"This paper considers a linear regression model with an endogenous regressor which arises from a nonlinear transformation of a latent variable. It is shown that the corresponding coefficient can be consistently estimated without external instruments by adding a rank-based transformation of the regressor to the model and performing standard OLS estimation. In contrast to other approaches, our nonparametric control function approach does not rely on a conformably specified copula. Furthermore, the approach allows for the presence of additional exogenous regressors which may be (linearly) correlated with the endogenous regressor(s). Consistency and asymptotic normality of the estimator are proved and the estimator is compared with copula based approaches by means of Monte Carlo simulations. An empirical application on wage data of the US current population survey demonstrates the usefulness of our method."},{"title":"The Role of Immigrants, Emigrants, and Locals in the Historical Formation of European Knowledge Agglomerations.","link":"http://arxiv.org/abs/2210.15914","abstract":"Did migrants help make Paris a Mecca for the arts and Vienna a beacon of classical music? Or was their rise a pure consequence of local actors? Here, we use data more than 22,000 famous historical individuals born between the years 1000 and 2000 to estimate the contribution of famous immigrants, emigrants, and locals to the knowledge specializations of European regions. We find that the probability that a region develops a specialization in a new activity (physics, philosophy, painting, etc.) grows with the presence of immigrants with knowledge on that activity and related activities, while the opposite holds for loosing existing specializations. In contrast, we do not find robust evidence that locals with related knowledge play a statistically significant role in entries or exits. We address some of the endogeneity concerns using highly restrictive fixed-effects models considering any location-period-activity specific factors (e.g. the presence of a new university attracting scientists)."},{"title":"Bicriteria Multidimensional Mechanism Design with Side Information.","link":"http://arxiv.org/abs/2302.14234","abstract":"We develop a versatile new methodology for multidimensional mechanism design that incorporates side information about agent types with the bicriteria goal of generating high social welfare and high revenue simultaneously. Side information can come from a variety of sources -- examples include advice from a domain expert, predictions from a machine-learning model trained on historical agent data, or even the mechanism designer's own gut instinct -- and in practice such sources are abundant. In this paper we adopt a prior-free perspective that makes no assumptions on the correctness, accuracy, or source of the side information. First, we design a meta-mechanism that integrates input side information with an improvement of the classical VCG mechanism. The welfare, revenue, and incentive properties of our meta-mechanism are characterized by a number of novel constructions we introduce based on the notion of a weakest competitor, which is an agent that has the smallest impact on welfare. We then show that our meta-mechanism -- when carefully instantiated -- simultaneously achieves strong welfare and revenue guarantees that are parameterized by errors in the side information. When the side information is highly informative and accurate, our mechanism achieves welfare and revenue competitive with the total social surplus, and its performance decays continuously and gradually as the quality of the side information decreases. Finally, we apply our meta-mechanism to a setting where each agent's type is determined by a constant number of parameters. Specifically, agent types lie on constant-dimensional subspaces (of the potentially high-dimensional ambient type space) that are known to the mechanism designer. We use our meta-mechanism to obtain the first known welfare and revenue guarantees in this setting."}]}]