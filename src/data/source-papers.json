[{"name":"Artificial Intelligence","feed":[{"id":"2403.03996","slug":"rethinking-urban-flood-risk-assessment-by-adapting-health-domain-perspective","title":"Rethinking Urban Flood Risk Assessment By Adapting Health Domain Perspective","link":"https://arxiv.org/abs/2403.03996","abstract":"Abstract: Inspired by ideas from health risk assessment, this paper presents a new perspective for flood risk assessment. The proposed perspective focuses on three pillars for examining flood risk: (1) inherent susceptibility, (2) mitigation strategies, and (3) external stressors. These pillars collectively encompass the physical and environmental characteristics of urban areas, the effectiveness of human-intervention measures, and the influence of uncontrollable external factors, offering a fresh point of view for decoding flood risks. For each pillar, we delineate its individual contributions to flood risk and illustrate their interactive and overall impact. The three-pillars model embodies a shift in focus from the quest to precisely model and quantify flood risk to evaluating pathways to high flood risk. The shift in perspective is intended to alleviate the quest for quantifying and predicting flood risk at fine resolutions as a panacea for enhanced flood risk management. The decomposition of flood risk pathways into the three intertwined pillars (i.e., inherent factors, mitigation factors, and external factors) enables evaluation of changes in factors within each pillar enhance and exacerbate flood risk, creating a platform from which to inform plans, decisions, and actions. Building on this foundation, we argue that a flood risk pathway analysis approach, which examines the individual and collective impacts of inherent factors, mitigation strategies, and external stressors, is essential for a nuanced evaluation of flood risk. Accordingly, the proposed perspective could complement the existing frameworks and approaches for flood risk assessment.","creator":"Zhewei Liu, Kai Yin, Ali Mostafavi"},{"id":"2403.03997","slug":"guiding-enumerative-program-synthesis-with-large-language-models","title":"Guiding Enumerative Program Synthesis with Large Language Models","link":"https://arxiv.org/abs/2403.03997","abstract":"Abstract: Pre-trained Large Language Models (LLMs) are beginning to dominate the discourse around automatic code generation with natural language specifications. In contrast, the best-performing synthesizers in the domain of formal synthesis with precise logical specifications are still based on enumerative algorithms. In this paper, we evaluate the abilities of LLMs to solve formal synthesis benchmarks by carefully crafting a library of prompts for the domain. When one-shot synthesis fails, we propose a novel enumerative synthesis algorithm, which integrates calls to an LLM into a weighted probabilistic search. This allows the synthesizer to provide the LLM with information about the progress of the enumerator, and the LLM to provide the enumerator with syntactic guidance in an iterative loop. We evaluate our techniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition. We find that GPT-3.5 as a stand-alone tool for formal synthesis is easily outperformed by state-of-the-art formal synthesis algorithms, but our approach integrating the LLM into an enumerative synthesis algorithm shows significant performance gains over both the LLM and the enumerative synthesizer alone and the winning SyGuS competition tool.","creator":"Yixuan Li, Julian Parsert, Elizabeth Polgreen"},{"id":"2403.04017","slug":"learning-guided-automated-reasoning-a-brief-survey","title":"Learning Guided Automated Reasoning: A Brief Survey","link":"https://arxiv.org/abs/2403.04017","abstract":"Abstract: Automated theorem provers and formal proof assistants are general reasoning systems that are in theory capable of proving arbitrarily hard theorems, thus solving arbitrary problems reducible to mathematics and logical reasoning. In practice, such systems however face large combinatorial explosion, and therefore include many heuristics and choice points that considerably influence their performance. This is an opportunity for trained machine learning predictors, which can guide the work of such reasoning systems. Conversely, deductive search supported by the notion of logically valid proof allows one to train machine learning systems on large reasoning corpora. Such bodies of proof are usually correct by construction and when combined with more and more precise trained guidance they can be boostrapped into very large corpora, with increasingly long reasoning chains and possibly novel proof ideas. In this paper we provide an overview of several automated reasoning and theorem proving domains and the learning and AI methods that have been so far developed for them. These include premise selection, proof guidance in several settings, AI systems and feedback loops iterating between reasoning and learning, and symbolic classification problems.","creator":"Lasse Blaauwbroek, David Cerna, Thibault Gauthier, Jan Jakub\\r{u}v, Cezary Kaliszyk, Martin Suda, Josef Urban"},{"id":"2403.04035","slug":"personalizing-explanations-of-ai-driven-hints-to-users-cognitive-abilities-an-empirical-evaluation","title":"Personalizing explanations of AI-driven hints to users cognitive abilities: an empirical evaluation","link":"https://arxiv.org/abs/2403.04035","abstract":"Abstract: We investigate personalizing the explanations that an Intelligent Tutoring System generates to justify the hints it provides to students to foster their learning. The personalization targets students with low levels of two traits, Need for Cognition and Conscientiousness, and aims to enhance these students' engagement with the explanations, based on prior findings that these students do not naturally engage with the explanations but they would benefit from them if they do. To evaluate the effectiveness of the personalization, we conducted a user study where we found that our proposed personalization significantly increases our target users' interaction with the hint explanations, their understanding of the hints and their learning. Hence, this work provides valuable insights into effectively personalizing AI-driven explanations for cognitively demanding tasks such as learning.","creator":"Vedant Bahel, Harshinee Sriram, Cristina Conati"},{"id":"2403.04072","slug":"forecasting-and-mitigating-disruptions-in-public-bus-transit-services","title":"Forecasting and Mitigating Disruptions in Public Bus Transit Services","link":"https://arxiv.org/abs/2403.04072","abstract":"Abstract: Public transportation systems often suffer from unexpected fluctuations in demand and disruptions, such as mechanical failures and medical emergencies. These fluctuations and disruptions lead to delays and overcrowding, which are detrimental to the passengers' experience and to the overall performance of the transit service. To proactively mitigate such events, many transit agencies station substitute (reserve) vehicles throughout their service areas, which they can dispatch to augment or replace vehicles on routes that suffer overcrowding or disruption. However, determining the optimal locations where substitute vehicles should be stationed is a challenging problem due to the inherent randomness of disruptions and due to the combinatorial nature of selecting locations across a city. In collaboration with the transit agency of Nashville, TN, we address this problem by introducing data-driven statistical and machine-learning models for forecasting disruptions and an effective randomized local-search algorithm for selecting locations where substitute vehicles are to be stationed. Our research demonstrates promising results in proactive disruption management, offering a practical and easily implementable solution for transit agencies to enhance the reliability of their services. Our results resonate beyond mere operational efficiency: by advancing proactive strategies, our approach fosters more resilient and accessible public transportation, contributing to equitable urban mobility and ultimately benefiting the communities that rely on public transportation the most.","creator":"Chaeeun Han, Jose Paolo Talusan, Dan Freudberg, Ayan Mukhopadhyay, Abhishek Dubey, Aron Laszka"},{"id":"2403.04087","slug":"the-cognitive-type-project-mapping-typography-to-cognition","title":"The Cognitive Type Project -- Mapping Typography to Cognition","link":"https://arxiv.org/abs/2403.04087","abstract":"Abstract: The Cognitive Type Project is focused on developing computational tools to enable the design of typefaces with varying cognitive properties. This initiative aims to empower typographers to craft fonts that enhance click-through rates for online ads, improve reading levels in children's books, enable dyslexics to create personalized type, or provide insights into customer reactions to textual content in media. A significant challenge in research related to mapping typography to cognition is the creation of thousands of typefaces with minor variations, a process that is both labor-intensive and requires the expertise of skilled typographers. Cognitive science research highlights that the design and form of letters, along with the text's overall layout, are crucial in determining the ease of reading and other cognitive properties of type such as perceived beauty and memorability. These factors affect not only the legibility and clarity of information presentation but also the likability of a typeface.","creator":"Nik Bear Brown"},{"id":"2403.04105","slug":"artificial-intelligence-exploring-the-patent-field","title":"Artificial Intelligence Exploring the Patent Field","link":"https://arxiv.org/abs/2403.04105","abstract":"Abstract: Advanced language-processing and machine-learning techniques promise massive efficiency improvements in the previously widely manual field of patent and technical knowledge management. This field presents large-scale and complex data with very precise contents and language representation of those contents. Particularly, patent texts can differ from mundane texts in various aspects, which entails significant opportunities and challenges. This paper presents a systematic overview of patent-related tasks and popular methodologies with a special focus on evolving and promising techniques. Language processing and particularly large language models as well as the recent boost of general generative methods promise to become game changers in the patent field. The patent literature and the fact-based argumentative procedures around patents appear almost as an ideal use case. However, patents entail a number of difficulties with which existing models struggle. The paper introduces fundamental aspects of patents and patent-related data that affect technology that wants to explore or manage them. It further reviews existing methods and approaches and points out how important reliable and unbiased evaluation metrics become. Although research has made substantial progress on certain tasks, the performance across many others remains suboptimal, sometimes because of either the special nature of patents and their language or inconsistencies between legal terms and the everyday meaning of terms. Moreover, yet few methods have demonstrated the ability to produce satisfactory text for specific sections of patents. By pointing out key developments, opportunities, and gaps, we aim to encourage further research and accelerate the advancement of this field.","creator":"Lekang Jiang, Stephan Goetz"},{"id":"2403.04106","slug":"understanding-biology-in-the-age-of-artificial-intelligence","title":"Understanding Biology in the Age of Artificial Intelligence","link":"https://arxiv.org/abs/2403.04106","abstract":"Abstract: Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models. Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge. We propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems. Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery. Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems.","creator":"Elsa Lawrence, Adham El-Shazly, Srijit Seal, Chaitanya K Joshi, Pietro Li\\`o, Shantanu Singh, Andreas Bender, Pietro Sormanni, Matthew Greenig"},{"id":"2403.04121","slug":"can-large-language-models-reason-and-plan","title":"Can Large Language Models Reason and Plan?","link":"https://arxiv.org/abs/2403.04121","abstract":"Abstract: While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.","creator":"Subbarao Kambhampati"},{"id":"2403.04124","slug":"privacy-preserving-fine-tuning-of-large-language-models-through-flatness","title":"Privacy-preserving Fine-tuning of Large Language Models through Flatness","link":"https://arxiv.org/abs/2403.04124","abstract":"Abstract: The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT. Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation. Our paper reveals that the flatness of DP-trained models' loss landscape plays an essential role in the trade-off between their privacy and generalization. We further propose a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation. It innovates from three coarse-to-grained levels, including perturbation-aware min-max optimization on model weights within a layer, flatness-guided sparse prefix-tuning on weights across layers, and weight knowledge distillation between DP \\& non-DP weights copies. Comprehensive experiments of both black-box and white-box scenarios are conducted to demonstrate the effectiveness of our proposal in enhancing generalization and maintaining DP characteristics. For instance, on text classification dataset QNLI, DP-Flat achieves similar performance with non-private full fine-tuning but with DP guarantee under privacy budget $\\epsilon=3$, and even better performance given higher privacy budgets. Codes are provided in the supplement.","creator":"Tiejin Chen, Longchao Da, Huixue Zhou, Pingzhi Li, Kaixiong Zhou, Tianlong Chen, Hua Wei"},{"id":"2403.04132","slug":"chatbot-arena-an-open-platform-for-evaluating-llms-by-human-preference","title":"Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference","link":"https://arxiv.org/abs/2403.04132","abstract":"Abstract: Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{https://chat.lmsys.org}.","creator":"Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, Ion Stoica"},{"id":"2403.04135","slug":"unsupervised-learning-of-harmonic-analysis-based-on-neural-hsmm-with-code-quality-templates","title":"Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with Code Quality Templates","link":"https://arxiv.org/abs/2403.04135","abstract":"Abstract: This paper presents a method of unsupervised learning of harmonic analysis based on a hidden semi-Markov model (HSMM). We introduce the chord quality templates, which specify the probability of pitch class emissions given a root note and a chord quality. Other probability distributions that comprise the HSMM are automatically learned via unsupervised learning, which has been a challenge in existing research. The results of the harmonic analysis of the proposed model were evaluated using existing labeled data. While our proposed method has yet to perform as well as existing models that used supervised learning and complex rule design, it has the advantage of not requiring expensive labeled data or rule elaboration. Furthermore, we also show how to recognize the tonic without prior knowledge, based on the transition probabilities of the Markov model.","creator":"Yui Uehara"},{"id":"2403.04140","slug":"contrastive-augmented-graph2graph-memory-interaction-for-few-shot-continual-learning","title":"Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning","link":"https://arxiv.org/abs/2403.04140","abstract":"Abstract: Few-Shot Class-Incremental Learning (FSCIL) has gained considerable attention in recent years for its pivotal role in addressing continuously arriving classes. However, it encounters additional challenges. The scarcity of samples in new sessions intensifies overfitting, causing incompatibility between the output features of new and old classes, thereby escalating catastrophic forgetting. A prevalent strategy involves mitigating catastrophic forgetting through the Explicit Memory (EM), which comprise of class prototypes. However, current EM-based methods retrieves memory globally by performing Vector-to-Vector (V2V) interaction between features corresponding to the input and prototypes stored in EM, neglecting the geometric structure of local features. This hinders the accurate modeling of their positional relationships. To incorporate information of local geometric structure, we extend the V2V interaction to Graph-to-Graph (G2G) interaction. For enhancing local structures for better G2G alignment and the prevention of local feature collapse, we propose the Local Graph Preservation (LGP) mechanism. Additionally, to address sample scarcity in classes from new sessions, the Contrast-Augmented G2G (CAG2G) is introduced to promote the aggregation of same class features thus helps few-shot learning. Extensive comparisons on CIFAR100, CUB200, and the challenging ImageNet-R dataset demonstrate the superiority of our method over existing methods.","creator":"Biqing Qi, Junqi Gao, Xingquan Chen, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou"},{"id":"2403.04204","slug":"on-the-essence-and-prospect-an-investigation-of-alignment-approaches-for-big-models","title":"On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models","link":"https://arxiv.org/abs/2403.04204","abstract":"Abstract: Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns. Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values. Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and demonstrate their intrinsic connections, strengths, and limitations, helping readers better understand this research area. In addition, two emerging topics, personal alignment, and multimodal alignment, are also discussed as novel frontiers in this field. Looking forward, we discuss potential alignment paradigms and how they could handle remaining challenges, prospecting where future alignment will go.","creator":"Xinpeng Wang, Shitong Duan, Xiaoyuan Yi, Jing Yao, Shanlin Zhou, Zhihua Wei, Peng Zhang, Dongkuan Xu, Maosong Sun, Xing Xie"},{"id":"2403.04261","slug":"advancing-biomedical-text-mining-with-community-challenges","title":"Advancing Biomedical Text Mining with Community Challenges","link":"https://arxiv.org/abs/2403.04261","abstract":"Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firstly, we collect the information of these evaluation tasks, such as data sources and task types. Secondly, we conduct systematic summary and comparative analysis, including named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation. Then, we summarize the potential clinical applications of these community challenge tasks from translational informatics perspective. Finally, we discuss the contributions and limitations of these community challenges, while highlighting future directions in the era of large language models.","creator":"Hui Zong, Rongrong Wu, Jiaxue Cha, Erman Wu, Jiakun Li, Liang Tao, Zuofeng Li, Buzhou Tang, Bairong Shen"},{"id":"2403.04264","slug":"competitive-facility-location-under-random-utilities-and-routing-constraints","title":"Competitive Facility Location under Random Utilities and Routing Constraints","link":"https://arxiv.org/abs/2403.04264","abstract":"Abstract: In this paper, we study a facility location problem within a competitive market context, where customer demand is predicted by a random utility choice model. Unlike prior research, which primarily focuses on simple constraints such as a cardinality constraint on the number of selected locations, we introduce routing constraints that necessitate the selection of locations in a manner that guarantees the existence of a tour visiting all chosen locations while adhering to a specified tour length upper bound. Such routing constraints find crucial applications in various real-world scenarios. The problem at hand features a non-linear objective function, resulting from the utilization of random utilities, together with complex routing constraints, making it computationally challenging. To tackle this problem, we explore three types of valid cuts, namely, outer-approximation and submodular cuts to handle the nonlinear objective function, as well as sub-tour elimination cuts to address the complex routing constraints. These lead to the development of two exact solution methods: a nested cutting plane and nested branch-and-cut algorithms, where these valid cuts are iteratively added to a master problem through two nested loops. We also prove that our nested cutting plane method always converges to optimality after a finite number of iterations. Furthermore, we develop a local search-based metaheuristic tailored for solving large-scale instances and show its pros and cons compared to exact methods. Extensive experiments are conducted on problem instances of varying sizes, demonstrating that our approach excels in terms of solution quality and computation time when compared to other baseline approaches.","creator":"Hoang Giang Pham, Tien Thanh Dam, Ngan Ha Duong, Tien Mai, Minh Hoang Ha"},{"id":"2403.04280","slug":"a-new-benchmark-for-evaluating-automatic-speech-recognition-in-the-arabic-call-domain","title":"A New Benchmark for Evaluating Automatic Speech Recognition in the Arabic Call Domain","link":"https://arxiv.org/abs/2403.04280","abstract":"Abstract: This work is an attempt to introduce a comprehensive benchmark for Arabic speech recognition, specifically tailored to address the challenges of telephone conversations in Arabic language. Arabic, characterized by its rich dialectal diversity and phonetic complexity, presents a number of unique challenges for automatic speech recognition (ASR) systems. These challenges are further amplified in the domain of telephone calls, where audio quality, background noise, and conversational speech styles negatively affect recognition accuracy. Our work aims to establish a robust benchmark that not only encompasses the broad spectrum of Arabic dialects but also emulates the real-world conditions of call-based communications. By incorporating diverse dialectical expressions and accounting for the variable quality of call recordings, this benchmark seeks to provide a rigorous testing ground for the development and evaluation of ASR systems capable of navigating the complexities of Arabic speech in telephonic contexts. This work also attempts to establish a baseline performance evaluation using state-of-the-art ASR technologies.","creator":"Qusai Abo Obaidah, Muhy Eddin Zater, Adnan Jaljuli, Ali Mahboub, Asma Hakouz, Bashar Alfrou, Yazan Estaitia"},{"id":"2403.04292","slug":"a-challenge-in-a-g-i-cybernetics-revived-in-the-ouroboros-model-as-one-algorithm-for-all-thinking","title":"A challenge in A(G)I, cybernetics revived in the Ouroboros Model as one algorithm for all thinking","link":"https://arxiv.org/abs/2403.04292","abstract":"Abstract: A topical challenge for algorithms in general and for automatic image categorization and generation in particular is presented in the form of a drawing for AI to understand. In a second vein, AI is challenged to produce something similar from verbal description. The aim of the paper is to highlight strengths and deficiencies of current Artificial Intelligence approaches while coarsely sketching a way forward. A general lack of encompassing symbol-embedding and (not only) -grounding in some bodily basis is made responsible for current deficiencies. A concomitant dearth of hierarchical organization of concepts follows suite. As a remedy for these shortcomings, it is proposed to take a wide step back and to newly incorporate aspects of cybernetics and analog control processes. It is claimed that a promising overarching perspective is provided by the Ouroboros Model with a valid and versatile algorithmic backbone for general cognition at all accessible levels of abstraction and capabilities. Reality, rules, truth, and Free Will are all useful abstractions according to the Ouroboros Model. Logic deduction as well as intuitive guesses are claimed as produced on the basis of one compartmentalized memory for schemata and a pattern-matching, i.e., monitoring process termed consumption analysis. The latter directs attention on short (attention proper) and also on long times scales (emotional biases). In this cybernetic approach, discrepancies between expectations and actual activations (e.g., sensory precepts) drive the general process of cognition and at the same time steer the storage of new and adapted memory entries. Dedicated structures in the human brain work in concert according to this scheme.","creator":"Knud Thomsen"},{"id":"2403.04293","slug":"mkf-ads-a-multi-knowledge-fused-anomaly-detection-system-for-automotive","title":"MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive","link":"https://arxiv.org/abs/2403.04293","abstract":"Abstract: With the requirements of Intelligent Transport Systems (ITSs) for extensive connectivity of Electronic Control Units (ECUs) to the outside world, safety and security have become stringent problems. Intrusion detection systems (IDSs) are a crucial safety component in remediating Controller Area Network (CAN) bus vulnerabilities. However, supervised-based IDSs fail to identify complexity attacks and anomaly-based IDSs have higher false alarms owing to capability bottleneck. In this paper, we propose a novel multi-knowledge fused anomaly detection model, called MKF-IDS. Specifically, the method designs an integration framework, including spatial-temporal correlation with an attention mechanism (STcAM) module and patch sparse-transformer module (PatchST). The STcAM with fine-pruning uses one-dimensional convolution (Conv1D) to extract spatial features and subsequently utilizes the Bidirectional Long Short Term Memory (Bi-LSTM) to extract the temporal features, where the attention mechanism will focus on the important time steps. Meanwhile, the PatchST captures the combined long-time historical features from independent univariate time series. Finally, the proposed method is based on knowledge distillation to STcAM as a student model for learning intrinsic knowledge and cross the ability to mimic PatchST. In the detection phase, the MKF-ADS only deploys STcAM to maintain efficiency in a resource-limited IVN environment. Moreover, the redundant noisy signal is reduced with bit flip rate and boundary decision estimation. We conduct extensive experiments on six simulation attack scenarios across various CAN IDs and time steps, and two real attack scenarios, which present a competitive prediction and detection performance. Compared with the baseline in the same paradigm, the error rate and FAR are 2.62% and 2.41% and achieve a promising F1-score of 97.3%.","creator":"Pengzhou Cheng, Zongru Wu, Gongshen Liu"},{"id":"2403.04311","slug":"alto-an-efficient-network-orchestrator-for-compound-ai-systems","title":"ALTO: An Efficient Network Orchestrator for Compound AI Systems","link":"https://arxiv.org/abs/2403.04311","abstract":"Abstract: We present ALTO, a network orchestrator for efficiently serving compound AI systems such as pipelines of language models. ALTO achieves high throughput and low latency by taking advantage of an optimization opportunity specific to generative language models: streaming intermediate outputs. As language models produce outputs token by token, ALTO exposes opportunities to stream intermediate outputs between stages when possible. We highlight two new challenges of correctness and load balancing which emerge when streaming intermediate data across distributed pipeline stage instances. We also motivate the need for an aggregation-aware routing interface and distributed prompt-aware scheduling to address these challenges. We demonstrate the impact of ALTO's partial output streaming on a complex chatbot verification pipeline, increasing throughput by up to 3x for a fixed latency target of 4 seconds / request while also reducing tail latency by 1.8x compared to a baseline serving approach.","creator":"Keshav Santhanam, Deepti Raghavan, Muhammad Shahir Rahman, Thejas Venkatesh, Neha Kunjal, Pratiksha Thaker, Philip Levis, Matei Zaharia"},{"id":"2403.04343","slug":"cotbal-comprehensive-task-balancing-for-multi-task-visual-instruction-tuning","title":"CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning","link":"https://arxiv.org/abs/2403.04343","abstract":"Abstract: Visual instruction tuning is a key training stage of large multimodal models (LMMs). Nevertheless, the common practice of indiscriminately mixing instruction-following data from various tasks may result in suboptimal overall performance due to different instruction formats and knowledge domains across tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing (CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our knowledge, this is the first work that explores multi-task optimization in visual instruction tuning. Specifically, we consider two key dimensions for task balancing: (1) Inter-Task Contribution, the phenomenon where learning one task potentially enhances the performance in other tasks, attributable to the overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to the learning difficulty within a single task. By quantifying these two dimensions with performance-based metrics, task balancing is thus enabled by assigning more weights to tasks that offer substantial contributions to others, receive minimal contributions from others, and also have great intra-task difficulties. Experiments show that our CoTBal leads to superior overall performance in multi-task visual instruction tuning.","creator":"Yanqi Dai, Dong Jing, Nanyi Fei, Zhiwu Lu"},{"id":"2403.04366","slug":"enhancing-court-view-generation-with-knowledge-injection-and-guidance","title":"Enhancing Court View Generation with Knowledge Injection and Guidance","link":"https://arxiv.org/abs/2403.04366","abstract":"Abstract: Court View Generation (CVG) is a challenging task in the field of Legal Artificial Intelligence (LegalAI), which aims to generate court views based on the plaintiff claims and the fact descriptions. While Pretrained Language Models (PLMs) have showcased their prowess in natural language generation, their application to the complex, knowledge-intensive domain of CVG often reveals inherent limitations. In this paper, we present a novel approach, named Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To efficiently incorporate domain knowledge during the training stage, we introduce a knowledge-injected prompt encoder for prompt tuning, thereby reducing computational overhead. Moreover, to further enhance the model's ability to utilize domain knowledge, we employ a generating navigator, which dynamically guides the text generation process in the inference stage without altering the model's architecture, making it readily transferable. Comprehensive experiments on real-world data demonstrate the effectiveness of our approach compared to several established baselines, especially in the responsivity of claims, where it outperforms the best baseline by 11.87%.","creator":"Ang Li, Yiquan Wu, Yifei Liu, Fei Wu, Ming Cai, Kun Kuang"},{"id":"2403.04369","slug":"from-graph-to-word-bag-introducing-domain-knowledge-to-confusing-charge-prediction","title":"From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction","link":"https://arxiv.org/abs/2403.04369","abstract":"Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the model's attention towards the differentiating information for each charge within the context, we expand the attention mechanism and introduce a new loss function with attention supervision through words in the word bag. We construct the confusing charges dataset from real-world judicial documents. Experiments demonstrate the effectiveness of our method, especially in maintaining exceptional performance in imbalanced label distributions.","creator":"Ang Li, Qiangchao Chen, Yiquan Wu, Ming Cai, Xiang Zhou, Fei Wu, Kun Kuang"},{"id":"2403.04449","slug":"feedback-generation-for-programming-exercises-with-gpt-4","title":"Feedback-Generation for Programming Exercises With GPT-4","link":"https://arxiv.org/abs/2403.04449","abstract":"Abstract: Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For example, the output is more structured and consistent. GPT-4 Turbo can also accurately identify invalid casing in student programs' output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4.","creator":"Imen Azaiz, Natalie Kiesler, Sven Strickroth"},{"id":"2403.04471","slug":"the-shutdown-problem-three-theorems","title":"The Shutdown Problem: Three Theorems","link":"https://arxiv.org/abs/2403.04471","abstract":"Abstract: I explain the shutdown problem: the problem of designing artificial agents that (1) shut down when a shutdown button is pressed, (2) don't try to prevent or cause the pressing of the shutdown button, and (3) otherwise pursue goals competently. I prove three theorems that make the difficulty precise. These theorems show that agents satisfying some innocuous-seeming conditions will often try to prevent or cause the pressing of the shutdown button, even in cases where it's costly to do so. And patience trades off against shutdownability: the more patient an agent, the greater the costs that agent is willing to incur to manipulate the shutdown button. I end by noting that these theorems can guide our search for solutions.","creator":"Elliott Thornley"},{"id":"2403.04483","slug":"graphinstruct-empowering-large-language-models-with-graph-understanding-and-reasoning-capability","title":"GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability","link":"https://arxiv.org/abs/2403.04483","abstract":"Abstract: Evaluating and enhancing the general capabilities of large language models (LLMs) has been an important research topic. Graph is a common data structure in the real world, and understanding graph data is a crucial part for advancing general intelligence. To evaluate and enhance the graph understanding abilities of LLMs, in this paper, we propose a benchmark named GraphInstruct, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed reasoning steps. Based on GraphInstruct, we further construct GraphLM through efficient instruction-tuning, which shows prominent graph understanding capability. In order to enhance the LLM with graph reasoning capability as well, we propose a step mask training strategy, and construct a model named GraphLM+. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphLM and GraphLM+ over other LLMs. We look forward to more researchers exploring the potential of LLMs in the graph data mining domain through GraphInstruct. Our code for generating GraphInstruct is released publicly at: https://github.com/CGCL-codes/GraphInstruct.","creator":"Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin"},{"id":"2403.04504","slug":"improving-matrix-completion-by-exploiting-rating-ordinality-in-graph-neural-networks","title":"Improving Matrix Completion by Exploiting Rating Ordinality in Graph Neural Networks","link":"https://arxiv.org/abs/2403.04504","abstract":"Abstract: Matrix completion is an important area of research in recommender systems. Recent methods view a rating matrix as a user-item bi-partite graph with labeled edges denoting observed ratings and predict the edges between the user and item nodes by using the graph neural network (GNN). Despite their effectiveness, they treat each rating type as an independent relation type and thus cannot sufficiently consider the ordinal nature of the ratings. In this paper, we explore a new approach to exploit rating ordinality for GNN, which has not been studied well in the literature. We introduce a new method, called ROGMC, to leverage Rating Ordinality in GNN-based Matrix Completion. It uses cumulative preference propagation to directly incorporate rating ordinality in GNN's message passing, allowing for users' stronger preferences to be more emphasized based on inherent orders of rating types. This process is complemented by interest regularization which facilitates preference learning using the underlying interest information. Our extensive experiments show that ROGMC consistently outperforms the existing strategies of using rating types for GNN. We expect that our attempt to explore the feasibility of utilizing rating ordinality for GNN may stimulate further research in this direction.","creator":"Jaehyun Lee, Seonku Kang, Hwanjo Yu"},{"id":"2403.04511","slug":"uncovering-the-deep-filter-bubble-narrow-exposure-in-short-video-recommendation","title":"Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video Recommendation","link":"https://arxiv.org/abs/2403.04511","abstract":"Abstract: Filter bubbles have been studied extensively within the context of online content platforms due to their potential to cause undesirable outcomes such as user dissatisfaction or polarization. With the rise of short-video platforms, the filter bubble has been given extra attention because these platforms rely on an unprecedented use of the recommender system to provide relevant content. In our work, we investigate the deep filter bubble, which refers to the user being exposed to narrow content within their broad interests. We accomplish this using one-year interaction data from a top short-video platform in China, which includes hierarchical data with three levels of categories for each video. We formalize our definition of a \"deep\" filter bubble within this context, and then explore various correlations within the data: first understanding the evolution of the deep filter bubble over time, and later revealing some of the factors that give rise to this phenomenon, such as specific categories, user demographics, and feedback type. We observe that while the overall proportion of users in a filter bubble remains largely constant over time, the depth composition of their filter bubble changes. In addition, we find that some demographic groups that have a higher likelihood of seeing narrower content and implicit feedback signals can lead to less bubble formation. Finally, we propose some ways in which recommender systems can be designed to reduce the risk of a user getting caught in a bubble.","creator":"Nicholas Sukiennik, Chen Gao, Nian Li"},{"id":"2403.04541","slug":"towards-automatic-composition-of-asp-programs-from-natural-language-specifications","title":"Towards Automatic Composition of ASP Programs from Natural Language Specifications","link":"https://arxiv.org/abs/2403.04541","abstract":"Abstract: This paper moves the first step towards automating the composition of Answer Set Programming (ASP) specifications. In particular, the following contributions are provided: (i) A dataset focused on graph-related problem specifications, designed to develop and assess tools for ASP automatic coding; (ii) A two-step architecture, implemented in the NL2ASP tool, for generating ASP programs from natural language specifications. NL2ASP uses neural machine translation to transform natural language into Controlled Natural Language (CNL) statements. Subsequently, CNL statements are converted into ASP code using the CNL2ASP tool. An experiment confirms the viability of the approach.","creator":"Manuel Borroto, Irfan Kareem, Francesco Ricca"},{"id":"2403.04571","slug":"machine-learning-and-information-theory-concepts-towards-an-ai-mathematician","title":"Machine learning and information theory concepts towards an AI Mathematician","link":"https://arxiv.org/abs/2403.04571","abstract":"Abstract: The current state-of-the-art in artificial intelligence is impressive, especially in terms of mastery of language, but not so much in terms of mathematical reasoning. What could be missing? Can we learn something useful about that gap from how the brains of mathematicians go about their craft? This essay builds on the idea that current deep learning mostly succeeds at system 1 abilities -- which correspond to our intuition and habitual behaviors -- but still lacks something important regarding system 2 abilities -- which include reasoning and robust uncertainty estimation. It takes an information-theoretical posture to ask questions about what constitutes an interesting mathematical statement, which could guide future work in crafting an AI mathematician. The focus is not on proving a given theorem but on discovering new and interesting conjectures. The central hypothesis is that a desirable body of theorems better summarizes the set of all provable statements, for example by having a small description length while at the same time being close (in terms of number of derivation steps) to many provable statements.","creator":"Yoshua Bengio, Nikolay Malkin"},{"id":"2403.04577","slug":"wiki-tabner-advancing-table-interpretation-through-named-entity-recognition","title":"Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition","link":"https://arxiv.org/abs/2403.04577","abstract":"Abstract: Web tables contain a large amount of valuable knowledge and have inspired tabular language models aimed at tackling table interpretation (TI) tasks. In this paper, we analyse a widely used benchmark dataset for evaluation of TI tasks, particularly focusing on the entity linking task. Our analysis reveals that this dataset is overly simplified, potentially reducing its effectiveness for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To overcome this drawback, we construct and annotate a new more challenging dataset. In addition to introducing the new dataset, we also introduce a novel problem aimed at addressing the entity linking task: named entity recognition within cells. Finally, we propose a prompting framework for evaluating the newly developed large language models (LLMs) on this novel TI task. We conduct experiments on prompting LLMs under various settings, where we use both random and similarity-based selection to choose the examples presented to the models. Our ablation study helps us gain insights into the impact of the few-shot examples. Additionally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed dataset.","creator":"Aneta Koleva, Martin Ringsquandl, Ahmed Hatem, Thomas Runkler, Volker Tresp"},{"id":"2403.04588","slug":"zero-shot-cross-modal-transfer-of-reinforcement-learning-policies-through-a-global-workspace","title":"Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace","link":"https://arxiv.org/abs/2403.04588","abstract":"Abstract: Humans perceive the world through multiple senses, enabling them to create a comprehensive representation of their surroundings and to generalize information across domains. For instance, when a textual description of a scene is given, humans can mentally visualize it. In fields like robotics and Reinforcement Learning (RL), agents can also access information about the environment through multiple sensors; yet redundancy and complementarity between sensors is difficult to exploit as a source of robustness (e.g. against sensor failure) or generalization (e.g. transfer across domains). Prior research demonstrated that a robust and flexible multimodal representation can be efficiently constructed based on the cognitive science notion of a 'Global Workspace': a unique representation trained to combine information across modalities, and to broadcast its signal back to each modality. Here, we explore whether such a brain-inspired multimodal representation could be advantageous for RL agents. First, we train a 'Global Workspace' to exploit information collected about the environment via two input modalities (a visual input, or an attribute vector representing the state of the agent and/or its environment). Then, we train a RL agent policy using this frozen Global Workspace. In two distinct environments and tasks, our results reveal the model's ability to perform zero-shot cross-modal transfer between input modalities, i.e. to apply to image inputs a policy previously trained on attribute vectors (and vice-versa), without additional training or fine-tuning. Variants and ablations of the full Global Workspace (including a CLIP-like multimodal representation trained via contrastive learning) did not display the same generalization abilities.","creator":"L\\'eopold Mayti\\'e, Benjamin Devillers, Alexandre Arnold, Rufin VanRullen"},{"id":"2403.04667","slug":"the-social-impact-of-generative-ai-an-analysis-on-chatgpt","title":"The Social Impact of Generative AI: An Analysis on ChatGPT","link":"https://arxiv.org/abs/2403.04667","abstract":"Abstract: In recent months, the social impact of Artificial Intelligence (AI) has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a human-centered AI.","creator":"Maria T. Baldassarre, Danilo Caivano, Berenice Fernandez Nieto, Domenico Gigante, Azzurra Ragone"},{"id":"2403.04732","slug":"how-far-are-we-from-intelligent-visual-deductive-reasoning","title":"How Far Are We from Intelligent Visual Deductive Reasoning?","link":"https://arxiv.org/abs/2403.04732","abstract":"Abstract: Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated incredible strides on diverse vision language tasks. We dig into vision-based deductive reasoning, a more sophisticated but less explored realm, and find previously unexposed blindspots in the current SOTA VLMs. Specifically, we leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to perform multi-hop relational and deductive reasoning relying solely on visual clues. We perform comprehensive evaluations of several popular VLMs employing standard strategies such as in-context learning, self-consistency, and Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test, IntelligenceTest, and RAVEN. The results reveal that despite the impressive capabilities of LLMs in text-based reasoning, we are still far from achieving comparable proficiency in visual deductive reasoning. We found that certain standard strategies that are effective when applied to LLMs do not seamlessly translate to the challenges presented by visual reasoning tasks. Moreover, a detailed analysis reveals that VLMs struggle to solve these tasks mainly because they are unable to perceive and comprehend multiple, confounding abstract patterns in RPM examples.","creator":"Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly"},{"id":"2403.03962","slug":"identify-critical-nodes-in-complex-network-with-large-language-models","title":"Identify Critical Nodes in Complex Network with Large Language Models","link":"https://arxiv.org/abs/2403.03962","abstract":"arXiv:2403.03962v1 Announce Type: cross  Abstract: Identifying critical nodes in networks is a classical decision-making task, and many methods struggle to strike a balance between adaptability and utility. Therefore, we propose an approach that empowers Evolutionary Algorithm (EA) with Large Language Models (LLMs), to generate a function called \"score\\_nodes\" which can further be used to identify crucial nodes based on their assigned scores. Our model consists of three main components: Manual Initialization, Population Management, and LLMs-based Evolution. It evolves from initial populations with a set of designed node scoring functions created manually. LLMs leverage their strong contextual understanding and rich programming skills to perform crossover and mutation operations on the individuals, generating excellent new functions. These functions are then categorized, ranked, and eliminated to ensure the stable development of the populations while preserving diversity. Extensive experiments demonstrate the excellent performance of our method, showcasing its strong generalization ability compared to other state-of-the-art algorithms. It can consistently and orderly generate diverse and efficient node scoring functions. All source codes and models that can reproduce all results in this work are publicly available at this link: \\url{https://anonymous.4open.science/r/LLM4CN-6520}","creator":"Jinzhu Mao, Dongyun Zou, Li Sheng, Siyi Liu, Chen Gao, Yue Wang, Yong Li"},{"id":"2403.03993","slug":"personalized-negative-reservoir-for-incremental-learning-in-recommender-systems","title":"Personalized Negative Reservoir for Incremental Learning in Recommender Systems","link":"https://arxiv.org/abs/2403.03993","abstract":"arXiv:2403.03993v1 Announce Type: cross  Abstract: Recommender systems have become an integral part of online platforms. Every day the volume of training data is expanding and the number of user interactions is constantly increasing. The exploration of larger and more expressive models has become a necessary pursuit to improve user experience. However, this progression carries with it an increased computational burden. In commercial settings, once a recommendation system model has been trained and deployed it typically needs to be updated frequently as new client data arrive. Cumulatively, the mounting volume of data is guaranteed to eventually make full batch retraining of the model from scratch computationally infeasible. Naively fine-tuning solely on the new data runs into the well-documented problem of catastrophic forgetting. Despite the fact that negative sampling is a crucial part of training with implicit feedback, no specialized technique exists that is tailored to the incremental learning framework. In this work, we take the first step to propose, a personalized negative reservoir strategy which is used to obtain negative samples for the standard triplet loss. This technique balances alleviation of forgetting with plasticity by encouraging the model to remember stable user preferences and selectively forget when user interests change. We derive the mathematical formulation of a negative sampler to populate and update the reservoir. We integrate our design in three SOTA and commonly used incremental recommendation models. We show that these concrete realizations of our negative reservoir framework achieve state-of-the-art results in standard benchmarks, on multiple standard top-k evaluation metrics.","creator":"Antonios Valkanas, Yuening Wang, Yingxue Zhang, Mark Coates"},{"id":"2403.04001","slug":"bidirectional-progressive-neural-networks-with-episodic-return-progress-for-emergent-task-sequencing-and-robotic-skill-transfer","title":"Bidirectional Progressive Neural Networks with Episodic Return Progress for Emergent Task Sequencing and Robotic Skill Transfer","link":"https://arxiv.org/abs/2403.04001","abstract":"arXiv:2403.04001v1 Announce Type: cross  Abstract: Human brain and behavior provide a rich venue that can inspire novel control and learning methods for robotics. In an attempt to exemplify such a development by inspiring how humans acquire knowledge and transfer skills among tasks, we introduce a novel multi-task reinforcement learning framework named Episodic Return Progress with Bidirectional Progressive Neural Networks (ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved manner by (2) autonomous task switching based on a novel intrinsic motivation signal and, in contrast to existing methods, (3) allows bidirectional skill transfer among tasks. ERP-BPNN is a general architecture applicable to several multi-task learning settings; in this paper, we present the details of its neural architecture and show its ability to enable effective learning and skill transfer among morphologically different robots in a reaching task. The developed Bidirectional Progressive Neural Network (BPNN) architecture enables bidirectional skill transfer without requiring incremental training and seamlessly integrates with online task arbitration. The task arbitration mechanism developed is based on soft Episodic Return progress (ERP), a novel intrinsic motivation (IM) signal. To evaluate our method, we use quantifiable robotics metrics such as 'expected distance to goal' and 'path straightness' in addition to the usual reward-based measure of episodic return common in reinforcement learning. With simulation experiments, we show that ERP-BPNN achieves faster cumulative convergence and improves performance in all metrics considered among morphologically different robots compared to the baselines.","creator":"Suzan Ece Ada, Hanne Say, Emre Ugur, Erhan Oztop"},{"id":"2403.04014","slug":"promptcharm-text-to-image-generation-through-multi-modal-prompting-and-refinement","title":"PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement","link":"https://arxiv.org/abs/2403.04014","abstract":"arXiv:2403.04014v1 Announce Type: cross  Abstract: The recent advancements in Generative AI have significantly advanced the field of text-to-image generation. The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text prompts that align with the model's interpretation and the user's intent thus becomes crucial. However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user's initial prompt. Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database. To assist users in effectively refining their prompts and images, PromptCharm renders model explanations by visualizing the model's attention values. If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm. To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user's expectations compared with using two variants of PromptCharm that lacked interaction or visualization support.","creator":"Zhijie Wang, Yuheng Huang, Da Song, Lei Ma, Tianyi Zhang"},{"id":"2403.04015","slug":"knockoff-guided-feature-selection-via-a-single-pre-trained-reinforced-agent","title":"Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced Agent","link":"https://arxiv.org/abs/2403.04015","abstract":"arXiv:2403.04015v1 Announce Type: cross  Abstract: Feature selection prepares the AI-readiness of data by eliminating redundant features. Prior research falls into two primary categories: i) Supervised Feature Selection, which identifies the optimal feature subset based on their relevance to the target variable; ii) Unsupervised Feature Selection, which reduces the feature space dimensionality by capturing the essential information within the feature set instead of using target variable. However, SFS approaches suffer from time-consuming processes and limited generalizability due to the dependence on the target variable and downstream ML tasks. UFS methods are constrained by the deducted feature space is latent and untraceable. To address these challenges, we introduce an innovative framework for feature selection, which is guided by knockoff features and optimized through reinforcement learning, to identify the optimal and effective feature subset. In detail, our method involves generating \"knockoff\" features that replicate the distribution and characteristics of the original features but are independent of the target variable. Each feature is then assigned a pseudo label based on its correlation with all the knockoff features, serving as a novel metric for feature evaluation. Our approach utilizes these pseudo labels to guide the feature selection process in 3 novel ways, optimized by a single reinforced agent: 1). A deep Q-network, pre-trained with the original features and their corresponding pseudo labels, is employed to improve the efficacy of the exploration process in feature selection. 2). We introduce unsupervised rewards to evaluate the feature subset quality based on the pseudo labels and the feature space reconstruction loss to reduce dependencies on the target variable. 3). A new {\\epsilon}-greedy strategy is used, incorporating insights from the pseudo labels to make the feature selection process more effective.","creator":"Xinyuan Wang, Dongjie Wang, Wangyang Ying, Rui Xie, Haifeng Chen, Yanjie Fu"},{"id":"2403.04031","slug":"can-large-language-models-do-analytical-reasoning","title":"Can Large Language Models do Analytical Reasoning?","link":"https://arxiv.org/abs/2403.04031","abstract":"arXiv:2403.04031v1 Announce Type: cross  Abstract: This paper explores the cutting-edge Large Language Model with analytical reasoning on sports. Our analytical reasoning embodies the tasks of letting large language models count how many points each team scores in a quarter in the NBA and NFL games. Our major discoveries are in two folds. Firstly, we find among all the models we employed, GPT-4 stands out in effectiveness, followed by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind. Specifically, we compare three different prompting techniques and a divide-and-conquer approach, we find that the latter was the most effective. Our divide-and-conquer approach breaks down play-by-play data into smaller, more manageable segments, solves each piece individually, and then aggregates them together. Besides the divide-and-conquer approach, we also explore the Chain of Thought (CoT) strategy, which markedly improves outcomes for certain models, notably GPT-4 and Claude-2.1, with their accuracy rates increasing significantly. However, the CoT strategy has negligible or even detrimental effects on the performance of other models like GPT-3.5 and Gemini-Pro. Secondly, to our surprise, we observe that most models, including GPT-4, struggle to accurately count the total scores for NBA quarters despite showing strong performance in counting NFL quarter scores. This leads us to further investigate the factors that impact the complexity of analytical reasoning tasks with extensive experiments, through which we conclude that task complexity depends on the length of context, the information density, and the presence of related information. Our research provides valuable insights into the complexity of analytical reasoning tasks and potential directions for developing future large language models.","creator":"Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh, Dong Yu, Fei Liu"},{"id":"2403.04033","slug":"online-learning-with-unknown-constraints","title":"Online Learning with Unknown Constraints","link":"https://arxiv.org/abs/2403.04033","abstract":"arXiv:2403.04033v1 Announce Type: cross  Abstract: We consider the problem of online learning where the sequence of actions played by the learner must adhere to an unknown safety constraint at every round. The goal is to minimize regret with respect to the best safe action in hindsight while simultaneously satisfying the safety constraint with high probability on each round. We provide a general meta-algorithm that leverages an online regression oracle to estimate the unknown safety constraint, and converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint. On the theoretical side, our algorithm's regret can be bounded by the regret of the online regression and online learning oracles, the eluder dimension of the model class containing the unknown safety constraint, and a novel complexity measure that captures the difficulty of safe learning. We complement our result with an asymptotic lower bound that shows that the aforementioned complexity measure is necessary. When the constraints are linear, we instantiate our result to provide a concrete algorithm with $\\sqrt{T}$ regret using a scaling transformation that balances optimistic exploration with pessimistic constraint satisfaction.","creator":"Karthik Sridharan, Seung Won Wilson Yoo"},{"id":"2403.04036","slug":"unsupervised-contrastive-learning-for-robust-rf-device-fingerprinting-under-time-domain-shift","title":"Unsupervised Contrastive Learning for Robust RF Device Fingerprinting Under Time-Domain Shift","link":"https://arxiv.org/abs/2403.04036","abstract":"arXiv:2403.04036v1 Announce Type: cross  Abstract: Radio Frequency (RF) device fingerprinting has been recognized as a potential technology for enabling automated wireless device identification and classification. However, it faces a key challenge due to the domain shift that could arise from variations in the channel conditions and environmental settings, potentially degrading the accuracy of RF-based device classification when testing and training data is collected in different domains. This paper introduces a novel solution that leverages contrastive learning to mitigate this domain shift problem. Contrastive learning, a state-of-the-art self-supervised learning approach from deep learning, learns a distance metric such that positive pairs are closer (i.e. more similar) in the learned metric space than negative pairs. When applied to RF fingerprinting, our model treats RF signals from the same transmission as positive pairs and those from different transmissions as negative pairs. Through experiments on wireless and wired RF datasets collected over several days, we demonstrate that our contrastive learning approach captures domain-invariant features, diminishing the effects of domain-specific variations. Our results show large and consistent improvements in accuracy (10.8\\% to 27.8\\%) over baseline models, thus underscoring the effectiveness of contrastive learning in improving device classification under domain shift.","creator":"Jun Chen, Weng-Keen Wong, Bechir Hamdaoui"},{"id":"2403.04070","slug":"improving-adversarial-training-using-vulnerability-aware-perturbation-budget","title":"Improving Adversarial Training using Vulnerability-Aware Perturbation Budget","link":"https://arxiv.org/abs/2403.04070","abstract":"arXiv:2403.04070v1 Announce Type: cross  Abstract: Adversarial Training (AT) effectively improves the robustness of Deep Neural Networks (DNNs) to adversarial attacks. Generally, AT involves training DNN models with adversarial examples obtained within a pre-defined, fixed perturbation bound. Notably, individual natural examples from which these adversarial examples are crafted exhibit varying degrees of intrinsic vulnerabilities, and as such, crafting adversarial examples with fixed perturbation radius for all instances may not sufficiently unleash the potency of AT. Motivated by this observation, we propose two simple, computationally cheap vulnerability-aware reweighting functions for assigning perturbation bounds to adversarial examples used for AT, named Margin-Weighted Perturbation Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The proposed methods assign perturbation radii to individual adversarial samples based on the vulnerability of their corresponding natural examples. Experimental results show that the proposed methods yield genuine improvements in the robustness of AT algorithms against various adversarial attacks.","creator":"Olukorede Fakorede, Modeste Atsague, Jin Tian"},{"id":"2403.04071","slug":"on-device-self-supervised-learning-of-visual-perception-tasks-aboard-hardware-limited-nano-quadrotors","title":"On-device Self-supervised Learning of Visual Perception Tasks aboard Hardware-limited Nano-quadrotors","link":"https://arxiv.org/abs/2403.04071","abstract":"arXiv:2403.04071v1 Announce Type: cross  Abstract: Sub-\\SI{50}{\\gram} nano-drones are gaining momentum in both academia and industry. Their most compelling applications rely on onboard deep learning models for perception despite severe hardware constraints (\\ie sub-\\SI{100}{\\milli\\watt} processor). When deployed in unknown environments not represented in the training data, these models often underperform due to domain shift. To cope with this fundamental problem, we propose, for the first time, on-device learning aboard nano-drones, where the first part of the in-field mission is dedicated to self-supervised fine-tuning of a pre-trained convolutional neural network (CNN). Leveraging a real-world vision-based regression task, we thoroughly explore performance-cost trade-offs of the fine-tuning phase along three axes: \\textit{i}) dataset size (more data increases the regression performance but requires more memory and longer computation); \\textit{ii}) methodologies (\\eg fine-tuning all model parameters vs. only a subset); and \\textit{iii}) self-supervision strategy. Our approach demonstrates an improvement in mean absolute error up to 30\\% compared to the pre-trained baseline, requiring only \\SI{22}{\\second} fine-tuning on an ultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem via on-device learning aboard nano-drones not only marks a novel result for hardware-limited robots but lays the ground for more general advancements for the entire robotics community.","creator":"Elia Cereda, Manuele Rusci, Alessandro Giusti, Daniele Palossi"},{"id":"2403.04073","slug":"semi-supervised-dialogue-abstractive-summarization-via-high-quality-pseudolabel-selection","title":"Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection","link":"https://arxiv.org/abs/2403.04073","abstract":"arXiv:2403.04073v1 Announce Type: cross  Abstract: Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks. Our code is available at \\url{https://github.com/amazon-science/summarization-sicf-score}.","creator":"Jianfeng He, Hang Su, Jason Cai, Igor Shalyminov, Hwanjun Song, Saab Mansour"},{"id":"2403.04115","slug":"dnact-diffusion-guided-multi-task-3d-policy-learning","title":"DNAct: Diffusion Guided Multi-Task 3D Policy Learning","link":"https://arxiv.org/abs/2403.04115","abstract":"arXiv:2403.04115v1 Announce Type: cross  Abstract: This paper presents DNAct, a language-conditioned multi-task policy framework that integrates neural rendering pre-training and diffusion training to enforce multi-modality learning in action sequence spaces. To learn a generalizable multi-task policy with few demonstrations, the pre-training phase of DNAct leverages neural rendering to distill 2D semantic features from foundation models such as Stable Diffusion to a 3D space, which provides a comprehensive semantic understanding regarding the scene. Consequently, it allows various applications to challenging robotic tasks requiring rich 3D semantics and accurate geometry. Furthermore, we introduce a novel approach utilizing diffusion training to learn a vision and language feature that encapsulates the inherent multi-modality in the multi-task demonstrations. By reconstructing the action sequences from different tasks via the diffusion process, the model is capable of distinguishing different modalities and thus improving the robustness and the generalizability of the learned representation. DNAct significantly surpasses SOTA NeRF-based multi-task manipulation approaches with over 30% improvement in success rate. Project website: dnact.github.io.","creator":"Ge Yan, Yueh-Hua Wu, Xiaolong Wang"},{"id":"2403.04146","slug":"fl-guard-a-holistic-framework-for-run-time-detection-and-recovery-of-negative-federated-learning","title":"FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning","link":"https://arxiv.org/abs/2403.04146","abstract":"arXiv:2403.04146v1 Announce Type: cross  Abstract: Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solutions when using those solutions to upgrade an FL system in use. This paper introduces FL-GUARD, a holistic framework that can be employed on any FL system for tackling NFL in a run-time paradigm. That is, to dynamically detect NFL at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary. Specifically, we devise a cost-effective NFL detection mechanism, which relies on an estimation of performance gain on clients. Only when NFL is detected, we activate the NFL recovery process, in which each client learns in parallel an adapted model when training the global model. Extensive experiment results confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL to a healthy learning state. We also show that FL-GUARD is compatible with previous NFL solutions and robust against clients unwilling/unable to take any recovery measures.","creator":"Hong Lin, Lidan Shou, Ke Chen, Gang Chen, Sai Wu"},{"id":"2403.04158","slug":"da-net-a-disentangled-and-adaptive-network-for-multi-source-cross-lingual-transfer-learning","title":"DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning","link":"https://arxiv.org/abs/2403.04158","abstract":"arXiv:2403.04158v1 Announce Type: cross  Abstract: Multi-Source cross-lingual transfer learning deals with the transfer of task knowledge from multiple labelled source languages to an unlabeled target language under the language shift. Existing methods typically focus on weighting the predictions produced by language-specific classifiers of different sources that follow a shared encoder. However, all source languages share the same encoder, which is updated by all these languages. The extracted representations inevitably contain different source languages' information, which may disturb the learning of the language-specific classifiers. Additionally, due to the language gap, language-specific classifiers trained with source labels are unable to make accurate predictions for the target language. Both facts impair the model's performance. To address these challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly, we devise a feedback-guided collaborative disentanglement method that seeks to purify input representations of classifiers, thereby mitigating mutual interference from multiple sources. Secondly, we propose a class-aware parallel adaptation method that aligns class-level distributions for each source-target language pair, thereby alleviating the language pairs' language gap. Experimental results on three different tasks involving 38 languages validate the effectiveness of our approach.","creator":"Ling Ge, Chunming Hu, Guanghui Ma, Jihong Liu, Hong Zhang"},{"id":"2403.04160","slug":"improving-retrieval-in-theme-specific-applications-using-a-corpus-topical-taxonomy","title":"Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy","link":"https://arxiv.org/abs/2403.04160","abstract":"arXiv:2403.04160v1 Announce Type: cross  Abstract: Document retrieval has greatly benefited from the advancements of large-scale pre-trained language models (PLMs). However, their effectiveness is often limited in theme-specific applications for specialized areas or industries, due to unique terminologies, incomplete contexts of user queries, and specialized search intents. To capture the theme-specific information and improve retrieval, we propose to use a corpus topical taxonomy, which outlines the latent topic structure of the corpus while reflecting user-interested aspects. We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which identifies the central topics of queries and documents with the guidance of the taxonomy, and exploits their topical relatedness to supplement missing contexts. As a plug-and-play framework, ToTER can be flexibly employed to enhance various PLM-based retrievers. Through extensive quantitative, ablative, and exploratory experiments on two real-world datasets, we ascertain the benefits of using topical taxonomy for retrieval in theme-specific applications and demonstrate the effectiveness of ToTER.","creator":"SeongKu Kang, Shivam Agarwal, Bowen Jin, Dongha Lee, Hwanjo Yu, Jiawei Han"},{"id":"2403.04164","slug":"promise-promptable-medical-image-segmentation-using-sam","title":"ProMISe: Promptable Medical Image Segmentation using SAM","link":"https://arxiv.org/abs/2403.04164","abstract":"arXiv:2403.04164v1 Announce Type: cross  Abstract: With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for medical image segmentation (MIS) has become popular. However, due to the large size of the SAM model and the significant domain gap between natural and medical images, fine-tuning-based strategies are costly with potential risk of instability, feature damage and catastrophic forgetting. Furthermore, some methods of transferring SAM to a domain-specific MIS through fine-tuning strategies disable the model's prompting capability, severely limiting its utilization scenarios. In this paper, we propose an Auto-Prompting Module (APM), which provides SAM-based foundation model with Euclidean adaptive prompts in the target domain. Our experiments demonstrate that such adaptive prompts significantly improve SAM's non-fine-tuned performance in MIS. In addition, we propose a novel non-invasive method called Incremental Pattern Shifting (IPS) to adapt SAM to specific medical domains. Experimental results show that the IPS enables SAM to achieve state-of-the-art or competitive performance in MIS without the need for fine-tuning. By coupling these two methods, we propose ProMISe, an end-to-end non-fine-tuned framework for Promptable Medical Image Segmentation. Our experiments demonstrate that both using our methods individually or in combination achieves satisfactory performance in low-cost pattern shifting, with all of SAM's parameters frozen.","creator":"Jinfeng Wang, Sifan Song, Xinkun Wang, Yiyi Wang, Yiyi Miao, Jionglong Su, S. Kevin Zhou"},{"id":"2403.04175","slug":"understanding-the-pulsar-effect-in-combined-radiotherapy-and-immunotherapy-through-attention-mechanisms-with-a-transformer-model","title":"Understanding the PULSAR Effect in Combined Radiotherapy and Immunotherapy through Attention Mechanisms with a Transformer Model","link":"https://arxiv.org/abs/2403.04175","abstract":"arXiv:2403.04175v1 Announce Type: cross  Abstract: PULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy) is the adaptation of stereotactic ablative radiotherapy towards personalized cancer management. For the first time, we applied a transformer-based attention mechanism to investigate the underlying interactions between combined PULSAR and PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung Carcinoma, LLC). The proposed approach is able to predict the trend of tumor volume change semi-quantitatively, and excels in identifying the potential causal relationships through both self-attention and cross-attention scores.","creator":"Hao Peng, Casey Moore, Debabrata Saha, Steve Jiang, Robert Timmerman"},{"id":"2403.04182","slug":"metric-aware-llm-inference","title":"Metric-aware LLM inference","link":"https://arxiv.org/abs/2403.04182","abstract":"arXiv:2403.04182v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated strong results on a range of NLP tasks. Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics. As a remedy, we propose metric aware LLM inference: a decision theoretic approach optimizing for custom metrics at inference time. We report improvements over baselines on academic benchmarks and publicly available models.","creator":"Michal Lukasik, Harikrishna Narasimhan, Aditya Krishna Menon, Felix Yu, Sanjiv Kumar"},{"id":"2403.04187","slug":"preference-optimization-of-protein-language-models-as-a-multi-objective-binder-design-paradigm","title":"Preference optimization of protein language models as a multi-objective binder design paradigm","link":"https://arxiv.org/abs/2403.04187","abstract":"arXiv:2403.04187v1 Announce Type: cross  Abstract: We present a multi-objective binder design paradigm based on instruction fine-tuning and direct preference optimization (DPO) of autoregressive protein language models (pLMs). Multiple design objectives are encoded in the language model through direct optimization on expert curated preference sequence datasets comprising preferred and dispreferred distributions. We show the proposed alignment strategy enables ProtGPT2 to effectively design binders conditioned on specified receptors and a drug developability criterion. Generated binder samples demonstrate median isoelectric point (pI) improvements by $17\\%-60\\%$.","creator":"Pouria Mistani, Venkatesh Mysore"},{"id":"2403.04190","slug":"generative-ai-for-synthetic-data-generation-methods-challenges-and-the-future","title":"Generative AI for Synthetic Data Generation: Methods, Challenges and the Future","link":"https://arxiv.org/abs/2403.04190","abstract":"arXiv:2403.04190v1 Announce Type: cross  Abstract: The recent surge in research focused on generating synthetic data from large language models (LLMs), especially for scenarios with limited data availability, marks a notable shift in Generative Artificial Intelligence (AI). Their ability to perform comparably to real-world data positions this approach as a compelling solution to low-resource challenges. This paper delves into advanced technologies that leverage these gigantic LLMs for the generation of task-specific training data. We outline methodologies, evaluation techniques, and practical applications, discuss the current limitations, and suggest potential pathways for future research.","creator":"Xu Guo, Yiqiang Chen"},{"id":"2403.04197","slug":"large-language-models-are-in-context-molecule-learners","title":"Large Language Models are In-Context Molecule Learners","link":"https://arxiv.org/abs/2403.04197","abstract":"arXiv:2403.04197v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Cross-modal Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Additionally, we also propose Post-retrieval Re-ranking with Sequence Reversal and Random Walk to further improve the quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the in-context molecule learning capability of LLMs with retrieved examples and adapts the parameters of LLMs for the molecule-caption translation task. Experimental results demonstrate that ICMT can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.","creator":"Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, Qing Li"},{"id":"2403.04202","slug":"dynamics-of-moral-behavior-in-heterogeneous-populations-of-learning-agents","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents","link":"https://arxiv.org/abs/2403.04202","abstract":"arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in populations is not well understood. In this paper, we present a study of the learning dynamics of morally heterogeneous populations interacting in a social dilemma setting. Using a Prisoner's Dilemma environment with a partner selection mechanism, we investigate the extent to which the prevalence of diverse moral agents in populations affects individual agents' learning behaviors and emergent population-level outcomes. We observe several types of non-trivial interactions between pro-social and anti-social agents, and find that certain classes of moral agents are able to steer selfish agents towards more cooperative behavior.","creator":"Elizaveta Tennant, Stephen Hailes, Mirco Musolesi"},{"id":"2403.04221","slug":"why-online-reinforcement-learning-is-causal","title":"Why Online Reinforcement Learning is Causal","link":"https://arxiv.org/abs/2403.04221","abstract":"arXiv:2403.04221v1 Announce Type: cross  Abstract: Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an agent learns from their {\\em own} experience, there are no unobserved confounders that influence both the agent's own exploratory actions and the rewards they receive. Our paper formalizes this argument. For offline RL, where an agent may and typically does learn from the experience of {\\em others}, we describe previous and new methods for leveraging a causal model, including support for counterfactual queries.","creator":"Oliver Schulte, Pascal Poupart"},{"id":"2403.04224","slug":"aligners-decoupling-llms-and-alignment","title":"Aligners: Decoupling LLMs and Alignment","link":"https://arxiv.org/abs/2403.04224","abstract":"arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an \"ethical\" aligner and verify its efficacy empirically.","creator":"Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun, Mikhail Yurochkin"},{"id":"2403.04232","slug":"generalizing-cooperative-eco-driving-via-multi-residual-task-learning","title":"Generalizing Cooperative Eco-driving via Multi-residual Task Learning","link":"https://arxiv.org/abs/2403.04232","abstract":"arXiv:2403.04232v1 Announce Type: cross  Abstract: Conventional control, such as model-based control, is commonly utilized in autonomous driving due to its efficiency and reliability. However, real-world autonomous driving contends with a multitude of diverse traffic scenarios that are challenging for these planning algorithms. Model-free Deep Reinforcement Learning (DRL) presents a promising avenue in this direction, but learning DRL control policies that generalize to multiple traffic scenarios is still a challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a generic learning framework based on multi-task learning that, for a set of task scenarios, decomposes the control into nominal components that are effectively solved by conventional control methods and residual terms which are solved using learning. We employ MRTL for fleet-level emission reduction in mixed traffic using autonomous vehicles as a means of system control. By analyzing the performance of MRTL across nearly 600 signalized intersections and 1200 traffic scenarios, we demonstrate that it emerges as a promising approach to synergize the strengths of DRL and conventional methods in generalizable control.","creator":"Vindula Jayawardana, Sirui Li, Cathy Wu, Yashar Farid, Kentaro Oguchi"},{"id":"2403.04233","slug":"deep-icl-definition-enriched-experts-for-language-model-in-context-learning","title":"DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning","link":"https://arxiv.org/abs/2403.04233","abstract":"arXiv:2403.04233v1 Announce Type: cross  Abstract: It has long been assumed that the sheer number of parameters in large language models (LLMs) drives in-context learning (ICL) capabilities, enabling remarkable performance improvements by leveraging task-specific demonstrations. Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts task definitions from given demonstrations and generates responses through learning task-specific examples. We argue that improvement from ICL does not directly rely on model size, but essentially stems from understanding task definitions and task-guided learning. Inspired by this, DEEP-ICL combines two 3B models with distinct roles (one for concluding task definitions and the other for learning task demonstrations) and achieves comparable performance to LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by overcoming pretraining sequence length limitations, by supporting unlimited demonstrations. We contend that DEEP-ICL presents a novel alternative for achieving efficient few-shot learning, extending beyond the conventional ICL.","creator":"Xingwei Qu, Yiming Liang, Yucheng Wang, Tianyu Zheng, Tommy Yue, Lei Ma, Stephen W. Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Ge Zhang"},{"id":"2403.04246","slug":"efficient-cnn-lstm-based-parameter-estimation-of-levy-driven-stochastic-differential-equations","title":"Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic Differential Equations","link":"https://arxiv.org/abs/2403.04246","abstract":"arXiv:2403.04246v1 Announce Type: cross  Abstract: This study addresses the challenges in parameter estimation of stochastic differential equations driven by non-Gaussian noises, which are critical in understanding dynamic phenomena such as price fluctuations and the spread of infectious diseases. Previous research highlighted the potential of LSTM networks in estimating parameters of alpha stable Levy driven SDEs but faced limitations including high time complexity and constraints of the LSTM chaining property. To mitigate these issues, we introduce the PEnet, a novel CNN-LSTM-based three-stage model that offers an end to end approach with superior accuracy and adaptability to varying data structures, enhanced inference speed for long sequence observations through initial data feature condensation by CNN, and high generalization capability, allowing its application to various complex SDE scenarios. Experiments on synthetic datasets confirm PEnet significant advantage in estimating SDE parameters associated with noise characteristics, establishing it as a competitive method for SDE parameter estimation in the presence of Levy noise.","creator":"Shuaiyu Li, Yang Ruan, Changzhou Long, Yuzhong Cheng"},{"id":"2403.04256","slug":"federated-recommendation-via-hybrid-retrieval-augmented-generation","title":"Federated Recommendation via Hybrid Retrieval Augmented Generation","link":"https://arxiv.org/abs/2403.04256","abstract":"arXiv:2403.04256v1 Announce Type: cross  Abstract: Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, the retrieved results are converted into text prompts and fed into GPT for re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods.","creator":"Huimin Zeng, Zhenrui Yue, Qian Jiang, Dong Wang"},{"id":"2403.04283","slug":"proxy-rlhf-decoupling-generation-and-alignment-in-large-language-model-with-proxy","title":"Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy","link":"https://arxiv.org/abs/2403.04283","abstract":"arXiv:2403.04283v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach to ensure Large Language Models (LLMs) align with human values. However, existing RLHF methods require a high computational cost, one main reason being that RLHF assigns both the generation and alignment tasks to the LLM simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the generation and alignment processes of LLMs, achieving alignment with human values at a much lower computational cost. We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself. Experiments show that our method achieves a comparable level of alignment with only 1\\% of the training parameters of other methods.","creator":"Yu Zhu, Chuxiong Sun, Wenfei Yang, Wenqiang Wei, Bo Tang, Tianzhu Zhang, Zhiyu Li, Shifeng Zhang, Feiyu Xiong, Jie Hu, Mingchuan yang"},{"id":"2403.04299","slug":"litsim-conflict-aware-policy-for-long-term-interactive-traffic-simulation","title":"LitSim: Conflict-aware Policy for Long-term Interactive Traffic Simulation","link":"https://arxiv.org/abs/2403.04299","abstract":"arXiv:2403.04299v1 Announce Type: cross  Abstract: Simulation is pivotal in evaluating the performance of autonomous driving systems due to the advantages in efficiency and cost compared to on-road testing. Realistic multi-agent behavior~(e.g., interactive and long-term) is needed to narrow the gap between the simulation and the reality. The existing work has the following shortcomings in achieving this goal:~(1) log replay offers realistic scenarios but leads to unrealistic collisions due to lacking dynamic interactions, and~(2) model-based and learning-based solutions encourage interactions but often deviate from real-world data in long horizons. In this work, we propose LitSim, a long-term interactive simulation approach that maximizes realism while avoiding unrealistic collisions. Specifically, we replay the log for most scenarios and intervene only when LitSim predicts unrealistic conflicts. We then encourage interactions among the agents and resolve the conflicts, thereby reducing the likelihood of unrealistic collisions. We train and validate our model on the real-world dataset NGSIM, and the experimental results demonstrate that LitSim outperforms the current popular approaches in realism and reactivity.","creator":"Haojie Xin, Xiaodong Zhang, Renzhi Tang, Songyang Yan, Qianrui Zhao, Chunze Yang, Zijiang Yang"},{"id":"2403.04306","slug":"effectiveness-assessment-of-recent-large-vision-language-models","title":"Effectiveness Assessment of Recent Large Vision-Language Models","link":"https://arxiv.org/abs/2403.04306","abstract":"arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localization. Moreover, we conduct empirical investigations utilizing the aforementioned models alongside GPT-4V, assessing their multi-modal understanding capacities in general tasks such as object counting, absurd question answering, affordance reasoning, attribute recognition, and spatial relation reasoning. Our investigations reveal that these models demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deeper into this inadequacy and suggest several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems. We hope this study would provide valuable insights for the future development of LVLMs, augmenting their power in coping with both general and specialized applications.","creator":"Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong, Deng-Ping Fan, Fahad Shahbaz Khan"},{"id":"2403.04309","slug":"ao-detr-anti-overlapping-detr-for-x-ray-prohibited-items-detection","title":"AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection","link":"https://arxiv.org/abs/2403.04309","abstract":"arXiv:2403.04309v1 Announce Type: cross  Abstract: Prohibited item detection in X-ray images is one of the most essential and highly effective methods widely employed in various security inspection scenarios. Considering the significant overlapping phenomenon in X-ray prohibited item images, we propose an Anti-Overlapping DETR (AO-DETR) based on one of the state-of-the-art general object detectors, DINO. Specifically, to address the feature coupling issue caused by overlapping phenomena, we introduce the Category-Specific One-to-One Assignment (CSA) strategy to constrain category-specific object queries in predicting prohibited items of fixed categories, which can enhance their ability to extract features specific to prohibited items of a particular category from the overlapping foreground-background features. To address the edge blurring problem caused by overlapping phenomena, we propose the Look Forward Densely (LFD) scheme, which improves the localization accuracy of reference boxes in mid-to-high-level decoder layers and enhances the ability to locate blurry edges of the final layer. Similar to DINO, our AO-DETR provides two different versions with distinct backbones, tailored to meet diverse application requirements. Extensive experiments on the PIXray and OPIXray datasets demonstrate that the proposed method surpasses the state-of-the-art object detectors, indicating its potential applications in the field of prohibited item detection. The source code will be released at https://github.com/Limingyuan001/AO-DETR-test.","creator":"Mingyuan Li, Tong Jia, Hao Wang, Bowen Ma, Shuyang Lin, Da Cai, Dongyue Chen"},{"id":"2403.04321","slug":"discriminative-probing-and-tuning-for-text-to-image-generation","title":"Discriminative Probing and Tuning for Text-to-Image Generation","link":"https://arxiv.org/abs/2403.04321","abstract":"arXiv:2403.04321v1 Announce Type: cross  Abstract: Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models.","creator":"Leigang Qu, Wenjie Wang, Yongqi Li, Hanwang Zhang, Liqiang Nie, Tat-Seng Chua"},{"id":"2403.04325","slug":"measuring-meaning-composition-in-the-human-brain-with-composition-scores-from-large-language-models","title":"Measuring Meaning Composition in the Human Brain with Composition Scores from Large Language Models","link":"https://arxiv.org/abs/2403.04325","abstract":"arXiv:2403.04325v1 Announce Type: cross  Abstract: The process of meaning composition, wherein smaller units like morphemes or words combine to form the meaning of phrases and sentences, is essential for human sentence comprehension. Despite extensive neurolinguistic research into the brain regions involved in meaning composition, a computational metric to quantify the extent of composition is still lacking. Drawing on the key-value memory interpretation of transformer feed-forward network blocks, we introduce the Composition Score, a novel model-based metric designed to quantify the degree of meaning composition during sentence comprehension. Experimental findings show that this metric correlates with brain clusters associated with word frequency, structural processing, and general sensitivity to words, suggesting the multifaceted nature of meaning composition during human sentence comprehension.","creator":"Changjiang Gao, Jixing Li, Jiajun Chen, Shujian Huang"},{"id":"2403.04326","slug":"edge-based-parametric-digital-twins-for-intelligent-building-indoor-climate-modeling","title":"Edge-based Parametric Digital Twins for Intelligent Building Indoor Climate Modeling","link":"https://arxiv.org/abs/2403.04326","abstract":"arXiv:2403.04326v1 Announce Type: cross  Abstract: Digital transformation in the built environment generates vast data for developing data-driven models to optimize building operations. This study presents an integrated solution utilizing edge computing, digital twins, and deep learning to enhance the understanding of climate in buildings. Parametric digital twins, created using an ontology, ensure consistent data representation across diverse service systems equipped by different buildings. Based on created digital twins and collected data, deep learning methods are employed to develop predictive models for identifying patterns in indoor climate and providing insights. Both the parametric digital twin and deep learning models are deployed on edge for low latency and privacy compliance. As a demonstration, a case study was conducted in a historic building in \\\"Osterg\\\"otland, Sweden, to compare the performance of five deep learning architectures. The results indicate that the time-series dense encoder model exhibited strong competitiveness in performing multi-horizon forecasts of indoor temperature and relative humidity with low computational costs.","creator":"Zhongjun NiDepartment of Science and Technology, Link\\\"oping University, Campus Norrk\\\"oping, Norrk\\\"oping, Sweden, Chi ZhangDepartment of Computer Science and Engineering, University of Gothenburg, Gothenburg, Sweden, Magnus KarlssonDepartment of Science and Technology, Link\\\"oping University, Campus Norrk\\\"oping, Norrk\\\"oping, Sweden, Shaofang GongDepartment of Science and Technology, Link\\\"oping University, Campus Norrk\\\"oping, Norrk\\\"oping, Sweden"},{"id":"2403.04359","slug":"symmetry-considerations-for-learning-task-symmetric-robot-policies","title":"Symmetry Considerations for Learning Task Symmetric Robot Policies","link":"https://arxiv.org/abs/2403.04359","abstract":"arXiv:2403.04359v1 Announce Type: cross  Abstract: Symmetry is a fundamental aspect of many real-world robotic tasks. However, current deep reinforcement learning (DRL) approaches can seldom harness and exploit symmetry effectively. Often, the learned behaviors fail to achieve the desired transformation invariances and suffer from motion artifacts. For instance, a quadruped may exhibit different gaits when commanded to move forward or backward, even though it is symmetrical about its torso. This issue becomes further pronounced in high-dimensional or complex environments, where DRL methods are prone to local optima and fail to explore regions of the state space equally. Past methods on encouraging symmetry for robotic tasks have studied this topic mainly in a single-task setting, where symmetry usually refers to symmetry in the motion, such as the gait patterns. In this paper, we revisit this topic for goal-conditioned tasks in robotics, where symmetry lies mainly in task execution and not necessarily in the learned motions themselves. In particular, we investigate two approaches to incorporate symmetry invariance into DRL -- data augmentation and mirror loss function. We provide a theoretical foundation for using augmented samples in an on-policy setting. Based on this, we show that the corresponding approach achieves faster convergence and improves the learned behaviors in various challenging robotic tasks, from climbing boxes with a quadruped to dexterous manipulation.","creator":"Mayank Mittal, Nikita Rudin, Victor Klemm, Arthur Allshire, Marco Hutter"},{"id":"2403.04374","slug":"model-free-load-frequency-control-of-nonlinear-power-systems-based-on-deep-reinforcement-learning","title":"Model-Free Load Frequency Control of Nonlinear Power Systems Based on Deep Reinforcement Learning","link":"https://arxiv.org/abs/2403.04374","abstract":"arXiv:2403.04374v1 Announce Type: cross  Abstract: Load frequency control (LFC) is widely employed in power systems to stabilize frequency fluctuation and guarantee power quality. However, most existing LFC methods rely on accurate power system modeling and usually ignore the nonlinear characteristics of the system, limiting controllers' performance. To solve these problems, this paper proposes a model-free LFC method for nonlinear power systems based on deep deterministic policy gradient (DDPG) framework. The proposed method establishes an emulator network to emulate power system dynamics. After defining the action-value function, the emulator network is applied for control actions evaluation instead of the critic network. Then the actor network controller is effectively optimized by estimating the policy gradient based on zeroth-order optimization (ZOO) and backpropagation algorithm. Simulation results and corresponding comparisons demonstrate the designed controller can generate appropriate control actions and has strong adaptability for nonlinear power systems.","creator":"Xiaodi Chen, Meng Zhang, Zhengguang Wu, Ligang Wu, Xiaohong Guan"},{"id":"2403.04382","slug":"acceleron-a-tool-to-accelerate-research-ideation","title":"Acceleron: A Tool to Accelerate Research Ideation","link":"https://arxiv.org/abs/2403.04382","abstract":"arXiv:2403.04382v1 Announce Type: cross  Abstract: Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose `Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the proposed problem. We leverage the reasoning and domain-specific skills of Large Language Models (LLMs) to create an agent-based architecture incorporating colleague and mentor personas for LLMs. The LLM agents emulate the ideation process undertaken by researchers, engaging researchers in an interactive fashion to aid in the development of the research proposal. Notably, our tool addresses challenges inherent in LLMs, such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability. As evaluation, we illustrate the execution of our motivation validation and method synthesis workflows on proposals from the ML and NLP domain, given by 3 distinct researchers. Our observations and evaluations provided by the researchers illustrate the efficacy of the tool in terms of assisting researchers with appropriate inputs at distinct stages and thus leading to improved time efficiency.","creator":"Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff"},{"id":"2403.04417","slug":"promising-and-worth-to-try-future-directions-for-advancing-state-of-the-art-surrogates-methods-of-agent-based-models-in-social-and-health-computational-sciences","title":"Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences","link":"https://arxiv.org/abs/2403.04417","abstract":"arXiv:2403.04417v1 Announce Type: cross  Abstract: The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SHCS.","creator":"Atiyah Elsheikh"},{"id":"2403.04427","slug":"sentiment-driven-prediction-of-financial-returns-a-bayesian-enhanced-finbert-approach","title":"Sentiment-driven prediction of financial returns: a Bayesian-enhanced FinBERT approach","link":"https://arxiv.org/abs/2403.04427","abstract":"arXiv:2403.04427v1 Announce Type: cross  Abstract: Predicting financial returns accurately poses a significant challenge due to the inherent uncertainty in financial time series data. Enhancing prediction models' performance hinges on effectively capturing both social and financial sentiment. In this study, we showcase the efficacy of leveraging sentiment information extracted from tweets using the FinBERT large language model. By meticulously curating an optimal feature set through correlation analysis and employing Bayesian-optimized Recursive Feature Elimination for automatic feature selection, we surpass existing methodologies, achieving an F1-score exceeding 70% on the test set. This success translates into demonstrably higher cumulative profits during backtested trading. Our investigation focuses on real-world SPY ETF data alongside corresponding tweets sourced from the StockTwits platform.","creator":"Raffaele Giuseppe Cestari, Simone Formentin"},{"id":"2403.04436","slug":"learning-human-to-humanoid-real-time-whole-body-teleoperation","title":"Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation","link":"https://arxiv.org/abs/2403.04436","abstract":"arXiv:2403.04436v1 Announce Type: cross  Abstract: We present Human to Humanoid (H2O), a reinforcement learning (RL) based framework that enables real-time whole-body teleoperation of a full-sized humanoid robot with only an RGB camera. To create a large-scale retargeted motion dataset of human movements for humanoid robots, we propose a scalable \"sim-to-data\" process to filter and pick feasible motions using a privileged motion imitator. Afterwards, we train a robust real-time humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zero-shot manner. We successfully achieve teleoperation of dynamic whole-body motions in real-world scenarios, including walking, back jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our knowledge, this is the first demonstration to achieve learning-based real-time whole-body humanoid teleoperation.","creator":"Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, Guanya Shi"},{"id":"2403.04442","slug":"cooperative-bayesian-optimization-for-imperfect-agents","title":"Cooperative Bayesian Optimization for Imperfect Agents","link":"https://arxiv.org/abs/2403.04442","abstract":"arXiv:2403.04442v1 Announce Type: cross  Abstract: We introduce a cooperative Bayesian optimization problem for optimizing black-box functions of two variables where two agents choose together at which points to query the function but have only control over one variable each. This setting is inspired by human-AI teamwork, where an AI-assistant helps its human user solve a problem, in this simplest case, collaborative optimization. We formulate the solution as sequential decision-making, where the agent we control models the user as a computationally rational agent with prior knowledge about the function. We show that strategic planning of the queries enables better identification of the global maximum of the function as long as the user avoids excessive exploration. This planning is made possible by using Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model that accounts for conservative belief updates and exploratory sampling of the points to query.","creator":"Ali Khoshvishkaie, Petrus Mikkola, Pierre-Alexandre Murena, Samuel Kaski"},{"id":"2403.04447","slug":"frri-a-novel-algorithm-for-fuzzy-rough-rule-induction","title":"FRRI: a novel algorithm for fuzzy-rough rule induction","link":"https://arxiv.org/abs/2403.04447","abstract":"arXiv:2403.04447v1 Announce Type: cross  Abstract: Interpretability is the next frontier in machine learning research. In the search for white box models - as opposed to black box models, like random forests or neural networks - rule induction algorithms are a logical and promising option, since the rules can easily be understood by humans. Fuzzy and rough set theory have been successfully applied to this archetype, almost always separately. As both approaches to rule induction involve granular computing based on the concept of equivalence classes, it is natural to combine them. The QuickRules\\cite{JensenCornelis2009} algorithm was a first attempt at using fuzzy rough set theory for rule induction. It is based on QuickReduct, a greedy algorithm for building decision reducts. QuickRules already showed an improvement over other rule induction methods. However, to evaluate the full potential of a fuzzy rough rule induction algorithm, one needs to start from the foundations. In this paper, we introduce a novel rule induction algorithm called Fuzzy Rough Rule Induction (FRRI). We provide background and explain the workings of our algorithm. Furthermore, we perform a computational experiment to evaluate the performance of our algorithm and compare it to other state-of-the-art rule induction approaches. We find that our algorithm is more accurate while creating small rulesets consisting of relatively short rules. We end the paper by outlining some directions for future work.","creator":"Henri Bollaert, Marko Palangeti\\'c, Chris Cornelis, Salvatore Greco, Roman S{\\l}owi\\'nski"},{"id":"2403.04454","slug":"low-resource-court-judgment-summarization-for-common-law-systems","title":"Low-Resource Court Judgment Summarization for Common Law Systems","link":"https://arxiv.org/abs/2403.04454","abstract":"arXiv:2403.04454v1 Announce Type: cross  Abstract: Common law courts need to refer to similar precedents' judgments to inform their current decisions. Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied. Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments. However, judges can refer to the judgments from all common law jurisdictions. Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions. To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents. Besides, this is the first court judgment summarization work adopting large language models (LLMs) in data augmentation, summary generation, and evaluation. Specifically, we design an LLM-based data augmentation method incorporating legal knowledge. We also propose a legal knowledge enhanced evaluation metric based on LLM to assess the quality of generated judgment summaries. Our experimental results verify that the LLM-based summarization methods can perform well in the few-shot and zero-shot settings. Our LLM-based data augmentation method can mitigate the impact of low data resources. Furthermore, we carry out comprehensive comparative experiments to find essential model components and settings that are capable of enhancing summarization performance.","creator":"Shuaiqi Liu, Jiannong Cao, Yicong Li, Ruosong Yang, Zhiyuan Wen"},{"id":"2403.04468","slug":"a-survey-of-graph-neural-networks-in-real-world-imbalance-noise-privacy-and-ood-challenges","title":"A Survey of Graph Neural Networks in Real world: Imbalance, Noise, Privacy and OOD Challenges","link":"https://arxiv.org/abs/2403.04468","abstract":"arXiv:2403.04468v1 Announce Type: cross  Abstract: Graph-structured data exhibits universality and widespread applicability across diverse domains, such as social network analysis, biochemistry, financial fraud detection, and network security. Significant strides have been made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success in these areas. However, in real-world scenarios, the training environment for models is often far from ideal, leading to substantial performance degradation of GNN models due to various unfavorable factors, including imbalance in data distribution, the presence of noise in erroneous data, privacy protection of sensitive information, and generalization capability for out-of-distribution (OOD) scenarios. To tackle these issues, substantial efforts have been devoted to improving the performance of GNN models in practical real-world scenarios, as well as enhancing their reliability and robustness. In this paper, we present a comprehensive survey that systematically reviews existing GNN models, focusing on solutions to the four mentioned real-world challenges including imbalance, noise, privacy, and OOD in practical scenarios that many existing reviews have not considered. Specifically, we first highlight the four key challenges faced by existing GNNs, paving the way for our exploration of real-world GNN models. Subsequently, we provide detailed discussions on these four aspects, dissecting how these solutions contribute to enhancing the reliability and robustness of GNN models. Last but not least, we outline promising directions and offer future perspectives in the field.","creator":"Wei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li, Yiyang Gu, Yifang Qin, Nan Yin, Senzhang Wang, Xinwang Liu, Xiao Luo, Philip S. Yu, Ming Zhang"},{"id":"2403.04473","slug":"textmonkey-an-ocr-free-large-multimodal-model-for-understanding-document","title":"TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document","link":"https://arxiv.org/abs/2403.04473","abstract":"arXiv:2403.04473v1 Announce Type: cross  Abstract: We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks, including document question answering (DocVQA) and scene text analysis. Our approach introduces enhancement across several dimensions: by adopting Shifted Window Attention with zero-initialization, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability and minimize hallucinations. Additionally, TextMonkey can be finetuned to gain the ability to comprehend commands for clicking screenshots. Overall, our method notably boosts performance across various benchmark datasets, achieving increases of 5.2%, 6.9%, and 2.8% in Scene Text-Centric VQA, Document Oriented VQA, and KIE, respectively, especially with a score of 561 on OCRBench, surpassing prior open-sourced large multimodal models for document understanding. Code will be released at https://github.com/Yuliang-Liu/Monkey.","creator":"Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, Xiang Bai"},{"id":"2403.04481","slug":"do-large-language-model-understand-multi-intent-spoken-language","title":"Do Large Language Model Understand Multi-Intent Spoken Language ?","link":"https://arxiv.org/abs/2403.04481","abstract":"arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth analysis of LLM proficiency in this complex field.","creator":"Shangjian Yin, Peijie Huang, Yuhong Xu, Haojing Huang, Jiatian Chen"},{"id":"2403.04500","slug":"a-learnable-prior-improves-inverse-tumor-growth-modeling","title":"A Learnable Prior Improves Inverse Tumor Growth Modeling","link":"https://arxiv.org/abs/2403.04500","abstract":"arXiv:2403.04500v1 Announce Type: cross  Abstract: Biophysical modeling, particularly involving partial differential equations (PDEs), offers significant potential for tailoring disease treatment protocols to individual patients. However, the inverse problem-solving aspect of these models presents a substantial challenge, either due to the high computational requirements of model-based approaches or the limited robustness of deep learning (DL) methods. We propose a novel framework that leverages the unique strengths of both approaches in a synergistic manner. Our method incorporates a DL ensemble for initial parameter estimation, facilitating efficient downstream evolutionary sampling initialized with this DL-based prior. We showcase the effectiveness of integrating a rapid deep-learning algorithm with a high-precision evolution strategy in estimating brain tumor cell concentrations from magnetic resonance images. The DL-Prior plays a pivotal role, significantly constraining the effective sampling-parameter space. This reduction results in a fivefold convergence acceleration and a Dice-score of 95%","creator":"Jonas Weidner, Ivan Ezhov, Michal Balcerak, Marie-Christin Metz, Sergey Litvinov, Sebastian Kaltenbach, Leonhard Feiner, Laurin Lux, Florian Kofler, Jana Lipkova, Jonas Latz, Daniel Rueckert, Bjoern Menze, Benedikt Wiestler"},{"id":"2403.04510","slug":"where-does-in-context-translation-happen-in-large-language-models","title":"Where does In-context Translation Happen in Large Language Models","link":"https://arxiv.org/abs/2403.04510","abstract":"arXiv:2403.04510v1 Announce Type: cross  Abstract: Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and \\textsc{Llama7b-chat}, we demonstrate evidence of a \"task recognition\" point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\\% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32. Our layer-wise fine-tuning experiments indicate that the most effective layers for MT fine-tuning are the layers critical to task recognition.","creator":"Suzanna Sia, David Mueller, Kevin Duh"},{"id":"2403.04523","slug":"t-tame-trainable-attention-mechanism-for-explaining-convolutional-networks-and-vision-transformers","title":"T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers","link":"https://arxiv.org/abs/2403.04523","abstract":"arXiv:2403.04523v1 Announce Type: cross  Abstract: The development and adoption of Vision Transformers and other deep-learning architectures for image classification tasks has been rapid. However, the \"black box\" nature of neural networks is a barrier to adoption in applications where explainability is essential. While some techniques for generating explanations have been proposed, primarily for Convolutional Neural Networks, adapting such techniques to the new paradigm of Vision Transformers is non-trivial. This paper presents T-TAME, Transformer-compatible Trainable Attention Mechanism for Explanations, a general methodology for explaining deep neural networks used in image classification tasks. The proposed architecture and training technique can be easily applied to any convolutional or Vision Transformer-like neural network, using a streamlined training approach. After training, explanation maps can be computed in a single forward pass; these explanation maps are comparable to or outperform the outputs of computationally expensive perturbation-based explainability techniques, achieving SOTA performance. We apply T-TAME to three popular deep learning classifier architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet dataset, and we demonstrate improvements over existing state-of-the-art explainability methods. A detailed analysis of the results and an ablation study provide insights into how the T-TAME design choices affect the quality of the generated explanation maps.","creator":"Mariano V. Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris"},{"id":"2403.04526","slug":"hyperspectral-unmixing-for-raman-spectroscopy-via-physics-constrained-autoencoders","title":"Hyperspectral unmixing for Raman spectroscopy via physics-constrained autoencoders","link":"https://arxiv.org/abs/2403.04526","abstract":"arXiv:2403.04526v1 Announce Type: cross  Abstract: Raman spectroscopy is widely used across scientific domains to characterize the chemical composition of samples in a non-destructive, label-free manner. Many applications entail the unmixing of signals from mixtures of molecular species to identify the individual components present and their proportions, yet conventional methods for chemometrics often struggle with complex mixture scenarios encountered in practice. Here, we develop hyperspectral unmixing algorithms based on autoencoder neural networks, and we systematically validate them using both synthetic and experimental benchmark datasets created in-house. Our results demonstrate that unmixing autoencoders provide improved accuracy, robustness and efficiency compared to standard unmixing methods. We also showcase the applicability of autoencoders to complex biological settings by showing improved biochemical characterization of volumetric Raman imaging data from a monocytic cell.","creator":"Dimitar Georgiev, \\'Alvaro Fern\\'andez-Galiana, Simon Vilms Pedersen, Georgios Papadopoulos, Ruoxiao Xie, Molly M. Stevens, Mauricio Barahona"},{"id":"2403.04529","slug":"enhancing-data-quality-in-federated-fine-tuning-of-foundation-models","title":"Enhancing Data Quality in Federated Fine-Tuning of Foundation Models","link":"https://arxiv.org/abs/2403.04529","abstract":"arXiv:2403.04529v1 Announce Type: cross  Abstract: In the current landscape of foundation model training, there is a significant reliance on public domain data, which is nearing exhaustion according to recent research. To further scale up, it is crucial to incorporate collaboration among multiple specialized and high-quality private domain data sources. However, the challenge of training models locally without sharing private data presents numerous obstacles in data quality control. To tackle this issue, we propose a data quality control pipeline for federated fine-tuning of foundation models. This pipeline computes scores reflecting the quality of training data and determines a global threshold for a unified standard, aiming for improved global performance. Our experiments show that the proposed quality control pipeline facilitates the effectiveness and reliability of the model training, leading to better performance.","creator":"Wanru Zhao, Yaxin Du, Nicholas Donald Lane, Siheng Chen, Yanfeng Wang"},{"id":"2403.04547","slug":"clip-the-bias-how-useful-is-balancing-data-in-multimodal-learning","title":"CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?","link":"https://arxiv.org/abs/2403.04547","abstract":"arXiv:2403.04547v1 Announce Type: cross  Abstract: We study the effectiveness of data-balancing for mitigating biases in contrastive language-image pretraining (CLIP), identifying areas of strength and limitation. First, we reaffirm prior conclusions that CLIP models can inadvertently absorb societal stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both representation and association biases (i.e. in first- and second-order statistics) in multimodal data. We use M4 to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how CLIP learns and unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and ImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with recommendations for improving the efficacy of data balancing in multimodal systems.","creator":"Ibrahim Alabdulmohsin, Xiao Wang, Andreas Steiner, Priya Goyal, Alexander D'Amour, Xiaohua Zhai"},{"id":"2403.04558","slug":"reducing-self-supervised-learning-complexity-improves-weakly-supervised-classification-performance-in-computational-pathology","title":"Reducing self-supervised learning complexity improves weakly-supervised classification performance in computational pathology","link":"https://arxiv.org/abs/2403.04558","abstract":"arXiv:2403.04558v1 Announce Type: cross  Abstract: Deep Learning models have been successfully utilized to extract clinically actionable insights from routinely available histology data. Generally, these models require annotations performed by clinicians, which are scarce and costly to generate. The emergence of self-supervised learning (SSL) methods remove this barrier, allowing for large-scale analyses on non-annotated data. However, recent SSL approaches apply increasingly expansive model architectures and larger datasets, causing the rapid escalation of data volumes, hardware prerequisites, and overall expenses, limiting access to these resources to few institutions. Therefore, we investigated the complexity of contrastive SSL in computational pathology in relation to classification performance with the utilization of consumer-grade hardware. Specifically, we analyzed the effects of adaptations in data volume, architecture, and algorithms on downstream clas- sification tasks, emphasizing their impact on computational resources. We trained breast cancer foundation models on a large public patient cohort and validated them on various downstream classification tasks in a weakly supervised manner on two external public patient cohorts. Our experiments demonstrate that we can improve downstream classification performance whilst reducing SSL training duration by 90%. In summary, we propose a set of adaptations which enable the utilization of SSL in computational pathology in non-resource abundant environments.","creator":"Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather"},{"id":"2403.04612","slug":"a-domain-translation-framework-with-an-adversarial-denoising-diffusion-model-to-generate-synthetic-datasets-of-echocardiography-images","title":"A Domain Translation Framework with an Adversarial Denoising Diffusion Model to Generate Synthetic Datasets of Echocardiography Images","link":"https://arxiv.org/abs/2403.04612","abstract":"arXiv:2403.04612v1 Announce Type: cross  Abstract: Currently, medical image domain translation operations show a high demand from researchers and clinicians. Amongst other capabilities, this task allows the generation of new medical images with sufficiently high image quality, making them clinically relevant. Deep Learning (DL) architectures, most specifically deep generative models, are widely used to generate and translate images from one domain to another. The proposed framework relies on an adversarial Denoising Diffusion Model (DDM) to synthesize echocardiography images and perform domain translation. Contrary to Generative Adversarial Networks (GANs), DDMs are able to generate high quality image samples with a large diversity. If a DDM is combined with a GAN, this ability to generate new data is completed at an even faster sampling time. In this work we trained an adversarial DDM combined with a GAN to learn the reverse denoising process, relying on a guide image, making sure relevant anatomical structures of each echocardiography image were kept and represented on the generated image samples. For several domain translation operations, the results verified that such generative model was able to synthesize high quality image samples: MSE: 11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposed method showed high generalization ability, introducing a framework to create echocardiography images suitable to be used for clinical research purposes.","creator":"Cristiana Tiago, Sten Roar Snare, Jurica Sprem, Kristin McLeod"},{"id":"2403.04629","slug":"explaining-bayesian-optimization-by-shapley-values-facilitates-human-ai-collaboration","title":"Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration","link":"https://arxiv.org/abs/2403.04629","abstract":"arXiv:2403.04629v1 Announce Type: cross  Abstract: Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human machine interface (HMI), allowing users to interfere with BO in case proposals do not align with human reasoning. We demonstrate this HMI's benefits for the use case of personalizing wearable robotic devices (assistive back exosuits) by human-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO can achieve lower regret than teams without.","creator":"Julian Rodemann, Federico Croppi, Philipp Arens, Yusuf Sale, Julia Herbinger, Bernd Bischl, Eyke H\\\"ullermeier, Thomas Augustin, Conor J. Walsh, Giuseppe Casalicchio"},{"id":"2403.04634","slug":"pix2gif-motion-guided-diffusion-for-gif-generation","title":"Pix2Gif: Motion-Guided Diffusion for GIF Generation","link":"https://arxiv.org/abs/2403.04634","abstract":"arXiv:2403.04634v1 Announce Type: cross  Abstract: We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure the transformed feature map remains within the same space as the target image, ensuring content consistency and coherence. In preparation for the model training, we meticulously curated data by extracting coherent image frames from the TGIF video-caption dataset, which provides rich information about the temporal changes of subjects. After pretraining, we apply our model in a zero-shot manner to a number of video datasets. Extensive qualitative and quantitative experiments demonstrate the effectiveness of our model -- it not only captures the semantic prompt from text but also the spatial ones from motion guidance. We train all our models using a single node of 16xV100 GPUs. Code, dataset and models are made public at: https://hiteshk03.github.io/Pix2Gif/.","creator":"Hitesh Kandala, Jianfeng Gao, Jianwei Yang"},{"id":"2403.04650","slug":"context-based-multimodal-fusion","title":"Context-Based Multimodal Fusion","link":"https://arxiv.org/abs/2403.04650","abstract":"arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training \"from scratch\" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. Additionally, the network learns to differentiate embeddings of different modalities through fusion with context and aligns data distributions using a contrastive approach for self-supervised learning. Thus, CBMF offers an effective and economical solution for solving complex multimodal tasks.","creator":"Bilal Faye, Hanane Azzag, Mustapha Lebbah, Djamel Bouchaffra"},{"id":"2403.04652","slug":"yi-open-foundation-models-by-01-ai","title":"Yi: Open Foundation Models by 01.AI","link":"https://arxiv.org/abs/2403.04652","abstract":"arXiv:2403.04652v1 Announce Type: cross  Abstract: We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.","creator":"01. AI,  :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai"},{"id":"2403.04690","slug":"faster-neighborhood-attention-reducing-the-o-n-2-cost-of-self-attention-at-the-threadblock-level","title":"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level","link":"https://arxiv.org/abs/2403.04690","abstract":"arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. We find certain inherent inefficiencies in all unfused neighborhood attention kernels that bound their performance and lower-precision scalability. We also developed fused neighborhood attention; an adaptation of fused dot-product attention kernels that allow fine-grained control over attention across different spatial axes. Known for reducing the quadratic time complexity of self attention to a linear complexity, neighborhood attention can now enjoy a reduced and constant memory footprint, and record-breaking half precision latency. We observe that our fused kernels successfully circumvent some of the unavoidable inefficiencies in unfused implementations. While our unfused GEMM-based kernels only improve half precision performance compared to naive kernels by an average of 496% and 113% in 1-D and 2-D problems respectively, our fused kernels improve naive kernels by an average of 1607% and 581% in 1-D and 2-D problems respectively.","creator":"Ali Hassani, Wen-Mei Hwu, Humphrey Shi"},{"id":"2403.04696","slug":"fact-checking-the-output-of-large-language-models-via-token-level-uncertainty-quantification","title":"Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification","link":"https://arxiv.org/abs/2403.04696","abstract":"arXiv:2403.04696v1 Announce Type: cross  Abstract: Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for six different LLMs and three languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.","creator":"Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov"},{"id":"2403.04697","slug":"auformer-vision-transformers-are-parameter-efficient-facial-action-unit-detectors","title":"AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors","link":"https://arxiv.org/abs/2403.04697","abstract":"arXiv:2403.04697v1 Announce Type: cross  Abstract: Facial Action Units (AU) is a vital concept in the realm of affective computing, and AU detection has always been a hot research topic. Existing methods suffer from overfitting issues due to the utilization of a large number of learnable parameters on scarce AU-annotated datasets or heavy reliance on substantial additional relevant data. Parameter-Efficient Transfer Learning (PETL) provides a promising paradigm to address these challenges, whereas its existing methods lack design for AU characteristics. Therefore, we innovatively investigate PETL paradigm to AU detection, introducing AUFormer and proposing a novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual MoKE specific to a certain AU with minimal learnable parameters first integrates personalized multi-scale and correlation knowledge. Then the MoKE collaborates with other MoKEs in the expert group to obtain aggregated information and inject it into the frozen Vision Transformer (ViT) to achieve parameter-efficient AU detection. Additionally, we design a Margin-truncated Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the model to focus more on activated AUs, differentiate the difficulty of unactivated AUs, and discard potential mislabeled samples. Extensive experiments from various perspectives, including within-domain, cross-domain, data efficiency, and micro-expression domain, demonstrate AUFormer's state-of-the-art performance and robust generalization abilities without relying on additional relevant data. The code for AUFormer is available at https://github.com/yuankaishen2001/AUFormer.","creator":"Kaishen Yuan, Zitong Yu, Xin Liu, Weicheng Xie, Huanjing Yue, Jingyu Yang"},{"id":"2403.04701","slug":"objectcompose-evaluating-resilience-of-vision-based-models-on-object-to-background-compositional-changes","title":"ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes","link":"https://arxiv.org/abs/2403.04701","abstract":"arXiv:2403.04701v1 Announce Type: cross  Abstract: Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, we evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse object-to-background changes while preserving the original semantics and appearance of the object. To achieve this goal, we harness the generative capabilities of text-to-image, image-to-text, and image-to-segment models to automatically generate a broad spectrum of object-to-background changes. We induce both natural and adversarial background changes by either modifying the textual prompts or optimizing the latents and textual embedding of text-to-image models. This allows us to quantify the role of background context in understanding the robustness and generalization of deep neural networks. We produce various versions of standard vision datasets (ImageNet, COCO), incorporating either diverse and realistic backgrounds into the images or introducing color, texture, and adversarial changes in the background. We conduct extensive experiment to analyze the robustness of vision-based models against object-to-background context variations across diverse tasks.","creator":"Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan"},{"id":"2403.04706","slug":"common-7b-language-models-already-possess-strong-math-capabilities","title":"Common 7B Language Models Already Possess Strong Math Capabilities","link":"https://arxiv.org/abs/2403.04706","abstract":"arXiv:2403.04706v1 Announce Type: cross  Abstract: Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.","creator":"Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, Houwen Peng"},{"id":"2403.04746","slug":"llms-in-the-imaginarium-tool-learning-through-simulated-trial-and-error","title":"LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error","link":"https://arxiv.org/abs/2403.04746","abstract":"arXiv:2403.04746v1 Announce Type: cross  Abstract: Tools are essential for large language models (LLMs) to acquire up-to-date information and take consequential actions in external environments. Existing work on tool-augmented LLMs primarily focuses on the broad coverage of tools and the flexibility of adding new tools. However, a critical aspect that has surprisingly been understudied is simply how accurately an LLM uses tools for which it has been trained. We find that existing LLMs, including GPT-4 and open-source LLMs specifically fine-tuned for tool use, only reach a correctness rate in the range of 30% to 60%, far from reliable use in practice. We propose a biologically inspired method for tool-augmented LLMs, simulated trial and error (STE), that orchestrates three key mechanisms for successful tool use behaviors in the biological system: trial and error, imagination, and memory. Specifically, STE leverages an LLM's 'imagination' to simulate plausible scenarios for using a tool, after which the LLM interacts with the tool to learn from its execution feedback. Both short-term and long-term memory are employed to improve the depth and breadth of the exploration, respectively. Comprehensive experiments on ToolBench show that STE substantially improves tool learning for LLMs under both in-context learning and fine-tuning settings, bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform GPT-4. We also show effective continual learning of tools via a simple experience replay strategy.","creator":"Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su"},{"id":"2403.04747","slug":"gnn-vpa-a-variance-preserving-aggregation-strategy-for-graph-neural-networks","title":"GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks","link":"https://arxiv.org/abs/2403.04747","abstract":"arXiv:2403.04747v1 Announce Type: cross  Abstract: Graph neural networks (GNNs), and especially message-passing neural networks, excel in various domains such as physics, drug discovery, and molecular modeling. The expressivity of GNNs with respect to their ability to discriminate non-isomorphic graphs critically depends on the functions employed for message aggregation and graph-level readout. By applying signal propagation theory, we propose a variance-preserving aggregation function (VPA) that maintains expressivity, but yields improved forward and backward dynamics. Experiments demonstrate that VPA leads to increased predictive performance for popular GNN architectures as well as improved learning dynamics. Our results could pave the way towards normalizer-free or self-normalizing GNNs.","creator":"Lisa Schneckenreiter, Richard Freinschlag, Florian Sestak, Johannes Brandstetter, G\\\"unter Klambauer, Andreas Mayr"},{"id":"2403.04758","slug":"knowledgevis-interpreting-language-models-by-comparing-fill-in-the-blank-prompts","title":"KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts","link":"https://arxiv.org/abs/2403.04758","abstract":"arXiv:2403.04758v1 Announce Type: cross  Abstract: Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVis, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVis reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships between predictions across all prompts. We demonstrate the capabilities of KnowledgeVis with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.","creator":"Adam Coscia, Alex Endert"},{"id":"2403.04760","slug":"iscore-visual-analytics-for-interpreting-how-language-models-automatically-score-summaries","title":"iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries","link":"https://arxiv.org/abs/2403.04760","abstract":"arXiv:2403.04760v1 Announce Type: cross  Abstract: The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting LLM scores, and visualize model weights at multiple levels of abstraction. To validate our approach, we deployed iScore with three learning engineers over the course of a month. We present a case study where interacting with iScore led a learning engineer to improve their LLM's score accuracy by three percentage points. Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their LLMs during deployment.","creator":"Adam Coscia, Langdon Holmes, Wesley Morris, Joon Suh Choi, Scott Crossley, Alex Endert"},{"id":"2308.06528","slug":"learning-abstract-visual-reasoning-via-task-decomposition-a-case-study-in-raven-progressive-matrices","title":"Learning Abstract Visual Reasoning via Task Decomposition: A Case Study in Raven Progressive Matrices","link":"https://arxiv.org/abs/2308.06528","abstract":"arXiv:2308.06528v2 Announce Type: replace  Abstract: Learning to perform abstract reasoning often requires decomposing the task in question into intermediate subgoals that are not specified upfront, but need to be autonomously devised by the learner. In Raven Progressive Matrices (RPM), the task is to choose one of the available answers given a context, where both the context and answers are composite images featuring multiple objects in various spatial arrangements. As this high-level goal is the only guidance available, learning to solve RPMs is challenging. In this study, we propose a deep learning architecture based on the transformer blueprint which, rather than directly making the above choice, addresses the subgoal of predicting the visual properties of individual objects and their arrangements. The multidimensional predictions obtained in this way are then directly juxtaposed to choose the answer. We consider a few ways in which the model parses the visual input into tokens and several regimes of masking parts of the input in self-supervised training. In experimental assessment, the models not only outperform state-of-the-art methods but also provide interesting insights and partial explanations about the inference. The design of the method also makes it immune to biases that are known to be present in some RPM benchmarks.","creator":"Jakub Kwiatkowski, Krzysztof Krawiec"},{"id":"2311.04235","slug":"can-llms-follow-simple-rules","title":"Can LLMs Follow Simple Rules?","link":"https://arxiv.org/abs/2311.04235","abstract":"arXiv:2311.04235v2 Announce Type: replace  Abstract: As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as \"do not generate abusive content\", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietary and open models show that almost all current models struggle to follow scenario rules, even on straightforward test cases. We also demonstrate that simple optimization attacks suffice to significantly increase failure rates on test cases. We conclude by exploring two potential avenues for improvement: test-time steering and supervised fine-tuning.","creator":"Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, David Wagner"},{"id":"2312.14625","slug":"multi-agent-reinforcement-learning-for-assessing-false-data-injection-attacks-on-transportation-networks","title":"Multi-Agent Reinforcement Learning for Assessing False-Data Injection Attacks on Transportation Networks","link":"https://arxiv.org/abs/2312.14625","abstract":"arXiv:2312.14625v2 Announce Type: replace  Abstract: The increasing reliance of drivers on navigation applications has made transportation networks more susceptible to data-manipulation attacks by malicious actors. Adversaries may exploit vulnerabilities in the data collection or processing of navigation services to inject false information, and to thus interfere with the drivers' route selection. Such attacks can significantly increase traffic congestions, resulting in substantial waste of time and resources, and may even disrupt essential services that rely on road networks. To assess the threat posed by such attacks, we introduce a computational framework to find worst-case data-injection attacks against transportation networks. First, we devise an adversarial model with a threat actor who can manipulate drivers by increasing the travel times that they perceive on certain roads. Then, we employ hierarchical multi-agent reinforcement learning to find an approximate optimal adversarial strategy for data manipulation. We demonstrate the applicability of our approach through simulating attacks on the Sioux Falls, ND network topology.","creator":"Taha Eghtesad, Sirui Li, Yevgeniy Vorobeychik, Aron Laszka"},{"id":"2402.04154","slug":"read-to-play-r2-play-decision-transformer-with-multimodal-game-instruction","title":"Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction","link":"https://arxiv.org/abs/2402.04154","abstract":"arXiv:2402.04154v4 Announce Type: replace  Abstract: Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning. However, these works encounter challenges in extending their capabilities to new tasks. Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction. However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks. This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability. Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer. Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities.","creator":"Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He, Jie Fu"},{"id":"2402.08957","slug":"mustard-mastering-uniform-synthesis-of-theorem-and-proof-data","title":"MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data","link":"https://arxiv.org/abs/2402.08957","abstract":"arXiv:2402.08957v2 Announce Type: replace  Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-wise formal solutions. (3) Lastly, the framework utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE with 5,866 valid data points. Each data point contains an informal statement, an informal proof, and a translated formal proof that passes the prover validation. We perform extensive analysis and demonstrate that MUSTARD generates validated high-quality step-by-step data. We further apply the MUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B achieves a 15.41% average relative performance gain in automated theorem proving, and 8.18% in math word problems. Codes and data are available at https://github.com/Eleanor-H/MUSTARD.","creator":"Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang"},{"id":"2402.09565","slug":"graph-skeleton-1-nodes-are-sufficient-to-represent-billion-scale-graph","title":"Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale Graph","link":"https://arxiv.org/abs/2402.09565","abstract":"arXiv:2402.09565v2 Announce Type: replace  Abstract: Due to the ubiquity of graph data on the web, web graph mining has become a hot research spot. Nonetheless, the prevalence of large-scale web graphs in real applications poses significant challenges to storage, computational capacity and graph model design. Despite numerous studies to enhance the scalability of graph models, a noticeable gap remains between academic research and practical web graph mining applications. One major cause is that in most industrial scenarios, only a small part of nodes in a web graph are actually required to be analyzed, where we term these nodes as target nodes, while others as background nodes. In this paper, we argue that properly fetching and condensing the background nodes from massive web graph data might be a more economical shortcut to tackle the obstacles fundamentally. To this end, we make the first attempt to study the problem of massive background nodes compression for target nodes classification. Through extensive experiments, we reveal two critical roles played by the background nodes in target node classification: enhancing structural connectivity between target nodes, and feature correlation with target nodes. Followingthis, we propose a novel Graph-Skeleton1 model, which properly fetches the background nodes, and further condenses the semantic and topological information of background nodes within similar target-background local structures. Extensive experiments on various web graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, for MAG240M dataset with 0.24 billion nodes, our generated skeleton graph achieves highly comparable performance while only containing 1.8% nodes of the original graph.","creator":"Linfeng Cao, Haoran Deng, Yang Yang, Chunping Wang, Lei Chen"},{"id":"2403.03186","slug":"towards-general-computer-control-a-multimodal-agent-for-red-dead-redemption-ii-as-a-case-study","title":"Towards General Computer Control: A Multimodal Agent for Red Dead Redemption II as a Case Study","link":"https://arxiv.org/abs/2403.03186","abstract":"arXiv:2403.03186v2 Announce Type: replace  Abstract: Despite the success in specific tasks and scenarios, existing foundation agents, empowered by large models (LMs) and advanced tools, still cannot generalize to different scenarios, mainly due to dramatic differences in the observations and actions across scenarios. In this work, we propose the General Computer Control (GCC) setting: building foundation agents that can master any computer task by taking only screen images (and possibly audio) of the computer as input, and producing keyboard and mouse operations as output, similar to human-computer interaction. The main challenges of achieving GCC are: 1) the multimodal observations for decision-making, 2) the requirements of accurate control of keyboard and mouse, 3) the need for long-term memory and reasoning, and 4) the abilities of efficient exploration and self-improvement. To target GCC, we introduce Cradle, an agent framework with six main modules, including: 1) information gathering to extract multi-modality information, 2) self-reflection to rethink past experiences, 3) task inference to choose the best next task, 4) skill curation for generating and updating relevant skills for given tasks, 5) action planning to generate specific operations for keyboard and mouse control, and 6) memory for storage and retrieval of past experiences and known skills. To demonstrate the capabilities of generalization and self-improvement of Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as a preliminary attempt towards GCC with a challenging target. To our best knowledge, our work is the first to enable LMM-based agents to follow the main storyline and finish real missions in complex AAA games, with minimal reliance on prior knowledge or resources. The project website is at https://baai-agents.github.io/Cradle/.","creator":"Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun Wang, B\\\"orje F. Karlsson, Bo An, Zongqing Lu"},{"id":"2109.14200","slug":"can-phones-syllables-and-words-emerge-as-side-products-of-cross-situational-audiovisual-learning-a-computational-investigation","title":"Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation","link":"https://arxiv.org/abs/2109.14200","abstract":"arXiv:2109.14200v2 Announce Type: replace-cross  Abstract: Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated...","creator":"Khazar Khorrami, Okko R\\\"as\\\"anen"},{"id":"2209.01621","slug":"interactive-question-answering-systems-literature-review","title":"Interactive Question Answering Systems: Literature Review","link":"https://arxiv.org/abs/2209.01621","abstract":"arXiv:2209.01621v2 Announce Type: replace-cross  Abstract: Question answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. This survey offers a detailed overview of the interactive question-answering methods that are prevalent in current literature. It begins by explaining the foundational principles of question-answering systems, hence defining new notations and taxonomies to combine all identified works inside a unified framework. The reviewed published work on interactive question-answering systems is then presented and examined in terms of its proposed methodology, evaluation approaches, and dataset/application domain. We also describe trends surrounding specific tasks and issues raised by the community, so shedding light on the future interests of scholars. Our work is further supported by a GitHub page with a synthesis of all the major topics covered in this literature study. https://sisinflab.github.io/interactive-question-answering-systems-survey/","creator":"Giovanni Maria Biancofiore, Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio, Fedelucio Narducci"},{"id":"2211.02680","slug":"climbing-routes-clustering-using-energy-efficient-accelerometers-attached-to-the-quickdraws","title":"Climbing Routes Clustering Using Energy-Efficient Accelerometers Attached to the Quickdraws","link":"https://arxiv.org/abs/2211.02680","abstract":"arXiv:2211.02680v2 Announce Type: replace-cross  Abstract: One of the challenges for climbing gyms is to find out popular routes for the climbers to improve their services and optimally use their infrastructure. This problem must be addressed preserving both the privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence becoming practical in terms of expenses and time consumption for replacement when used in large quantities in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in ultra-low power mode, detect patterns in data during climbing different routes, and develops an unsupervised approach for route clustering.","creator":"Sadaf Moaveninejad, Andrea Janes, Camillo Porcaro, Luca Barletta, Lorenzo Mucchi, Massimiliano Pierobon"},{"id":"2212.01551","slug":"quantify-the-causes-of-causal-emergence-critical-conditions-of-uncertainty-and-asymmetry-in-causal-structure","title":"Quantify the Causes of Causal Emergence: Critical Conditions of Uncertainty and Asymmetry in Causal Structure","link":"https://arxiv.org/abs/2212.01551","abstract":"arXiv:2212.01551v3 Announce Type: replace-cross  Abstract: Beneficial to advanced computing devices, models with massive parameters are increasingly employed to extract more information to enhance the precision in describing and predicting the patterns of objective systems. This phenomenon is particularly pronounced in research domains associated with deep learning. However, investigations of causal relationships based on statistical and informational theories have posed an interesting and valuable challenge to large-scale models in the recent decade. Macroscopic models with fewer parameters can outperform their microscopic counterparts with more parameters in effectively representing the system. This valuable situation is called \"Causal Emergence.\" This paper introduces a quantification framework, according to the Effective Information and Transition Probability Matrix, for assessing numerical conditions of Causal Emergence as theoretical constraints of its occurrence. Specifically, our results quantitatively prove the cause of Causal Emergence. By a particular coarse-graining strategy, optimizing uncertainty and asymmetry within the model's causal structure is significantly more influential than losing maximum information due to variations in model scales. Moreover, by delving into the potential exhibited by Partial Information Decomposition and Deep Learning networks in the study of Causal Emergence, we discuss potential application scenarios where our quantification framework could play a role in future investigations of Causal Emergence.","creator":"Liye Jia, Fengyufan Yang, Ka Lok Man, Erick Purwanto, Sheng-Uei Guan, Jeremy Smith, Yutao Yue"},{"id":"2301.10774","slug":"rdesign-hierarchical-data-efficient-representation-learning-for-tertiary-structure-based-rna-design","title":"RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design","link":"https://arxiv.org/abs/2301.10774","abstract":"arXiv:2301.10774v3 Announce Type: replace-cross  Abstract: While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at https://github.com/A4Bio/RDesign.","creator":"Cheng Tan, Yijie Zhang, Zhangyang Gao, Bozhen Hu, Siyuan Li, Zicheng Liu, Stan Z. Li"},{"id":"2305.04288","slug":"towards-achieving-near-optimal-utility-for-privacy-preserving-federated-learning-via-data-generation-and-parameter-distortion","title":"Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion","link":"https://arxiv.org/abs/2305.04288","abstract":"arXiv:2305.04288v3 Announce Type: replace-cross  Abstract: Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \\textit{privacy} and maintaining high model \\textit{utility}. The nature of the widely-adopted protection mechanisms including \\textit{Randomization Mechanism} and \\textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis inspires the design of appropriate protection parameters for the protection mechanisms to achieve near-optimal utility and meet the privacy requirements simultaneously. The main techniques for the protection mechanism include parameter distortion and data generation, which are generic and can be applied extensively. Furthermore, we provide an upper bound for the trade-off between privacy and utility, \\blue{which together with the lower bound provided by no free lunch theorem in federated learning (\\cite{zhang2022no}) form the conditions for achieving optimal trade-off.","creator":"Xiaojin Zhang, Kai Chen, Qiang Yang"},{"id":"2305.05666","slug":"policy-gradient-methods-in-the-presence-of-symmetries-and-state-abstractions","title":"Policy Gradient Methods in the Presence of Symmetries and State Abstractions","link":"https://arxiv.org/abs/2305.05666","abstract":"arXiv:2305.05666v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of Markov decision process (MDP) homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual control tasks from the DeepMind Control Suite. Our method's ability to utilize MDP homomorphisms for representation learning leads to improved performance, and the visualizations of the latent space clearly demonstrate the structure of the learned abstraction.","creator":"Prakash Panangaden, Sahand Rezaei-Shoshtari, Rosie Zhao, David Meger, Doina Precup"},{"id":"2305.16344","slug":"enabling-and-analyzing-how-to-efficiently-extract-information-from-hybrid-long-documents-with-llms","title":"Enabling and Analyzing How to Efficiently Extract Information from Hybrid Long Documents with LLMs","link":"https://arxiv.org/abs/2305.16344","abstract":"arXiv:2305.16344v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored. In this research, we specialize in harnessing the potential of LLMs to comprehend critical information from financial reports, which are hybrid long-documents. We propose an Automated Financial Information Extraction (AFIE) framework that enhances LLMs' ability to comprehend and extract information from financial reports. To evaluate AFIE, we develop a Financial Reports Numerical Extraction (FINE) dataset and conduct an extensive experimental analysis. Our framework is effectively validated on GPT-3.5 and GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively, compared to a naive method. These results suggest that the AFIE framework offers accuracy for automated numerical extraction from complex, hybrid documents.","creator":"Chongjian Yue, Xinrun Xu, Xiaojun Ma, Lun Du, Hengyu Liu, Zhiming Ding, Yanbing Jiang, Shi Han, Dongmei Zhang"},{"id":"2306.06836","slug":"tackling-heavy-tailed-rewards-in-reinforcement-learning-with-function-approximation-minimax-optimal-and-instance-dependent-regret-bounds","title":"Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds","link":"https://arxiv.org/abs/2306.06836","abstract":"arXiv:2306.06836v3 Announce Type: replace-cross  Abstract: While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are \\emph{heavy-tailed}, i.e., with only finite $(1+\\epsilon)$-th moments for some $\\epsilon\\in(0,1]$. In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, \\textsc{Heavy-OFUL}, for heavy-tailed linear bandits, achieving an \\emph{instance-dependent} $T$-round regret of $\\tilde{O}\\big(d T^{\\frac{1-\\epsilon}{2(1+\\epsilon)}} \\sqrt{\\sum_{t=1}^T \\nu_t^2} + d T^{\\frac{1-\\epsilon}{2(1+\\epsilon)}}\\big)$, the \\emph{first} of this kind. Here, $d$ is the feature dimension, and $\\nu_t^{1+\\epsilon}$ is the $(1+\\epsilon)$-th central moment of the reward at the $t$-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in stochastic and deterministic linear bandits. We then extend this algorithm to the RL settings with linear function approximation. Our algorithm, termed as \\textsc{Heavy-LSVI-UCB}, achieves the \\emph{first} computationally efficient \\emph{instance-dependent} $K$-episode regret of $\\tilde{O}(d \\sqrt{H \\mathcal{U}^*} K^\\frac{1}{1+\\epsilon} + d \\sqrt{H \\mathcal{V}^* K})$. Here, $H$ is length of the episode, and $\\mathcal{U}^*, \\mathcal{V}^*$ are instance-dependent quantities scaling with the central moment of reward and value functions, respectively. We also provide a matching minimax lower bound $\\Omega(d H K^{\\frac{1}{1+\\epsilon}} + d \\sqrt{H^3 K})$ to demonstrate the optimality of our algorithm in the worst case. Our result is achieved via a novel robust self-normalized concentration inequality that may be of independent interest in handling heavy-tailed noise in general online regression problems.","creator":"Jiayi Huang, Han Zhong, Liwei Wang, Lin F. Yang"},{"id":"2306.12059","slug":"equiformerv2-improved-equivariant-transformer-for-scaling-to-higher-degree-representations","title":"EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations","link":"https://arxiv.org/abs/2306.12059","abstract":"arXiv:2306.12059v3 Announce Type: replace-cross  Abstract: Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on large-scale OC20 dataset by up to $9\\%$ on forces, $4\\%$ on energies, offers better speed-accuracy trade-offs, and $2\\times$ reduction in DFT calculations needed for computing adsorption energies. Additionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC trained on both OC20 and OC22 datasets, achieving much better data efficiency. Finally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M datasets to better understand the performance gain brought by higher degrees.","creator":"Yi-Lun Liao, Brandon Wood, Abhishek Das, Tess Smidt"},{"id":"2306.16384","slug":"accelerating-sampling-and-aggregation-operations-in-gnn-frameworks-with-gpu-initiated-direct-storage-accesses","title":"Accelerating Sampling and Aggregation Operations in GNN Frameworks with GPU Initiated Direct Storage Accesses","link":"https://arxiv.org/abs/2306.16384","abstract":"arXiv:2306.16384v2 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are emerging as a powerful tool for learning from graph-structured data and performing sophisticated inference tasks in various application domains. Although GNNs have been shown to be effective on modest-sized graphs, training them on large-scale graphs remains a significant challenge due to lack of efficient data access and data movement methods. Existing frameworks for training GNNs use CPUs for graph sampling and feature aggregation, while the training and updating of model weights are executed on GPUs. However, our in-depth profiling shows the CPUs cannot achieve the throughput required to saturate GNN model training throughput, causing gross under-utilization of expensive GPU resources. Furthermore, when the graph and its embeddings do not fit in the CPU memory, the overhead introduced by the operating system, say for handling page-faults, comes in the critical path of execution.   To address these issues, we propose the GPU Initiated Direct Storage Access (GIDS) dataloader, to enable GPU-oriented GNN training for large-scale graphs while efficiently utilizing all hardware resources, such as CPU memory, storage, and GPU memory with a hybrid data placement strategy. By enabling GPU threads to fetch feature vectors directly from storage, GIDS dataloader solves the memory capacity problem for GPU-oriented GNN training. Moreover, GIDS dataloader leverages GPU parallelism to tolerate storage latency and eliminates expensive page-fault overhead. Doing so enables us to design novel optimizations for exploiting locality and increasing effective bandwidth for GNN training. Our evaluation using a single GPU on terabyte-scale GNN datasets shows that GIDS dataloader accelerates the overall DGL GNN training pipeline by up to 392X when compared to the current, state-of-the-art DGL dataloader.","creator":"Jeongmin Brian Park, Vikram Sharma Mailthody, Zaid Qureshi, Wen-mei Hwu"},{"id":"2308.11838","slug":"a-benchmark-study-on-calibration","title":"A Benchmark Study on Calibration","link":"https://arxiv.org/abs/2308.11838","abstract":"arXiv:2308.11838v5 Announce Type: replace-cross  Abstract: Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through the use of specific loss functions, data preprocessing and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different datasets? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS. The project page can be found at https://www.taolinwei.com/calibration-study","creator":"Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu"},{"id":"2309.06553","slug":"query-dependent-prompt-evaluation-and-optimization-with-offline-inverse-rl","title":"Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL","link":"https://arxiv.org/abs/2309.06553","abstract":"arXiv:2309.06553v4 Announce Type: replace-cross  Abstract: In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges that impede the successful and economical design of prompt optimization techniques. One primary issue is the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable. Concurrently, learning via interactions with the LLMs to navigate the expansive natural language prompting space proves to be resource-intensive. To address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data. Such data exists as by-products when diverse prompts are benchmarked on open-accessible datasets. With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model. This model can evaluate any query-prompt pairs without accessing LLMs. Subsequently, a best-of-N strategy is deployed to recommend the optimal prompt. Our experimental evaluations across various LLM scales and arithmetic reasoning datasets underscore both the efficacy and economic viability of the proposed approach.","creator":"Hao Sun, Alihan H\\\"uy\\\"uk, Mihaela van der Schaar"},{"id":"2309.06692","slug":"tackling-the-non-iid-issue-in-heterogeneous-federated-learning-by-gradient-harmonization","title":"Tackling the Non-IID Issue in Heterogeneous Federated Learning by Gradient Harmonization","link":"https://arxiv.org/abs/2309.06692","abstract":"arXiv:2309.06692v2 Announce Type: replace-cross  Abstract: Federated learning (FL) is a privacy-preserving paradigm for collaboratively training a global model from decentralized clients. However, the performance of FL is hindered by non-independent and identically distributed (non-IID) data and device heterogeneity. In this work, we revisit this key challenge through the lens of gradient conflicts on the server side. Specifically, we first investigate the gradient conflict phenomenon among multiple clients and reveal that stronger heterogeneity leads to more severe gradient conflicts. To tackle this issue, we propose FedGH, a simple yet effective method that mitigates local drifts through Gradient Harmonization. This technique projects one gradient vector onto the orthogonal plane of the other within conflicting client pairs. Extensive experiments demonstrate that FedGH consistently enhances multiple state-of-the-art FL baselines across diverse benchmarks and non-IID scenarios. Notably, FedGH yields more significant improvements in scenarios with stronger heterogeneity. As a plug-and-play module, FedGH can be seamlessly integrated into any FL framework without requiring hyperparameter tuning.","creator":"Xinyu Zhang, Weiyu Sun, Ying Chen"},{"id":"2309.11143","slug":"cot-bert-enhancing-unsupervised-sentence-representation-through-chain-of-thought","title":"CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought","link":"https://arxiv.org/abs/2309.11143","abstract":"arXiv:2309.11143v3 Announce Type: replace-cross  Abstract: Unsupervised sentence representation learning endeavors to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent strides in this domain have been significantly propelled by breakthroughs in contrastive learning and prompt engineering. Despite these advancements, the field has reached a plateau, leading some researchers to incorporate external components to enhance sentence embeddings' quality. Such integration, though beneficial, complicates the solutions and inflates the demand for computational resources. In response to these challenges, this paper presents CoT-BERT, an innovative method that harnesses the progressive thinking of Chain-of-Thought reasoning to tap into the latent potential of pre-trained models like BERT. Additionally, we develop an advanced contrastive learning loss function and propose a novel template denoising strategy. Rigorous experimentation substantiates CoT-BERT surpasses a range of well-established baselines by relying exclusively on the intrinsic strengths of pre-trained models.","creator":"Bowen Zhang, Kehua Chang, Chunping Li"},{"id":"2309.12708","slug":"pointssc-a-cooperative-vehicle-infrastructure-point-cloud-benchmark-for-semantic-scene-completion","title":"PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for Semantic Scene Completion","link":"https://arxiv.org/abs/2309.12708","abstract":"arXiv:2309.12708v2 Announce Type: replace-cross  Abstract: Semantic Scene Completion (SSC) aims to jointly generate space occupancies and semantic labels for complex 3D scenes. Most existing SSC models focus on volumetric representations, which are memory-inefficient for large outdoor spaces. Point clouds provide a lightweight alternative but existing benchmarks lack outdoor point cloud scenes with semantic labels. To address this, we introduce PointSSC, the first cooperative vehicle-infrastructure point cloud benchmark for semantic scene completion. These scenes exhibit long-range perception and minimal occlusion. We develop an automated annotation pipeline leveraging Semantic Segment Anything to efficiently assign semantics. To benchmark progress, we propose a LiDAR-based model with a Spatial-Aware Transformer for global and local feature extraction and a Completion and Segmentation Cooperative Module for joint completion and segmentation. PointSSC provides a challenging testbed to drive advances in semantic point cloud completion for real-world navigation. The code and datasets are available at https://github.com/yyxssm/PointSSC.","creator":"Yuxiang Yan, Boda Liu, Jianfei Ai, Qinbu Li, Ru Wan, Jian Pu"},{"id":"2310.02772","slug":"spike-accumulation-forwarding-for-effective-training-of-spiking-neural-networks","title":"Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks","link":"https://arxiv.org/abs/2310.02772","abstract":"arXiv:2310.02772v5 Announce Type: replace-cross  Abstract: In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the above contents through experiments and showed that it is possible to reduce memory and training time while maintaining accuracy.","creator":"Ryuji Saiin, Tomoya Shirakawa, Sota Yoshihara, Yoshihide Sawada, Hiroyuki Kusumoto"},{"id":"2310.04218","slug":"a-fixed-parameter-tractable-algorithm-for-counting-markov-equivalence-classes-with-the-same-skeleton","title":"A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton","link":"https://arxiv.org/abs/2310.04218","abstract":"arXiv:2310.04218v3 Announce Type: replace-cross  Abstract: Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same \"skeleton\" (underlying undirected graph) and v-structures (induced subgraph of the form $a\\rightarrow b \\leftarrow c$).   These combinatorial characterizations also suggest several natural algorithmic questions. One of these is: given an undirected graph $G$ as input, how many distinct Markov equivalence classes have the skeleton $G$? Much work has been devoted in the last few years to this and other closely related problems. However, to the best of our knowledge, a polynomial time algorithm for the problem remains unknown.   In this paper, we make progress towards this goal by giving a fixed parameter tractable algorithm for the above problem, with the parameters being the treewidth and the maximum degree of the input graph $G$. The main technical ingredient in our work is a construction we refer to as shadow, which lets us create a \"local description\" of long-range constraints imposed by the combinatorial characterizations of MECs.","creator":"Vidya Sagar Sharma"},{"id":"2310.07699","slug":"veclip-improving-clip-training-via-visual-enriched-captions","title":"VeCLIP: Improving CLIP Training via Visual-enriched Captions","link":"https://arxiv.org/abs/2310.07699","abstract":"arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our dataset up to 300 million samples named VeCap dataset. Our results show significant advantages in image-text alignment and overall model performance. For example, VeCLIP achieves up to +25.2% gain in COCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency, VeCLIP achieves +3% gain while only using 14% of the data employed in the vanilla CLIP and 11% in ALIGN. We also note the VeCap data is complementary with other well curated datasets good for zero-shot classification tasks. When combining VeCap and DFN, our model can achieve strong performance on both of image-text retrieval and zero-shot classification tasks, e.g. 83.1% accuracy@1 on ImageNet zero-shot for a H/14 model. We release the pre-trained models at https://github.com/apple/ml-veclip.","creator":"Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, Meng Cao"},{"id":"2310.07793","slug":"gentkg-generative-forecasting-on-temporal-knowledge-graph","title":"GenTKG: Generative Forecasting on Temporal Knowledge Graph","link":"https://arxiv.org/abs/2310.07793","abstract":"arXiv:2310.07793v3 Announce Type: replace-cross  Abstract: The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional embedding-based and rule-based methods dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval-augmented generation framework named GenTKG combining a temporal logical rule-based retrieval strategy and few-shot parameter-efficient instruction tuning to solve the above challenges, respectively. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting with low computation resources using extremely limited training data as few as 16 samples. GenTKG also highlights remarkable cross-domain generalizability with outperforming performance on unseen datasets without re-training, and in-domain generalizability regardless of time split in the same dataset. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs.","creator":"Ruotong Liao, Xu Jia, Yunpu Ma, Yangzhe Li, Volker Tresp"},{"id":"2310.18191","slug":"is-scaling-learned-optimizers-worth-it-evaluating-the-value-of-velo-s-4000-tpu-months","title":"Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months","link":"https://arxiv.org/abs/2310.18191","abstract":"arXiv:2310.18191v2 Announce Type: replace-cross  Abstract: We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose \"foundational\" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.","creator":"Fady Rezk, Antreas Antoniou, Henry Gouk, Timothy Hospedales"},{"id":"2311.01534","slug":"approximate-multiagent-reinforcement-learning-for-on-demand-urban-mobility-problem-on-a-large-map-extended-version","title":"Approximate Multiagent Reinforcement Learning for On-Demand Urban Mobility Problem on a Large Map (extended version)","link":"https://arxiv.org/abs/2311.01534","abstract":"arXiv:2311.01534v2 Announce Type: replace-cross  Abstract: In this paper, we focus on the autonomous multiagent taxi routing problem for a large urban environment where the location and number of future ride requests are unknown a-priori, but can be estimated by an empirical distribution. Recent theory has shown that a rollout algorithm with a stable base policy produces a near-optimal stable policy. In the routing setting, a policy is stable if its execution keeps the number of outstanding requests uniformly bounded over time. Although, rollout-based approaches are well-suited for learning cooperative multiagent policies with considerations for future demand, applying such methods to a large urban environment can be computationally expensive due to the large number of taxis required for stability. In this paper, we aim to address the computational bottleneck of multiagent rollout by proposing an approximate multiagent rollout-based two phase algorithm that reduces computational costs, while still achieving a stable near-optimal policy. Our approach partitions the graph into sectors based on the predicted demand and the maximum number of taxis that can run sequentially given the user's computational resources. The algorithm then applies instantaneous assignment (IA) for re-balancing taxis across sectors and a sector-wide multiagent rollout algorithm that is executed in parallel for each sector. We provide two main theoretical results: 1) characterize the number of taxis $m$ that is sufficient for IA to be stable; 2) derive a necessary condition on $m$ to maintain stability for IA as time goes to infinity. Our numerical results show that our approach achieves stability for an $m$ that satisfies the theoretical conditions. We also empirically demonstrate that our proposed two phase algorithm has equivalent performance to the one-at-a-time rollout over the entire map, but with significantly lower runtimes.","creator":"Daniel Garces, Sushmita Bhattacharya, Dimitri Bertsekas, Stephanie Gil"},{"id":"2311.07619","slug":"modeling-user-viewing-flow-using-large-language-models-for-article-recommendation","title":"Modeling User Viewing Flow Using Large Language Models for Article Recommendation","link":"https://arxiv.org/abs/2311.07619","abstract":"arXiv:2311.07619v2 Announce Type: replace-cross  Abstract: This paper proposes the User Viewing Flow Modeling (SINGLE) method for the article recommendation task, which models the user constant preference and instant interest from user-clicked articles. Specifically, we first employ a user constant viewing flow modeling method to summarize the user's general interest to recommend articles. In this case, we utilize Large Language Models (LLMs) to capture constant user preferences from previously clicked articles, such as skills and positions. Then we design the user instant viewing flow modeling method to build interactions between user-clicked article history and candidate articles. It attentively reads the representations of user-clicked articles and aims to learn the user's different interest views to match the candidate article. Our experimental results on the Alibaba Technology Association (ATA) website show the advantage of SINGLE, achieving a 2.4% improvement over previous baseline models in the online A/B test. Our further analyses illustrate that SINGLE has the ability to build a more tailored recommendation system by mimicking different article viewing behaviors of users and recommending more appropriate and diverse articles to match user interests.","creator":"Zhenghao Liu, Zulong Chen, Moufeng Zhang, Shaoyang Duan, Hong Wen, Liangyue Li, Nan Li, Yu Gu, Ge Yu"},{"id":"2311.08094","slug":"skelvit-consensus-of-vision-transformers-for-a-lightweight-skeleton-based-action-recognition-system","title":"SkelVIT: Consensus of Vision Transformers for a Lightweight Skeleton-Based Action Recognition System","link":"https://arxiv.org/abs/2311.08094","abstract":"arXiv:2311.08094v2 Announce Type: replace-cross  Abstract: Skeleton-based action recognition receives the attention of many researchers as it is robust to viewpoint and illumination changes, and its processing is much more efficient than the processing of video frames. With the emergence of deep learning models, it has become very popular to represent the skeleton data in pseudo-image form and apply CNN for action recognition. Thereafter, studies concentrated on finding effective methods for forming pseudo-images. Recently, attention networks, more specifically transformers have provided promising results in various vision problems. In this study, the effectiveness of VIT for skeleton-based action recognition is examined and its robustness on the pseudo-image representation scheme is investigated. To this end, a three-level architecture, SkelVit is proposed, which forms a set of pseudo images, applies a classifier on each of the representations, and combines their results to find the final action class. The performance of SkelVit is examined thoroughly via a set of experiments. First, the sensitivity of the system to representation is investigated by comparing it with two of the state-of-the-art pseudo-image representation methods. Then, the classifiers of SkelVit are realized in two experimental setups by CNNs and VITs, and their performances are compared. In the final experimental setup, the contribution of combining classifiers is examined by applying the model with a different number of classifiers. Experimental studies reveal that the proposed system with its lightweight representation scheme achieves better results than the state-of-the-art methods. It is also observed that the vision transformer is less sensitive to the initial pseudo-image representation compared to CNN. Nevertheless, even with the vision transformer, the recognition performance can be further improved by the consensus of classifiers.","creator":"Ozge Oztimur Karadag"},{"id":"2311.09796","slug":"interpreting-user-requests-in-the-context-of-natural-language-standing-instructions","title":"Interpreting User Requests in the Context of Natural Language Standing Instructions","link":"https://arxiv.org/abs/2311.09796","abstract":"arXiv:2311.09796v2 Announce Type: replace-cross  Abstract: Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. We describe an approach to LLM-based dialogue modeling in which persistent user constraints and preferences -- collectively termed standing instructions -- as additional context for such interfaces. For example, when a user states \"I'm hungry\", a previously expressed preference for Persian food can be automatically added to the LLM prompt, influencing the search for relevant restaurants. We develop NLSI, a language-to-program dataset consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is paired with a user profile (a set of users specific standing instructions) and corresponding structured representations (API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains diverse phenomena, from simple preferences to interdependent instructions such as triggering a hotel search whenever the user is booking tickets to an event. We conduct experiments on NLSI using prompting with large language models and various retrieval approaches, achieving a maximum of 44.7% exact match on API prediction. Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls.","creator":"Nikita Moghe, Patrick Xia, Jacob Andreas, Jason Eisner, Benjamin Van Durme, Harsh Jhamtani"},{"id":"2311.12943","slug":"interact-transformer-models-for-human-intent-prediction-conditioned-on-robot-actions","title":"InteRACT: Transformer Models for Human Intent Prediction Conditioned on Robot Actions","link":"https://arxiv.org/abs/2311.12943","abstract":"arXiv:2311.12943v3 Announce Type: replace-cross  Abstract: In collaborative human-robot manipulation, a robot must predict human intents and adapt its actions accordingly to smoothly execute tasks. However, the human's intent in turn depends on actions the robot takes, creating a chicken-or-egg problem. Prior methods ignore such inter-dependency and instead train marginal intent prediction models independent of robot actions. This is because training conditional models is hard given a lack of paired human-robot interaction datasets. Can we instead leverage large-scale human-human interaction data that is more easily accessible? Our key insight is to exploit a correspondence between human and robot actions that enables transfer learning from human-human to human-robot data. We propose a novel architecture, InteRACT, that pre-trains a conditional intent prediction model on large human-human datasets and fine-tunes on a small human-robot dataset. We evaluate on a set of real-world collaborative human-robot manipulation tasks and show that our conditional model improves over various marginal baselines. We also introduce new techniques to tele-operate a 7-DoF robot arm and collect a diverse range of human-robot collaborative manipulation data, which we open-source.","creator":"Kushal Kedia, Atiksh Bhardwaj, Prithwish Dan, Sanjiban Choudhury"},{"id":"2311.16176","slug":"mitigating-biases-with-diverse-ensembles-and-diffusion-models","title":"Mitigating Biases with Diverse Ensembles and Diffusion Models","link":"https://arxiv.org/abs/2311.16176","abstract":"arXiv:2311.16176v3 Announce Type: replace-cross  Abstract: Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut learning, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) to mitigate this form of bias. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on primary shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalization and diversification performance on par with prior work that relies on auxiliary data collection.","creator":"Luca Scimeca, Alexander Rubinstein, Damien Teney, Seong Joon Oh, Armand Mihai Nicolicioiu, Yoshua Bengio"},{"id":"2312.06681","slug":"steering-llama-2-via-contrastive-activation-addition","title":"Steering Llama 2 via Contrastive Activation Addition","link":"https://arxiv.org/abs/2312.06681","abstract":"arXiv:2312.06681v3 Announce Type: replace-cross  Abstract: We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes \"steering vectors\" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).","creator":"Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, Alexander Matt Turner"},{"id":"2401.00110","slug":"diffusion-model-with-perceptual-loss","title":"Diffusion Model with Perceptual Loss","link":"https://arxiv.org/abs/2401.00110","abstract":"arXiv:2401.00110v5 Announce Type: replace-cross  Abstract: Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, we show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the conditional input and therefore does not sacrifice sample diversity. Our method can also improve sample quality for unconditional generation, which was not possible with classifier-free guidance before.","creator":"Shanchuan Lin, Xiao Yang"},{"id":"2401.00230","slug":"transformer-multivariate-forecasting-less-is-more","title":"Transformer Multivariate Forecasting: Less is More?","link":"https://arxiv.org/abs/2401.00230","abstract":"arXiv:2401.00230v2 Announce Type: replace-cross  Abstract: In the domain of multivariate forecasting, transformer models stand out as powerful apparatus, displaying exceptional capabilities in handling messy datasets from real-world contexts. However, the inherent complexity of these datasets, characterized by numerous variables and lengthy temporal sequences, poses challenges, including increased noise and extended model runtime. This paper focuses on reducing redundant information to elevate forecasting accuracy while optimizing runtime efficiency. We propose a novel transformer forecasting framework enhanced by Principal Component Analysis (PCA) to tackle this challenge. The framework is evaluated by five state-of-the-art (SOTA) models and four diverse real-world datasets. Our experimental results demonstrate the framework's ability to minimize prediction errors across all models and datasets while significantly reducing runtime. From the model perspective, one of the PCA-enhanced models: PCA+Crossformer, reduces mean square errors (MSE) by 33.3% and decreases runtime by 49.2% on average. From the dataset perspective, the framework delivers 14.3% MSE and 76.6% runtime reduction on Electricity datasets, as well as 4.8% MSE and 86.9% runtime reduction on Traffic datasets. This study aims to advance various SOTA models and enhance transformer-based time series forecasting for intricate data. Code is available at: https://github.com/jingjing-unilu/PCA_Transformer.","creator":"Jingjing Xu, Caesar Wu, Yuan-Fang Li, Pascal Bouvry"},{"id":"2401.08095","slug":"durflex-evc-duration-flexible-emotional-voice-conversion-with-parallel-generation","title":"DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel Generation","link":"https://arxiv.org/abs/2401.08095","abstract":"arXiv:2401.08095v2 Announce Type: replace-cross  Abstract: Emotional voice conversion (EVC) seeks to modify the emotional tone of a speaker's voice while preserving the original linguistic content and the speaker's unique vocal characteristics. Recent advancements in EVC have involved the simultaneous modeling of pitch and duration, utilizing the potential of sequence-to-sequence (seq2seq) models. To enhance reliability and efficiency in conversion, this study shifts focus towards parallel speech generation. We introduce Duration-Flexible EVC (DurFlex-EVC), which integrates a style autoencoder and unit aligner. Traditional models, while incorporating self-supervised learning (SSL) representations that contain both linguistic and paralinguistic information, have neglected this dual nature, leading to reduced controllability. Addressing this issue, we implement cross-attention to synchronize these representations with various emotions. Additionally, a style autoencoder is developed for the disentanglement and manipulation of style elements. The efficacy of our approach is validated through both subjective and objective evaluations, establishing its superiority over existing models in the field.","creator":"Hyung-Seok Oh, Sang-Hoon Lee, Deok-Hyeon Cho, Seong-Whan Lee"},{"id":"2401.10712","slug":"q-a-prompts-discovering-rich-visual-clues-through-mining-question-answer-prompts-for-vqa-requiring-diverse-world-knowledge","title":"Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge","link":"https://arxiv.org/abs/2401.10712","abstract":"arXiv:2401.10712v3 Announce Type: replace-cross  Abstract: With the breakthrough of multi-modal large language models, answering complex visual questions that demand advanced reasoning abilities and world knowledge has become a much more important testbed for developing AI models than ever. However, equipping AI models with robust cross-modality reasoning ability remains challenging since the cognition scheme of humans has not been understood systematically. In this paper, we believe that if we can collect visual clues in the given image as much as possible, we will recognize the image more accurately, understand the question better, recall relevant knowledge more easily, and finally reason out the answer. We discover these rich visual clues by mining question-answer pairs in images and sending them into multi-modal large language models as prompts. We call the proposed method Q&A Prompts. Specifically, we first use the image-answer pairs and the corresponding questions in the training set as inputs and outputs to train a visual question generation model. Then, we use an image tagging model to identify various instances and send packaged image-tag pairs into the visual question generation model to generate relevant questions with the extracted image tags as answers. Finally, we encode these generated question-answer pairs as prompts with a visual-aware prompting module and send them into pre-trained multi-modal large language models to reason out the final answers. Experimental results show that, compared with state-of-the-art methods, our Q&A Prompts achieves substantial improvements on the challenging visual question answering datasets requiring reasoning over diverse world knowledge, such as OK-VQA and A-OKVQA.","creator":"Haibi Wang, Weifeng Ge"},{"id":"2401.14362","slug":"the-typing-cure-experiences-with-large-language-model-chatbots-for-mental-health-support","title":"The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support","link":"https://arxiv.org/abs/2401.14362","abstract":"arXiv:2401.14362v2 Announce Type: replace-cross  Abstract: People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethical and effective use of LLM chatbots and other AI mental health support tools in mental health care.","creator":"Inhwa Song, Sachin R. Pendse, Neha Kumar, Munmun De Choudhury"},{"id":"2401.17435","slug":"can-large-language-models-replace-economic-choice-prediction-labs","title":"Can Large Language Models Replace Economic Choice Prediction Labs?","link":"https://arxiv.org/abs/2401.17435","abstract":"arXiv:2401.17435v3 Announce Type: replace-cross  Abstract: Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predict human behavior in a language-based persuasion game, and can even outperform models trained on actual human data.","creator":"Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz"},{"id":"2402.12226","slug":"anygpt-unified-multimodal-llm-with-discrete-sequence-modeling","title":"AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling","link":"https://arxiv.org/abs/2402.12226","abstract":"arXiv:2402.12226v3 Announce Type: replace-cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/","creator":"Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, Xipeng Qiu"},{"id":"2402.13602","slug":"hybrid-reasoning-based-on-large-language-models-for-autonomous-car-driving","title":"Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving","link":"https://arxiv.org/abs/2402.13602","abstract":"arXiv:2402.13602v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems.","creator":"Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab, Achim Rettberg"},{"id":"2402.13754","slug":"reinforcement-learning-assisted-quantum-architecture-search-for-variational-quantum-algorithms","title":"Reinforcement learning-assisted quantum architecture search for variational quantum algorithms","link":"https://arxiv.org/abs/2402.13754","abstract":"arXiv:2402.13754v3 Announce Type: replace-cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accuracy in solving the given problem. The task of automating the search for optimal quantum circuits is known as quantum architecture search (QAS). The majority of research in QAS is primarily focused on a noiseless scenario. Yet, the impact of noise on the QAS remains inadequately explored. In this thesis, we tackle the issue by introducing a tensor-based quantum circuit encoding, restrictions on environment dynamics to explore the search space of possible circuits efficiently, an episode halting scheme to steer the agent to find shorter circuits, a double deep Q-network (DDQN) with an $\\epsilon$-greedy policy for better stability. The numerical experiments on noiseless and noisy quantum hardware show that in dealing with various VQAs, our RL-based QAS outperforms existing QAS. Meanwhile, the methods we propose in the thesis can be readily adapted to address a wide range of other VQAs.","creator":"Akash Kundu"},{"id":"2402.14860","slug":"ranking-large-language-models-without-ground-truth","title":"Ranking Large Language Models without Ground Truth","link":"https://arxiv.org/abs/2402.14860","abstract":"arXiv:2402.14860v2 Announce Type: replace-cross  Abstract: Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable low-resource mechanism for practical use.","creator":"Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy"},{"id":"2402.16899","slug":"a-priori-estimates-for-deep-residual-network-in-continuous-time-reinforcement-learning","title":"A priori Estimates for Deep Residual Network in Continuous-time Reinforcement Learning","link":"https://arxiv.org/abs/2402.16899","abstract":"arXiv:2402.16899v3 Announce Type: replace-cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \\emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \\emph{a priori} generalization error without the curse of dimensionality.","creator":"Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo"},{"id":"2402.17785","slug":"bytecomposer-a-human-like-melody-composition-method-based-on-language-model-agent","title":"ByteComposer: a Human-like Melody Composition Method based on Language Model Agent","link":"https://arxiv.org/abs/2402.17785","abstract":"arXiv:2402.17785v2 Announce Type: replace-cross  Abstract: Large Language Models (LLM) have shown encouraging progress in multimodal understanding and generation tasks. However, how to design a human-aligned and interpretable melody composition system is still under-explored. To solve this problem, we propose ByteComposer, an agent framework emulating a human's creative pipeline in four separate steps : \"Conception Analysis - Draft Composition - Self-Evaluation and Modification - Aesthetic Selection\". This framework seamlessly blends the interactive and knowledge-understanding features of LLMs with existing symbolic music generation models, thereby achieving a melody composition agent comparable to human creators. We conduct extensive experiments on GPT4 and several open-source large language models, which substantiate our framework's effectiveness. Furthermore, professional music composers were engaged in multi-dimensional evaluations, the final results demonstrated that across various facets of music composition, ByteComposer agent attains the level of a novice melody composer.","creator":"Xia Liang, Xingjian Du, Jiaju Lin, Pei Zou, Yuan Wan, Bilei Zhu"},{"id":"2403.00758","slug":"mitigating-reversal-curse-via-semantic-aware-permutation-training","title":"Mitigating Reversal Curse via Semantic-aware Permutation Training","link":"https://arxiv.org/abs/2403.00758","abstract":"arXiv:2403.00758v2 Announce Type: replace-cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the \"reversal curse\". It is a typical example that the model knows \"A's father is B\", but is unable to reason \"B's child is A\". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may disrupt complete phrases or entities, thereby posing challenges for the model to comprehend and learn from training data. To address this issue, we propose Semantic-aware Permutation Training (SPT), which addresses this issue by segmenting the training sentences into semantic units (i.e., entities or phrases) with an assistant language model and permuting these units before feeding into the model. Extensive experiments demonstrate that SPT effectively mitigates the reversal curse since the performance on reversed questions approximates that on the forward ones, and significantly advances the performance of existing works.","creator":"Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, Yujiu Yang"},{"id":"2403.00858","slug":"direct-alignment-of-draft-model-for-speculative-decoding-with-chat-fine-tuned-llms","title":"Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs","link":"https://arxiv.org/abs/2403.00858","abstract":"arXiv:2403.00858v2 Announce Type: replace-cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional alignment procedure. For the finetuning step, we use instruction-response pairs generated by target model for distillation in plausible data distribution, and propose a new Total Variation Distance++ (TVD++) loss that incorporates variance reduction techniques inspired from the policy gradient method in reinforcement learning. Our empirical results show that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3 block efficiency and 2.4$\\times$ speed-up relative to autoregressive decoding on various tasks with no further task-specific fine-tuning.","creator":"Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, Christopher Lott"},{"id":"2403.00986","slug":"merging-text-transformer-models-from-different-initializations","title":"Merging Text Transformer Models from Different Initializations","link":"https://arxiv.org/abs/2403.00986","abstract":"arXiv:2403.00986v2 Announce Type: replace-cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a masked-language modeling task or fine-tuned on a language understanding benchmark. Our results show that the minima of these models are less sharp and isolated than previously understood, and provide a basis for future work on merging separately trained Transformer models.","creator":"Neha Verma, Maha Elbayad"},{"id":"2403.02131","slug":"deep-reinforcement-learning-for-dynamic-algorithm-selection-a-proof-of-principle-study-on-differential-evolution","title":"Deep Reinforcement Learning for Dynamic Algorithm Selection: A Proof-of-Principle Study on Differential Evolution","link":"https://arxiv.org/abs/2403.02131","abstract":"arXiv:2403.02131v3 Announce Type: replace-cross  Abstract: Evolutionary algorithms, such as Differential Evolution, excel in solving real-parameter optimization challenges. However, the effectiveness of a single algorithm varies across different problem instances, necessitating considerable efforts in algorithm selection or configuration. This paper aims to address the limitation by leveraging the complementary strengths of a group of algorithms and dynamically scheduling them throughout the optimization progress for specific problems. We propose a deep reinforcement learning-based dynamic algorithm selection framework to accomplish this task. Our approach models the dynamic algorithm selection a Markov Decision Process, training an agent in a policy gradient manner to select the most suitable algorithm according to the features observed during the optimization process. To empower the agent with the necessary information, our framework incorporates a thoughtful design of landscape and algorithmic features. Meanwhile, we employ a sophisticated deep neural network model to infer the optimal action, ensuring informed algorithm selections. Additionally, an algorithm context restoration mechanism is embedded to facilitate smooth switching among different algorithms. These mechanisms together enable our framework to seamlessly select and switch algorithms in a dynamic online fashion. Notably, the proposed framework is simple and generic, offering potential improvements across a broad spectrum of evolutionary algorithms. As a proof-of-principle study, we apply this framework to a group of Differential Evolution algorithms. The experimental results showcase the remarkable effectiveness of the proposed framework, not only enhancing the overall optimization performance but also demonstrating favorable generalization ability across different problem classes.","creator":"Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang, Zhiguang Cao, Jun Zhang, Yue-Jiao Gong"},{"id":"2403.03218","slug":"the-wmdp-benchmark-measuring-and-reducing-malicious-use-with-unlearning","title":"The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning","link":"https://arxiv.org/abs/2403.03218","abstract":"arXiv:2403.03218v2 Announce Type: replace-cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop CUT, a state-of-the-art unlearning method based on controlling model representations. CUT reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai","creator":"Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, Dan Hendrycks"},{"id":"2403.03456","slug":"dlp-gan-learning-to-draw-modern-chinese-landscape-photos-with-generative-adversarial-network","title":"DLP-GAN: learning to draw modern Chinese landscape photos with generative adversarial network","link":"https://arxiv.org/abs/2403.03456","abstract":"arXiv:2403.03456v2 Announce Type: replace-cross  Abstract: Chinese landscape painting has a unique and artistic style, and its drawing technique is highly abstract in both the use of color and the realistic representation of objects. Previous methods focus on transferring from modern photos to ancient ink paintings. However, little attention has been paid to translating landscape paintings into modern photos. To solve such problems, in this paper, we (1) propose DLP-GAN (Draw Modern Chinese Landscape Photos with Generative Adversarial Network), an unsupervised cross-domain image translation framework with a novel asymmetric cycle mapping, and (2) introduce a generator based on a dense-fusion module to match different translation directions. Moreover, a dual-consistency loss is proposed to balance the realism and abstraction of model painting. In this way, our model can draw landscape photos and sketches in the modern sense. Finally, based on our collection of modern landscape and sketch datasets, we compare the images generated by our model with other benchmarks. Extensive experiments including user studies show that our model outperforms state-of-the-art methods.","creator":"Xiangquan Gui, Binxuan Zhang, Li Li, Yi Yang"},{"id":"2403.03643","slug":"a-survey-on-applications-of-reinforcement-learning-in-spatial-resource-allocation","title":"A Survey on Applications of Reinforcement Learning in Spatial Resource Allocation","link":"https://arxiv.org/abs/2403.03643","abstract":"arXiv:2403.03643v2 Announce Type: replace-cross  Abstract: The challenge of spatial resource allocation is pervasive across various domains such as transportation, industry, and daily life. As the scale of real-world issues continues to expand and demands for real-time solutions increase, traditional algorithms face significant computational pressures, struggling to achieve optimal efficiency and real-time capabilities. In recent years, with the escalating computational power of computers, the remarkable achievements of reinforcement learning in domains like Go and robotics have demonstrated its robust learning and sequential decision-making capabilities. Given these advancements, there has been a surge in novel methods employing reinforcement learning to tackle spatial resource allocation problems. These methods exhibit advantages such as rapid solution convergence and strong model generalization abilities, offering a new perspective on resolving spatial resource allocation problems. Therefore, this paper aims to summarize and review recent theoretical methods and applied research utilizing reinforcement learning to address spatial resource allocation problems. It provides a summary and comprehensive overview of its fundamental principles, related methodologies, and applied research. Additionally, it highlights several unresolved issues that urgently require attention in this direction for the future.","creator":"Di Zhang, Moyang Wang, Joseph Mango, Xiang Li, Xianrui Xu"}]},{"name":"Plant Biology","feed":[{"id":"2024.03.04.583280v1","slug":"microbe-tree-metabolite-interactions-in-the-soil-phyllosphere-continuum-of-poplar-tree-when-microbes-rewire-poplar-root-exudate-and-metabolome","title":"Microbe tree metabolite interactions in the soil - phyllosphere continuum of poplar tree: when microbes rewire poplar root exudate and metabolome","link":"http://biorxiv.org/cgi/content/short/2024.03.04.583280v1?rss=1","abstract":"O_LITrees are associated with a broad range of microorganisms colonising the diverse tissues of their host. However, the early dynamics of the assembly of the microbiota from the root to shoot axis and how it is linked to root exudates and metabolite contents of tissues remain unclear. C_LIO_LIHere, we characterized how fungal and bacterial communities are altering root exudates as well as root and shoot metabolomes in parallel with their establishment in poplar cuttings (Populus tremula x tremuloides clone T89) over 30 days of growth. Sterile poplar cuttings were planted in natural or gamma-irradiated soils. Bulk and rhizospheric soils, root and shoot tissues were collected from day 1 to day 30 to track the dynamic changes of fungal and bacterial communities in the different habitats by DNA metabarcoding. Root exudates and root and shoot metabolites were analysed in parallel by gas chromatography-mass spectrometry. C_LIO_LIOur study reveals that microbial colonization triggered rapid and substantial alterations in both the composition and quantity of root exudates, with over 70 metabolites exclusively identified in remarkably high abundances in the absence of microorganisms. Noteworthy among these were lipid-related metabolites and defence compounds. The microbial colonization of both roots and shoots exhibited a similar dynamic response, initially involving saprophytic microorganisms and later transitioning to endophytes and symbionts. Key constituents of the shoot microbiota were also discernible at earlier time points in the rhizosphere and roots, indicating that the soil constituted a primary source for shoot microbiota. C_LIO_LIFurthermore, the microbial colonization of belowground and aerial compartments induced a reconfiguration of plant metabolism. Specifically, microbial colonization predominantly instigated alterations in primary metabolism in roots, while in shoots, it primarily influenced defence metabolism. This highlighted the profound impact of microbial interactions on metabolic pathways of plants, shedding light on the intricate interplay between plants and their associated microbial communities. C_LI","creator":"Fracchia, F., Guinet, F., Engle, N., Tschaplinski, T., Veneault-Fourrey, C., Deveau, A."},{"id":"2024.03.04.583414v1","slug":"large-scale-single-cell-profiling-of-stem-cells-uncovers-redundant-regulators-of-shoot-development-and-yield-trait-variation","title":"Large-scale single-cell profiling of stem cells uncovers redundant regulators of shoot development and yield trait variation","link":"http://biorxiv.org/cgi/content/short/2024.03.04.583414v1?rss=1","abstract":"Stem cells in plant shoots are a rare population of cells that produce leaves, fruits and seeds, vital sources for food and bioethanol. Uncovering regulators expressed in these stem cells will inform crop engineering to boost productivity. Single-cell analysis is a powerful tool for identifying regulators expressed in specific groups of cells. However, accessing plant shoot stem cells is challenging. Recent single-cell analyses of plant shoots have not captured these cells, and failed to detect stem cell regulators like CLAVATA3 and WUSCHEL. In this study, we finely dissected stem cell-enriched shoot tissues from both maize and arabidopsis for single-cell RNA-seq profiling. We optimized protocols to efficiently recover thousands of CLAVATA3 and WUSCHEL expressed cells. A cross-species comparison identified conserved stem cell regulators between maize and arabidopsis. We also performed single-cell RNA-seq on maize stem cell overproliferation mutants to find additional candidate regulators. Expression of candidate stem cell genes was validated using spatial transcriptomics, and we functionally confirmed roles in shoot development. These candidates include a family of ribosome-associated RNA-binding proteins, and two families of sugar kinase genes related to hypoxia signaling and cytokinin hormone homeostasis. These large-scale single-cell profiling of stem cells provide a resource for mining stem cell regulators, which show significant association with yield traits. Overall, our discoveries advance the understanding of shoot development and open avenues for manipulating diverse crops to enhance food and energy security.","creator":"Xu, X., Passalacqua, M., Rice, B., Demesa-Arevalo, E., Kojima, M., Takebayashi, Y., Harris, B., Sakakibara, H., Gallavotti, A., Gillis, J., Jackson, D."},{"id":"2024.03.05.583564v1","slug":"high-throughput-phenotyping-of-seed-quality-traits-using-imaging-and-deep-learning-in-dry-pea","title":"High-Throughput Phenotyping of Seed Quality Traits Using Imaging and Deep Learning in Dry Pea","link":"http://biorxiv.org/cgi/content/short/2024.03.05.583564v1?rss=1","abstract":"Seed traits, such as seed color and seed size, directly impact seed quality, affecting the marketability and value of dry peas [1]. Assessing seed quality is integral to a plant breeding programs to ensure optimal seed standards. This research introduced a phenotyping tool to assess seed quality traits specifically tailored for pulse crops, which integrates image processing with cutting-edge deep learning models. The proposed method is designed for automation, seamlessly processing a sequence of images while minimizing human intervention. The pipeline standardized red-green-blue (RGB) images captured from a color light box and used deep learning models to segment and detect seed features. Our method extracted up to 86 distinct seed characteristics, ranging from basic size metrics to intricate texture details and color nuances. Compared to traditional methods, our pipeline demonstrated a 95 percent similarity in seed quality assessment and increased time efficiency (from 2 weeks to 30 minutes for processing time). Specifically, we observed an improvement in the accuracy of seed trait identification by simply using an RGB value instead of a categorical, non-standard description, which allowed for an increase in the range of detectable seed quality characteristics. By integrating conventional image processing techniques with foundational deep learning models, this approach emerges as a pivotal instrument in pulse breeding programs, guaranteeing the maintenance of superior seed quality standards.","creator":"Morales, M., Worral, H., Piche, L., Atanda, S. A., Dariva, F., Ramos, C., Hoang, K., Yan, C., Flores, P., Bandillo, N."},{"id":"2024.03.04.583290v1","slug":"disentangling-plant-response-to-biotic-and-abiotic-stress-using-hive-a-novel-tool-to-perform-unpaired-multi-omics-integration","title":"Disentangling plant response to biotic and abiotic stress using HIVE, a novel tool to perform unpaired multi-omics integration","link":"http://biorxiv.org/cgi/content/short/2024.03.04.583290v1?rss=1","abstract":"Plants live in a constantly changing environment that is often unfavorable or even hostile. Indeed, they developed high phenotypic plasticity that includes rapid responses to aggressive biotic and abiotic factors and adaptations to changing environments. Multiple stresses can occur at the same time, requiring the plants to activate appropriate signaling pathways to respond to both or by prioritising the response to one stress factor. While several studies have been conducted to individual stress factors, only very few studies focus on the simultaneous plant response to multiple stressors. Currently used methods to integrate unpaired experiments consist of performing meta-analysis or finding differentially expressed genes for each condition separately and then selecting the common ones. Although these approaches allowed to find valuable results, they cannot identify non-specific conserved mechanisms that may hold promise for a broader understanding of plant defence response mechanisms. For this purpose, we developed HIVE (Horizontal Integration analysis using Variational AutoEncoders) to analyse horizontally integrated multi-omics datasets composed of unpaired and/or longitudinal experiments. Briefly, we coupled a variational autoencoder, that captures non-linear relationships and encoded them in the latent space, with a random forest regression and the SHAP explainer to select relevant genes for the studied phenotype. We illustrate the functionality of HIVE to study the transcriptional changes of two peanut wild relatives plants during root-knot nematode Meloidogyne arenaria infection and/or drought stress from seven unpaired experiments. HIVE performed better than the meta-analysis and the state-of-the-art tool and identified novel promising candidates responsible for triggering effective defense responses to biotic and abiotic stress.","creator":"Calia, G., Zotta Mota, A. P., Seynabou, M., Nguyen, H. T., Ghat, A., Schuler, H., Gwizdek, C., Brasileiro, A., Guimares, P., BOTTINI, S."},{"id":"2024.03.04.582885v1","slug":"identification-of-rack1a-as-a-component-of-the-auxin-ethylene-crosstalk-regulating-apical-hook-development-in-arabidopsis-thaliana","title":"Identification of RACK1A as a component of the auxin-ethylene crosstalk regulating apical hook development in Arabidopsis thaliana","link":"http://biorxiv.org/cgi/content/short/2024.03.04.582885v1?rss=1","abstract":"Apical hook development is an ideal model for studying differential growth in plants, and is controlled by complex hormonal crosstalk, with auxin and ethylene being the major players. Here, we identified a bioactive small molecule that decelerates apical hook opening in Arabidopsis thaliana. Our genetic studies suggest that this molecule enhances or maintains the auxin maximum found in the inner hook side and requires certain auxin and ethylene signaling components to modulate apical hook opening. Using biochemical approaches, we then revealed the WD40 repeat scaffold protein RECEPTOR FOR ACTIVATED C KINASE 1A (RACK1A) as a direct target of this compound. We present data in support of RACK1A playing a positive role in apical hook opening by negatively regulating the differential auxin response gradient across the hook via specific auxin and ethylene signaling mechanisms and thereby adjusting differential cell growth, an essential process for organ structure and function in plants. We have thus identified a role for RACK1A and auxin-ethylene crosstalk in negatively regulating differential cell growth to promote apical hook opening.  Significance StatementDifferential growth, or the growth of cells at different rates across tissues, is essential for providing shape and structure during plant development. The apical hook is a transient structure formed by differential cell growth across the hypocotyl tip in dark-grown seedlings, which protects the underlying tissues, and which opens during seedling development. We identified a small molecule that decelerates hook opening and discovered that it targets the protein RECEPTOR FOR ACTIVATED C KINASE 1A (RACK1A). We then showed that RACK1A promotes apical hook opening at the level of crosstalk between the plant hormones auxin and ethylene, by adjusting differential cell growth. Our work paves the way to a better understanding of how plants regulate and adapt their growth during development.","creator":"Ma, Q., Liu, S., Raggi, S., Doyle, S. M., Parizkova, B., Barange, D. K., Wilkinson, E. G., Crespo Garcia, I., Bygdell, J., Wingsle, G., Boer, D. R., Strader, L. C., Almqvist, F., Novak, O., Robert, S."},{"id":"2024.03.04.583282v1","slug":"regulation-of-arabidopsis-polyamine-acetylation-by-nata1-and-nata2","title":"Regulation of Arabidopsis polyamine acetylation by NATA1 and NATA2","link":"http://biorxiv.org/cgi/content/short/2024.03.04.583282v1?rss=1","abstract":"Polyamines have vital functions in organisms, including bacteria, plants, and animals, with key roles in growth, development, and stress responses. Spermine/spermidine N1-acetyl transferases (SSATs) regulate polyamine abundance by catalysing their N-acetylation, thereby reducing the pool of polyamines and producing other bioactive components. The regulatory mechanisms controlling SSAT enzymes are incompletely understood. Here, we investigate the biological role and regulation of the two SSAT isoforms present in Arabidopsis thaliana, N-ACETYLTRANSFERASE ACTIVITY (NATA) 1 and 2. We show that NATA2 is a heat-stable isoform, induced in response to heat. Intriguingly, a nata2 knockout mutation proved beneficial for growth and pathogen defence under heat stress in Arabidopsis, aligning with the stress-mitigating effect of polyamines. In contrast, the double knockout of nata1 and nata2 was lethal, highlighting the essential role of basal SSAT activity. Our numerous crystal structures of both NATAs, supported by functional assays, revealed that stress-produced acidic metabolites can selectively inhibit polyamine acetylation by occupying the NATA substrate-binding pocket. This environment-responsive regulation mechanism may allow Arabidopsis to adjust the deleterious action of NATAs under stress conditions, without eliminating the enzyme. More generally, metabolite-ensemble inhibition may be a novel paradigm for non-genetic feedback regulation of plant enzymes.","creator":"Hameed, U. F. S., Luo, Y.-R., Yan, J., Guzman-Vega, F. J., Aleksenko, E., Briozzo, P., MORERA, S., Jander, G., Arold, S. T."},{"id":"2024.03.04.583436v1","slug":"small-but-mitey-investigating-the-molecular-genetic-basis-for-mite-domatia-development-and-intraspecific-variation-in-vitis-riparia-using-transcriptomics","title":"Small, but mitey: Investigating the molecular genetic basis for mite domatia development and intraspecific variation in Vitis riparia using transcriptomics","link":"http://biorxiv.org/cgi/content/short/2024.03.04.583436v1?rss=1","abstract":"* Here, we investigated the molecular genetic basis of mite domatia, structures on the underside of leaves that house mutualistic mites, and intraspecific variation in domatia size in Vitis riparia (riverbank grape).  * Domatia and leaf traits were measured, and the transcriptomes of mite domatia from two genotypes of V. riparia with distinct domatia sizes were sequenced to investigate the molecular genetic pathways that regulate domatia development and intraspecific variation in domatia traits.  * Key trichome regulators as well as auxin and jasmonic acid are involved in domatia development. Genes involved in cell wall biosynthesis, biotic interactions, and molecule transport/metabolism are upregulated in domatia, consistent with their role in domatia development and function.  * This work is one of the first to date that provides insight into the molecular genetic bases of mite domatia. We identified key genetic pathways involved in domatia development and function, and uncovered unexpected pathways that provide an avenue for future investigation. We also found that intraspecific variation in domatia size in V. riparia seems to be driven by differences in overall leaf development between genotypes.","creator":"Ritter, E. J., Graham, C. D. K., Niederhuth, C., Weber, M. G."},{"id":"2024.03.04.583427v1","slug":"identification-of-new-resistance-qtl-regions-in-loquat-cultivar-champagne-against-pseudomonas-syringae-pv-eriobotryae-group-c","title":"Identification of new resistance QTL regions in loquat cultivar Champagne against  Pseudomonas syringae  pv.  eriobotryae  group C","link":"http://biorxiv.org/cgi/content/short/2024.03.04.583427v1?rss=1","abstract":"Loquat canker, caused by Pseudomonas syringae pv. eriobotryae, is a bacterial disease that infects loquat (Eriobotrya japonica) and has been reported in several countries. Three pathotypes, A, B, and C, have been reported in Japan. The loquat cultivar  Champagne is resistant to the loquat canker group C and possesses a qualitative trait governed by a recessive homozygous pse-c gene located on Linkage Group 3 (LG3), and quantitative traits located at unidentified loci. In this study, we identified novel quantitative trait loci (QTL) regions for resistance to group C in this cultivar. A seedling population with  Tanaka (Pse-c/Pse-c) crossed with  Champagne (pse-c/pse-c) was tested. The genetic map of  Champagne includes a total of 1,016 SNP markers mapped across 17 LGs, covering a total distance of 1,301 cM and an average marker density of 1.4 cM/locus. In addition to minor potential QTLs, the major QTL for resistance to loquat canker group C was detected in the upper region of LG14, with the QTL contributing 6.9% to the disease index. The results of this study open new possibilities for resistance breeding against this disease.  Highlights.A total of 1,016 SNP markers were mapped on a linkage map consisting of 17 linkage groups with a total distance of 1,301 cM.  QTL analysis revealed a novel resistance QTL region in  Champagne against loquat canker group C in the upper part of the LG14.  The identified QTLs in this study provide new possibilities for resistance breeding in loquat.","creator":"Koga, S., Kawaguchi, R., Tanaka, T., Moriya, S., Hiehata, N., Kabashima, K., Nagano, A. J., Nagano, Y., Fukuda, S."},{"id":"2024.03.05.583495v1","slug":"biological-control-of-green-mold-in-simulated-post-harvest-chain-of-citrus-fruit-efficacy-of-candida-oleophila-strain-o-and-molecular-insight-into-elicitation-of-host-immune-system","title":"Biological Control of Green Mold in Simulated Post-harvest Chain of Citrus Fruit: Efficacy of Candida oleophila Strain O and Molecular Insight into Elicitation of Host Immune System","link":"http://biorxiv.org/cgi/content/short/2024.03.05.583495v1?rss=1","abstract":"Managing post-harvest decays in citrus fruit without relying on conventional pesticides presents a significant challenge in modern Plant Pathology. This study aimed to evaluate the efficacy of the biological control agent Candida oleophila strain O in controlling green mold caused by Penicillium digitatum throughout various stages of the post-harvest supply chain. Using a series of in vivo experiments, different scenarios of P. digitatum infections in clementine tangerine, orange, and lemon fruit were examined, with treatments applied before, during or after infection. The study simulated typical conditions of the citrus supply chain, including picking, processing in packinghouses, and transportation, as well as cold storage and shelf-life phases. Results indicated that C. oleophila exhibited significant efficacy in reducing green mold symptoms, even at shelf-life temperatures, making it a practical alternative to conventional fungicides. Additionally, the study provided insights into the molecular mechanisms underlying the defensive response of citrus fruit to C. oleophila treatment, with up-regulation of defense-related genes observed across different fruit types. Overall, this study underscores the potential of C. oleophila as a sustainable and effective solution for managing post-harvest decays in citrus fruit within the complexities of the supply chain.","creator":"Rovetto, E. I., La Spada, F., El Boumlasy, S., Conti Taguali, S., Riolo, M., Pane, A., Cacciola, S. O."},{"id":"2024.03.03.583219v1","slug":"viral-delivery-of-recombinases-to-activate-heritable-genetic-switches-in-plants","title":"Viral delivery of recombinases to activate heritable genetic switches in plants","link":"http://biorxiv.org/cgi/content/short/2024.03.03.583219v1?rss=1","abstract":"Viral vectors provide an increasingly versatile platform for transformation-free reagent delivery to plants. RNA viral vectors can be used to induce gene silencing, overexpress proteins, or introduce gene editing reagents, but they are often constrained by carrying capacity or restricted tropism in germline cells. Site-specific recombinases that catalyze precise genetic rearrangements are powerful tools for genome engineering that vary in size and, potentially, efficacy in plants. In this work, we show that viral vectors based on Tobacco rattle virus (TRV) deliver and stably express four recombinases ranging in size from [~]0.6kb to [~]1.5kb, and achieve simultaneous marker removal and reporter activation through targeted excision in transgenic Nicotiana benthamiana target lines. TRV vectors with Cre, FLP, CinH, and Integrase13 efficiently mediated recombination in infected somatic tissue, and also led to heritable modifications at high frequency. An excision-activated Ruby reporter enabled simple and high-resolution tracing of infected cell lineages, without the need for molecular genotyping. Together, our experiments broaden the scope of viral recombinase delivery, and offer insights into infection dynamics that may be useful in the development of future viral vectors.","creator":"Chamness, J. C., Cody, J. P., Cruz, A. J., Voytas, D."},{"id":"2024.03.05.583474v1","slug":"spray-induced-gene-silencing-sigs-as-a-tool-for-the-management-of-pine-pitch-canker-forest-disease","title":"Spray-induced gene silencing (SIGS) as a tool for the management of Pine Pitch Canker forest disease","link":"http://biorxiv.org/cgi/content/short/2024.03.05.583474v1?rss=1","abstract":"Global change is exacerbating the prevalence of plant diseases caused by pathogenic fungi in forests worldwide. The conventional use of chemical fungicides, which is commonplace in agricultural settings, is not sanctioned for application in forest ecosystems, so novel control strategies are imperative. The promising approach SIGS (Spray-Induced Gene Silencing) involves the external application of specific double-stranded RNA (dsRNA), which can modulate the expression of target genes through environmental RNA interference in eukaryotes. SIGS exhibited notable success in reducing virulence when deployed against some crop fungal pathogens, such as Fusarium graminearum, Botrytis cinerea and Sclerotinia sclerotiorum, among others. However, there is a conspicuous dearth of studies evaluating the applicability of SIGS for managing forest pathogens. This research aimed to determine whether SIGS could be used to control Fusarium circinatum, a widely impactful forest pathogen that causes Pine Pitch Canker disease. To achieve this, we designed and produced though a bacterial synthesis, dsRNA molecules to target fungal essential genes involved to vesicle trafficking (Vps51, DCTN1, and SAC1), signal transduction (Pp2a, Sit4, Ppg1, and Tap42), and cell wall biogenesis (Chs1, Chs2, Chs3b, Gls1) metabolic pathways. We confirmed that F. circinatum is able to uptake externally applied dsRNA, triggering an inhibition of the pathogens virulence. Furthermore, this study pioneers the demonstration that recurrent applications of dsRNAs in SIGS are more effective in protecting plants than single applications. Therefore, SIGS emerges as an effective and sustainable approach for managing plant pathogens, showcasing its efficacy in controlling a globally significant forest pathogen subject to quarantine measures.","creator":"Bocos Asenjo, I. T., Amin, H., Mosquera, S., Diez Hermano, S., Ginesy, M., Diez, J. J., Nino Sanchez, J."},{"id":"2024.03.03.583192v1","slug":"overexpression-of-ppgl2-from-prunus-persica-enhanced-soybean-drought-tolerance","title":"Overexpression of PpGL2 from Prunus persica enhanced soybean drought tolerance","link":"http://biorxiv.org/cgi/content/short/2024.03.03.583192v1?rss=1","abstract":"The HD-ZIP transcription factor family plays crucial roles in plant growth and abiotic stress responses. While its diverse functions and regulatory mechanisms are well-documented, its role in conferring abiotic stress tolerance in peaches remains largely unexplored. Here, we report the bioinformatics profile of PpGL2, a member of the HD-ZIP transcription factor family, and its integration into the soybean genome to assess its potential impact on drought tolerance. Localization studies in onion cells revealed nuclear localization of PpGL2-GFP fusion protein, while yeast hybridization experiments demonstrated its transactivation and DNA binding abilities. PpGL2 overexpression under drought conditions led to reduced accumulation of reactive oxygen species and malondialdehyde compared to wild-type, decreased water loss rate, and increased chlorophyll content and relative water content. Additionally, PpGL2 overexpression promoted plant height and root length under drought stress, accompanied by altered transcription levels of stress-related genes across different plant genotypes. Furthermore, PpGL2 overexpression enhanced oxidative tolerance. Therefore, our findings suggest that PpGL2 overexpression holds promise for enhancing soybean drought resistance, offering a novel approach to improving soybean drought resistance.","creator":"Li, W., Li, D., Zhao, L., Li, H."},{"id":"2024.03.03.582478v1","slug":"unveiling-the-intricate-sinking-behavior-of-large-diatoms-insights-from-time-frequency-analysis-of-palmerina-hardmaniana-sinking-under-silicate-depleted-conditions","title":"Unveiling the Intricate Sinking behavior of Large Diatoms: Insights from Time-Frequency Analysis of Palmerina hardmaniana Sinking under Silicate-Depleted Conditions","link":"http://biorxiv.org/cgi/content/short/2024.03.03.582478v1?rss=1","abstract":"Nutrient limits impact diatom sinking in time domain, but response in time-frequency domain is unclear. Studying the response of large diatoms to nutrients exclusively in the time domain fails to fully capture the complete impact of nutrient limitations on sinking behavior due to the absence of crucial information, including period and frequency. Wavelet analysis provides valuable insights into the period and frequency of signals and unveils their positions in the time. This study investigated the sinking behavior response of the large diatom Palmerina hardmaniana to silicate-depleted conditions in the time-frequency domain using wavelet analysis. The results showed that P. hardmaniana was capable of regulating its sinking speed, and this regulation occurred intermittently in time. The predominant frequency of intermittent regulation fell within the range of 0.13-0.50 Hz (equivalent to a period of 2-8 s) for both control and silicate-depleted conditions. The similarity in the frequency range of regulation between the two groups suggests the involvement of shared physiological mechanisms. P. hardmaniana responded to silicate depletion by intensifying the regulation of 0.13-0.50 Hz, which was reflected in the time domain as a change in the proportion of different instantaneous sinking speeds and consequently lead to a significant increase or decrease in the mean sinking speed (p<0.05). Additionally, the regulation of P. hardmaniana sinking behavior was also influenced by the physiological state of the cells. Short-term silicate stress (30 min) enhanced the oscillation power of sinking regulation, while prolonged silicate stress ([&ge;] 3 d) led to a decline in oscillation power.","creator":"Ping, Z., Lu, J., Li, L., Zhuang, J., Lai, J., Shi, T., Li, J."},{"id":"2024.03.04.583410v1","slug":"variations-of-floral-temperature-in-changing-weather-conditions","title":"Variations of floral temperature in changing weather conditions","link":"http://biorxiv.org/cgi/content/short/2024.03.04.583410v1?rss=1","abstract":"O_LIFloral temperature is a flower characteristic that has the potential to impact the fitness of flowering plants and their pollinators. Likewise, the presence of floral temperature patterns, areas of contrasting temperature across the flower, can have similar impacts on the fitness of both mutualists. C_LIO_LIIt is currently poorly understood how floral temperature changes under the influence of different weather conditions, and how floral traits may moderate these changes. Such weather dependency will impact how stable floral temperatures are over time and their utility to plant and pollinator. The stability of floral temperature cues is likely to facilitate effective plant-pollinator interactions and play a role in the plants reproductive success. C_LIO_LIWe use thermal imaging to monitor how floral temperatures and temperature patterns of four plant species (Cistus  snow fire and  snow white, Coreopsis verticillata and Geranium psilostemon) change with several weather variables (illumination, temperature; windspeed; cloud cover; humidity and pressure) during times that pollinators are active. C_LIO_LIAll weather variables influenced floral temperature in one or more species. The directionality of these relationships were similar across species. In all species light conditions (illumination) had the greatest influence on floral temperature overall, and in generation of contrasting temperatures between parts of the flower, temperature patterns. The effect sizes of other weather variables were lower and more varied across the four species. Most likely, floral traits such as pigmentation and structure influence these relationships between weather conditions and generation of floral temperature. C_LIO_LISynthesis: Floral temperature and the extent to which flowers showed contrasting temperature patterns were influenced predominantly by light conditions. However, several weather variables had additional, lesser, influences. Furthermore, differences in floral traits, pigmentation and structure, likely resulted in differences in temperature responses to given conditions between species and different parts of the same flower. However, floral temperatures and contrasting temperature patterns that are sufficiently elevated for detection by pollinators were maintained across most conditions if flowers received moderate illumination. This suggests the presence of elevated floral temperature and contrasting temperature patterns are fairly constant and may have potential to influence plant-pollinator interactions across weather conditions. C_LI","creator":"Harrap, M. J. M., de Vere, N., Hempel de Ibarra, N., Whitney, H. M., Rands, S. A."},{"id":"2024.03.04.583368v1","slug":"periplasmic-carbonic-anhydrase-cah1-contributes-to-high-inorganic-carbon-affinity-in-chlamydomonas-reinhardtii","title":"Periplasmic carbonic anhydrase CAH1 contributes to high inorganic carbon affinity in Chlamydomonas reinhardtii","link":"http://biorxiv.org/cgi/content/short/2024.03.04.583368v1?rss=1","abstract":"Carbonic anhydrase (CA), an enzyme conserved across species, is pivotal in the interconversion of inorganic carbon (Ci; CO2 and HCO3-). Compared to the well-studied intracellular CA, the specific role of extracellular CA in photosynthetic organisms is still not well understood. In the green alga Chlamydomonas reinhardtii, CAH1, located at the periplasmic space, is strongly induced under CO2-limiting conditions by the Myb transcription factor LCR1. While it has been observed that the lcr1 mutant shows decreased Ci-affinity, the detailed mechanisms behind this phenomenon are yet to be elucidated. In this study, we aimed to unravel the LCR1-dependent genes essential for maintaining high Ci-affinity. To achieve this, we identified a total of 12 LCR1-dependent inducible genes under CO2-limiting conditions, focusing specifically on the most prominent ones - CAH1, LCI1, LCI6, and Cre10.g426800. We then created mutants of these genes using the CRISPR-Cas9 system, all from the same parental strain, and compared their Ci-affinity. Contrary to earlier findings (Van and Spalding, 1999) that reported no reduction in Ci-affinity in the cah1 mutant, our newly created cah1-1 mutant exhibited a significant decrease in Ci-affinity under high HCO3-/CO2-ratio conditions. Additionally, when we treated wild-type cells with a CA inhibitor with low membrane permeability, a similar reduction in Ci-affinity was observed. Moreover, the addition of exogenous CA to the cah1 mutant restored the decreased Ci-affinity. These results, highlighting the crucial function of the periplasmic CAH1 in maintaining high Ci-affinity in Chlamydomonas cells, provide new insights into the functions of periplasmic CA in algal carbon assimilation.  One-sentence summaryCAH1, a periplasmic carbonic anhydrase in Chlamydomonas reinhardtii, plays a crucial role in maintaining a high affinity for inorganic carbon, particularly under CO2-limiting conditions.","creator":"Shimamura, D., Ikeuchi, T., Tsuji, Y., Fukuzawa, H., Yamano, T."},{"id":"2024.03.01.583049v1","slug":"lhp1-and-ino80-cooperate-with-ethylene-signaling-for-warm-ambient-temperature-response-by-activating-specific-bivalent-genes","title":"LHP1 and INO80 cooperate with ethylene signaling for warm ambient temperature response by activating specific bivalent genes","link":"http://biorxiv.org/cgi/content/short/2024.03.01.583049v1?rss=1","abstract":"Ethylene signaling has been indicated as a potential positive regulator of plant warm ambient temperature response but its underlying molecular mechanisms are largely unknown. Here, we show that LHP1 and INO80 cooperate with ethylene signaling for warm ambient temperature response by activating specific bivalent genes. We found that the presence of warm ambient temperature activates ethylene signaling through EIN2 and EIN3, leading to an interaction between LHP1 and accumulated EIN2-C to co-regulate a subset of LHP1-bound genes marked by H3K27me3 and H3K4me3 bivalency. Furthermore, we demonstrate that INO80 is recruited to bivalent genes by interacting with EIN2-C and EIN3, promoting H3K4me3 enrichment and facilitating transcriptional activation in response to warm ambient temperature. Together, our findings illustrate a novel mechanism wherein ethylene signaling orchestrates LHP1 and INO80 to regulate warm ambient temperature response through activating specific bivalent genes in Arabidopsis.","creator":"Shao, Z., Bai, Y., Huq, E., Qiao, H."},{"id":"2024.02.29.582853v1","slug":"impact-of-alternative-splicing-on-arabidopsis-proteome","title":"Impact of alternative splicing on Arabidopsis proteome","link":"http://biorxiv.org/cgi/content/short/2024.02.29.582853v1?rss=1","abstract":"Alternative splicing is an important regulatory process in eukaryotes. In plants, the major form of alternative splicing is intron retention. Despite its importance, the global impact of AS on the Arabidopsis proteome has not been investigated. In this study, we address this gap by performing a comprehensive integrated analysis of how changes in AS can affect the Arabidopsis proteome using mutants that disrupt ACINUS and PININ, two evolutionarily conserved alternative splicing factors. We used tandem mass tagging (TMT) with real-time search MS3 (RTS-SPS-MS3) coupled with extensive sample fractionations to achieve very high coverage and accurate protein quantification. We then integrated our proteomic data with transcriptomic data to assess how transcript changes and increased intron retention (IIR) affect the proteome. For differentially expressed transcripts, we have observed a weak to moderate correlation between transcript changes and protein changes. Our studies revealed that some IIRs have no effect on either transcript or protein levels, while some IIRs can significantly affect protein levels. Surprisingly, we found that IIRs have a much smaller effect on increasing protein diversity. Notably, the increased intron retention events detected in the double mutant are also detected in the WT under various biotic or abiotic stresses. We further investigated the characteristics of the retained introns. Our extensive proteomic data help to guide the phenotypic analysis and reveal that collective protein changes contribute to the observed phenotypes of the increased anthocyanin, pale green, reduced growth, and short root observed in the acinus pnn double mutant. Overall, our study provides insight into the intricate regulatory mechanism of intron retention and its impact on protein abundance in plants.","creator":"Reyes, A. V., Shrestha, R., Grismer, T. S., Byun, D., Xu, S."},{"id":"2024.02.29.582862v1","slug":"atg6-interacting-with-npr1-increases-arabidopsis-thaliana-resistance-to-pst-dc3000-avrrps4-by-increasing-its-nuclear-accumulation-and-stability","title":"ATG6 interacting with NPR1 increases Arabidopsis thaliana resistance to Pst DC3000/avrRps4 by increasing its nuclear accumulation and stability","link":"http://biorxiv.org/cgi/content/short/2024.02.29.582862v1?rss=1","abstract":"Autophagy-related gene 6 (ATG6) plays a crucial role in plant immunity. Nonexpressor of pathogenesis-related genes1 (NPR1) acts as a signaling hub of plant immunity. However, the relationship between ATG6 and NPR1 is unclear. Here, we find that ATG6 directly interacts with NPR1. ATG6 overexpression significantly increased nuclear accumulation of NPR1. Furthermore, we demonstrate that ATG6 increases NPR1 protein levels and improves its stability. Interestingly, ATG6 promotes the formation of SINCs (SA-induced NPR1 condensates)-like condensates. Additionally, ATG6 and NPR1 synergistically promote the expression of pathogenesis-related genes. Further results showed that silencing ATG6 in NPR1-GFP exacerbates Pst DC3000/avrRps4 invasion, while double overexpression of ATG6 and NPR1 synergistically inhibits Pst DC3000/avrRps4 invasion. In summary, our findings unveil an interplay of NPR1 with ATG6 and elucidate important molecular mechanisms for enhancing plant immunity.  HighlightWe unveil a novel relationship in which ATG6 positively regulates NPR1 in plant immunity.","creator":"Zhang, B., Huang, S., Guo, S., Meng, Y., Tian, Y., Zhou, Y., Chen, H., Li, X., Zhou, J., Chen, W."},{"id":"2024.03.01.582956v1","slug":"ptac3-and-ptac14-are-required-for-binding-of-plastid-encoded-rna-polymerase-to-dna","title":"pTAC3 and pTAC14 are required for binding of plastid-encoded RNA polymerase to DNA","link":"http://biorxiv.org/cgi/content/short/2024.03.01.582956v1?rss=1","abstract":"Plastid-encoded RNA polymerase (PEP) is a bacterial-type multisubunit RNA polymerase responsible for the majority of transcription in chloroplasts. PEP consists of four core subunits, which are orthologs of their cyanobacterial counterparts. In Arabidopsis thaliana, PEP associates with 12 PEP-associated proteins (PAPs), which serve as peripheral subunits of the RNA polymerase. The exact contributions of PAPs to PEP function are still poorly understood. We use ptChIP-seq to show that PAP1/pTAC3, a peripheral subunit of PEP, binds to the same genomic loci as RpoB, a core subunit of PEP. The pap1/ptac3 mutant shows a complete loss of RpoB binding to DNA throughout the genome, indicating that PAP1/pTAC3 is necessary for RpoB binding to DNA. A similar loss of RpoB binding to DNA is observed in the pap7/ptac14 mutant, which is defective in another peripheral PEP subunit. We propose that the peripheral subunits of PEP are required for the recruitment of core PEP subunits to DNA.  KEY MESSAGEThe peripheral subunits of plastid-encoded RNA polymerase play a crucial role in recruiting the core PEP subunits to DNA in Arabidopsis chloroplasts.","creator":"Wang, J., Palomar, M., Min, J.-H., Wierzbicki, A."},{"id":"2024.02.29.582824v1","slug":"redox-regulation-by-the-cdsp32-thioredoxin-of-atp-synthase-activity-and-enzymatic-antioxidant-network-in-solanum-tuberosum","title":"Redox regulation by the CDSP32 thioredoxin of ATP-synthase activity and enzymatic antioxidant network in Solanum tuberosum","link":"http://biorxiv.org/cgi/content/short/2024.02.29.582824v1?rss=1","abstract":"Plant thioredoxins (TRXs) form a complex family involved in numerous metabolic and signalling pathways, such as the regulation of photosynthetic metabolism in relation with light conditions. The atypical CDSP32, chloroplastic drought-induced stress protein of 32 kDa, TRX includes two TRX-fold domains, one of which has an atypical redox-active HCGPC motif, and has been initially reported to participate in responses to oxidative stress as an electron donor to peroxiredoxins and methionine sulfoxide reductases. Here, we further characterized potato lines modified for CDSP32 expression to clarify the physiological roles of the TRX. Upon high salt treatments, modified lines displayed changes in the abundance and redox status of CDSP32 antioxidant partners, and exhibited sensitivity to NaHCO3, but not to NaCl. In non-stressed plants overexpressing CDSP32, a lower abundance of photosystem II PsbO and D1 subunits and ATP-synthase {gamma} subunit was noticed. The CDSP32 co-suppressed line showed altered chlorophyll a fluorescence induction and modified regulation of the plastidial ATP-synthase activity during dark/light and light/dark transitions, revealing the involvement of CDSP32 in the control of the photosynthetic machinery. In agreement with the previously reported interaction in planta between CDSP32 and the ATP-synthase {gamma} subunit, our data show that CDSP32 participates in the regulation of the transthylakoid membrane potential. Consistently, modeling of protein complex 3-D structure indicates that the CDSP32 TRX constitutes a suitable partner of ATP-synthase {gamma} subunit. We discuss the roles of CDSP32 in chloroplast redox homeostasis through the regulation of both photosynthetic activity and enzymatic antioxidant network.","creator":"Rey, P., Henri, P., Alric, J., Blanchard, L., Viola, S."},{"id":"2024.03.03.583161v1","slug":"two-pyridoxal-phosphate-homeostasis-proteins-are-essential-for-management-of-the-coenzyme-in-plants","title":"Two PYRIDOXAL PHOSPHATE HOMEOSTASIS PROTEINs are essential for management of the coenzyme in plants","link":"http://biorxiv.org/cgi/content/short/2024.03.03.583161v1?rss=1","abstract":"Coenzyme management is believed to be important for the required pool of active enzymes driving metabolic routes to facilitate homeostasis and match environmental circumstance. The coenzyme pyridoxal 5-phosphate (PLP) (a vitamin B6 derivative) is involved in a diverse array of enzyme reactions spanning amino acid to hormone metabolism. However, dedicated proteins that contribute to PLP homeostasis have not yet been studied in plants. Here we demonstrate the importance of proteins annotated PLP HOMEOSTASIS PROTEINs (PLPHPs) for control of PLP in Arabidopsis. A systematic analysis indicates that while most kingdoms have a single PLPHP homolog, Angiosperms within the plant kingdom have two. PLPHPs from Arabidopsis bind PLP and exist as monomers in solution in contrast to reported PLP-dependent enzymes from all kingdoms. Disrupting functionality of both homologs perturbs vitamin B6 content including a PLP deficit accompanied by impaired and light hypersensitive root growth, unlike biosynthesis mutants. Micrografting studies show that the PLP deficit can be relieved distally between shoots and roots. Yet, supplementation experiments do not restore vitamin B6 homeostasis in the absence of PLPHP. A series of chemical treatments probing PLP-dependent reactions, notably those for auxin and ethylene, provide evidence that the physiological role of PLPHPs is dynamic management of PLP. Assays in vitro show that Arabidopsis PLPHP can coordinate both PLP transfer and withdrawal. This study expands our broader knowledge of vitamin B6 biology and highlights the importance of PLP coenzyme homeostasis in plants, providing a platform for further investigations in boosting adaptive responses.  One sentence summaryPLPHPs contribute to surveillance of vitamin B6 homeostasis, likely acting as a rheostat in adaptive responses as a function of the use of the coenzyme PLP.","creator":"Farkas, P., Fitzpatrick, T. B."},{"id":"2024.02.29.582789v1","slug":"the-early-dodder-gets-the-host-decoding-the-coiling-patterns-of-cuscuta-campestris-with-automated-image-processing","title":"The Early Dodder Gets the Host: Decoding the Coiling Patterns of Cuscuta campestris with Automated Image Processing","link":"http://biorxiv.org/cgi/content/short/2024.02.29.582789v1?rss=1","abstract":"Cuscuta spp., (dodder) is a rootless and leafless parasitic plant posing significant agricultural challenges. In this study, we aimed to elucidate the dynamics of the coiling patterns in Cuscuta campestris and examine the role of circadian rhythms in its host-seeking ability. Using time-lapse photography, we recorded the circumnutation and coiling movements of C. campestris at different inoculation times on non-living hosts. Subsequent image analyses were facilitated through an in-house Python-based image analysis pipeline. We observed that the coiling efficacy of C. campestris varied with the inoculation time of day, showing higher success and faster initiation in morning than in evening. These observations suggest that Cuscuta, despite lacking leaves and a developed chloroplast, can discern photoperiod changes, which significantly determine its parasitic efficiency. The automated image analysis confirmed the reliability of our Python pipeline, aligning closely with manual annotations. This study provides significant insights into the parasitic strategies of C. campestris and demonstrates the potential of integrating computational image analysis in plant biology for exploring complex plant behaviors. Furthermore, this method provides an efficient tool for investigating plant movement dynamics, laying the foundation for future studies on mitigating the economic impacts of parasitic plants.","creator":"Bentelspacher, M., Amezquita, E. J., Adhikari, S., Barros-Rios, J., Park, S."},{"id":"2024.03.03.582138v1","slug":"systematics-of-the-fleshy-fruited-sonerileae-melastomataceae","title":"Systematics of the fleshy-fruited Sonerileae (Melastomataceae)","link":"http://biorxiv.org/cgi/content/short/2024.03.03.582138v1?rss=1","abstract":"With approximately 1080 species, Sonerileae is the second largest tribe in the Melastomataceae. Approximately 40% of Sonerileae species belong to fleshy-fruited genera (Catanthera, Heteroblemma, Kendrickia, Medinilla, Pachycentria, and Plethiandra). Relatively few species, especially of the fleshy-fruited taxa, have been sampled for phylogenetic study. Consequently, there is huge uncertainty resulting in many unanswered questions about their evolutionary history, including the monophyly of the largest genus, Medinilla. In this study, the phylogeny of the fleshy-fruited Sonerileae was reconstructed using 385 nuclear and 81 plastid protein-coding loci recovered from target capture. Our study revealed that the fleshy fruited Sonerileae are polyphyletic and belong to three lineages. Kendrickia is sister to an Afrotropical endemic clade. Heteroblemma and Catanthera belong to a second clade and are most closely related to some Phyllagathis and Driessenia species. Medinilla forms a third clade, and includes Pachycentria and Plethiandra. Within Medinilla, fifteen clades are identified and characterized. To make Medinilla monophyletic, the genus is redefined to include Pachycentria and Plethiandra. Major lineages identified within Medinilla lay the groundwork for an infrageneric classification system. Areas of the phylogenetic tree with high conflict or weak sampling are identified to aid further studies in the tribe.","creator":"Quakenbush, J. P., Chen, L., Penneys, D. S., Barkman, T. J., Liu, Y., Yakandawala, D., Libalah, M. C. V. E., Kadereit, G."},{"id":"2024.02.29.582735v1","slug":"cold-stress-induces-a-rapid-redistribution-of-the-antagonistic-marks-h3k4me3-and-h3k27me3-in-arabidopsis-thaliana","title":"Cold stress induces a rapid redistribution of the antagonistic marks H3K4me3 and H3K27me3 in Arabidopsis thaliana","link":"http://biorxiv.org/cgi/content/short/2024.02.29.582735v1?rss=1","abstract":"When exposed to low temperatures, plants undergo a drastic reprogramming of their transcriptome in order to adapt to their new environmental conditions, which primes them for potential freezing temperatures. While the involvement of transcription factors in this process, termed cold acclimation, has been deeply investigated, the potential contribution of chromatin regulation remains largely unclear. A large proportion of cold-inducible genes carries the repressive mark histone 3 lysine 27 trimethylation (H3K27me3), which has been hypothesized as maintaining them in a silenced state in the absence of stress, but which would need to be removed or counteracted upon stress perception. However, the fate of H3K27me3 during cold exposure has not been studied genome-wide. In this study, we offer an epigenome profiling of H3K27me3 and its antagonistic active mark H3K4me3 during short-term cold exposure. Both chromatin marks undergo rapid redistribution upon cold exposure, however, the gene sets undergoing H3K4me3 or H3K27me3 differential methylation are distinct, refuting the simplistic idea that gene activation relies on a switch from an H3K27me3 repressed chromatin to an active form enriched in H3K4me3. Coupling the ChIP-seq experiments with transcriptome profiling reveals that differential histone methylation correlates with changes in expression. Interestingly, only a subset of cold-regulated genes lose H3K27me3 during their induction, indicating that H3K27me3 is not an obstacle to transcriptional activation. In the H3K27me3 methyltransferase curly leaf (clf) mutant, many cold regulated genes display reduced H3K27me3 levels but their transcriptional activity is not altered prior or during a cold exposure, suggesting that H3K27me3 may serve a more intricate role in the cold response than simply repressing the cold-inducible genes in naive conditions.","creator":"Faivre, L. A. C., Kinscher, N. F., Kuhlmann, A. B., Xu, X., Kaufmann, K., Schubert, D."},{"id":"2024.02.29.582690v1","slug":"brassinosteroid-and-gibberellin-signaling-are-required-for-tomato-internode-elongation-in-response-to-low-red-far-red-light","title":"Brassinosteroid and gibberellin signaling are required for Tomato internode elongation in response to low red: far-red light","link":"http://biorxiv.org/cgi/content/short/2024.02.29.582690v1?rss=1","abstract":"In this study, we explore the dynamic interplay between the plant hormones gibberellins (GA), brassinosteroids (BR), and Indole-3-Acetic Acid (IAA) in their collective impact on plant shade avoidance elongation under varying light conditions. We focus particularly on low Red: Far-red (R:FR) light conditions achieved by supplementing the background light with FR. Our research delves into how these hormones individually and synergistically influence stem elongation in tomato plants. Through meticulous experimental modulations of GA, IAA, and BR, we demonstrate that GA and BR are sufficient but also necessary for inducing stem elongation under low R:FR light conditions. Intriguingly, while IAA alone shows limited effects, its combination with GA yields significant elongation, suggesting a nuanced hormonal balance. Furthermore, we unveil the complex interplay of these hormones under light with low R:FR, where the suppression of one hormones effect can be compensated by the others. This study provides insights into the hormonal mechanisms governing plant adaptation to light, highlighting the intricate and adaptable nature of plant growth responses. Our findings have far-reaching implications for agricultural practices, offering potential strategies for optimizing plant growth and productivity in various lighting environments.  HighlightThis study unveils the interplay of brassinosteroids and gibberellins in shade avoidance elongation, revealing how tomatoes acclimate in response to far-red enriched light conditions.","creator":"Li, L., Helming, T., Wonder, J., van Asselt, G., Pantazopoulou, C. K., van de Kaa, Y. T. R., Kohlen, W., Pierik, R., Kajala, K."},{"id":"2024.02.28.582521v1","slug":"the-trade-off-between-grain-weight-and-grain-number-in-wheat-is-explained-by-the-overlapping-of-the-key-phases-determining-these-major-yield-components","title":"The trade-off between grain weight and grain number in wheat is explained by the overlapping of the key phases determining these major yield components","link":"http://biorxiv.org/cgi/content/short/2024.02.28.582521v1?rss=1","abstract":"Enhancing grain yield is a primary goal in the cultivation of major staple crops, including wheat. Recent research has focused on identifying the physiological and molecular factors that influence grain weight, a critical determinant of crop yield. However, a bottleneck has arisen due to the trade-off between grain weight and grain number, whose underlying causes remain elusive. In a novel approach, a wheat expansin gene, TaExpA6, known for its expression in root tissues, was engineered to express in the grains of the spring wheat cultivar Fielder. This modification led to increases in both grain weight and yield without adversely affecting grain number. Conversely, a triple mutant line targeting the gene TaGW2, a known negative regulator of grain weight, resulted in increased grain weight but decreased grain number, potentially offsetting yield gains. This study aimed to evaluate four wheat genotypes: (i) a transgenic line expressing TaExpA6, (ii) its wild-type counterpart (Fielder), (iii) a TaGW2 triple mutant line, and (iv) its wild-type. Conducted in southern Chile, the study employed a Complete Randomized Block Design with four replications, under well-managed field conditions including fertilization, irrigation, and pest control. The primary metrics assessed were grain yield, grain number, and average grain weight per spike, along with detailed measurements of grain weight and dimensions across the spike, and ovary weight at pollination (Waddingtons scale 10). The expression levels of TaExpA6 and TaGW2 were also monitored post-anthesis. Results indicated that both the TaExpA6 line and the triple mutant line achieved significantly higher average grain weights compared to their respective wild types. Notably, the TaExpA6 line did not exhibit a reduction in grain number, thereby enhancing grain yield per spike. In contrast, the triple mutant line showed a reduced grain number per spike, with no significant change in overall yield. Analysis of ovary size, grain weight dynamics, and gene expression patterns suggests that the trade-off between grain weight and number could be attributed to the overlapping of the critical periods for the determination of these traits.","creator":"Vicentin, L., Canales, J., Calderini, D. F."},{"id":"2024.02.28.582661v1","slug":"hy5-and-cop1-function-antagonistically-in-the-light-dependent-regulation-of-nicotine-biosynthesis-in-tobacco","title":"HY5 and COP1 function antagonistically in the light-dependent regulation of nicotine biosynthesis in tobacco","link":"http://biorxiv.org/cgi/content/short/2024.02.28.582661v1?rss=1","abstract":"Nicotine constitutes approximately 90% of the total alkaloid content within the Nicotiana species, rendering it the most prevalent alkaloid. While the majority of genes responsible for nicotine biosynthesis express in root tissue, the influence of light on this process through shoot-to-root mobile ELONGATED HYPOCOTYL 5 (HY5) has been recognized. CONSTITUTIVE PHOTOMORPHOGENIC1 (COP1), a key regulator of light-associated responses, known for its role in modulating HY5 accumulation, remains largely unexplored in its relationship to light-dependent nicotine accumulation. Here, we identified NtCOP1, a COP1 homolog in Nicotiana tabacum, and demonstrated its ability to complement the cop1 mutant in Arabidopsis thaliana at molecular, morphological, and biochemical levels. Through the development of NtCOP1 overexpression (NtCOP1OX) plants, we observed a significant reduction in nicotine and flavonol content, inversely correlated with the down-regulation of nicotine and phenylpropanoid pathway. Conversely, CRISPR/Cas9-based knockout mutant plants (NtCOP1CR) exhibited an increase in nicotine levels. Further investigations, including yeast-two hybrid assays, grafting experiments, and western blot analyses, revealed that NtCOP1 modulates nicotine biosynthesis by targeting NtHY5, thereby impeding its transport from shoot-to-root. We conclude that the interplay between HY5 and COP1 functions antagonistically in the light-dependent regulation of nicotine biosynthesis in tobacco.  HighlightCharacterization of CONSTITUTIVE PHOTOMORPHOGENIC1 (COP1) overexpressing and CRISPR/Cas9-based mutant plants suggests the intricate role of COP1 in modulating nicotine biosynthesis in tobacco.","creator":"Singh, D., Dwivedi, S., Singh, N., Trivedi, P."},{"id":"2024.02.28.582481v1","slug":"a-novel-tomato-inter-specific-solanum-lycopersicum-var-cerasiforme-and-s-pimpinellifolium-magic-population-facilitates-trait-association-and-candidate-gene-discovery-in-untapped-exotic-germplasm","title":"A novel tomato inter-specific (Solanum lycopersicum var. cerasiforme and S. pimpinellifolium) MAGIC population facilitates trait association and candidate gene discovery in untapped exotic germplasm","link":"http://biorxiv.org/cgi/content/short/2024.02.28.582481v1?rss=1","abstract":"We developed a novel eight-way tomato multi-parental advanced generation inter-cross (MAGIC) population to improve the accessibility of the genetic resources of tomato relatives to geneticists and breeders. The inter-specific MAGIC population (ToMAGIC) was obtained by inter-crossing four accessions each of Solanum lycopersicum var. cerasiforme (SLC) and S. pimpinellifolium (SP), which respectively are the weedy relative and the ancestor of cultivated tomato. The eight exotic ToMAGIC founders were selected based on a representation of the genetic diversity and geographical distribution of the two taxa. The resulting MAGIC population comprises 354 lines which were genotyped using a new 12k tomato Single Primer Enrichment Technology (SPET) panel and yielded 6,488 high-quality SNPs. The genotyping data revealed a high degree of homozygosity (average 93.69%), an absence of genetic structure, and a balanced representation (11.62% to 14.16%) of the founder genomes. To evaluate the potential of the ToMAGIC population for tomato genetics and breeding, a proof-of-concept was conducted by phenotyping it for fruit size, plant pigmentation, leaf morphology, and earliness traits. Genome-wide association studies (GWAS) identified strong associations for the studied traits, pinpointing both previously identified and novel candidate genes near or within the linkage disequilibrium blocks. Domesticated alleles for fruit size were recessive and were found, at low frequencies, in wild/ancestral populations. Our findings demonstrate that the newly developed ToMAGIC population is a valuable resource for genetic research in tomato, offering significant potential for identifying new genes that govern key traits in tomato breeding. ToMAGIC lines displaying a pyramiding of traits of interest could have direct applicability for integration into breeding pipelines providing untapped variation for tomato breeding.","creator":"Arrones, A., Antar, O., Pereira-Dias, L., Solana, A., Ferrante, P., Aprea, G., Plazas, M., Prohens, J., Diez, M. J., Giuliano, G., Gramazio, P., Vilanova, S."},{"id":"2024.02.28.582440v1","slug":"super-pangenome-of-grapevines-empowers-improvement-of-the-oldest-domesticated-fruit","title":"Super Pangenome of Grapevines Empowers Improvement of the Oldest Domesticated Fruit","link":"http://biorxiv.org/cgi/content/short/2024.02.28.582440v1?rss=1","abstract":"Grapevine (Vitis) is the oldest domesticated fruit crop with great cultural and economic importance. Here, we assemble and annotate haplotype-resolved genomes of 72 Vitis accessions including 25 wild and 47 cultivated grapevines, and a haplotype-resolved complete genome of V. vinifera. Coalescent phylogenomics of 142 haplotype genomes disentangles the mysterious hybridization history of grapevines, revealing enormous genetic diversity among species. Pangenome analysis together with phenotyping data reveals that European cultivars, more susceptible to the most destructive disease downy mildew (DM), had a smaller repertoire of disease resistance genes of NLR family. Through extensive structural variation (SV) characterization, phenotyping, transcriptome profiling of 113 Vitis accessions, and SV-eQTL analysis, we have identified over 79 SVs and their relevant genes significantly associated with DM resistance, exemplified by a lysine histidine transporter, VvLHT8. This haplotype-resolved complete genome and pangenome of Vitis genus will accelerate grapevine breeding and enrich our understanding of the evolution and biology of grapevines.","creator":"Guo, L., Wang, X., Ayhan, D. H., Rhaman, M. S., Yan, M., Jiang, J., Wang, D., Zheng, W., Mei, J., Ji, W., Jiao, J., Chen, S., Sun, J., Yi, S., Meng, D., Wang, J., Bhuiyan, M. N., Qin, G., Guo, L., Yang, Q., Zhang, X., Sun, H., Liu, C., Ye, W."},{"id":"2024.02.27.582216v1","slug":"seminal-root-angle-is-associated-with-root-system-architecture-in-durum-wheat","title":"Seminal root angle is associated with root system architecture in durum wheat","link":"http://biorxiv.org/cgi/content/short/2024.02.27.582216v1?rss=1","abstract":"Optimal root system architecture (RSA) is critical for efficient resource capture in soils, hence being an interest in crop breeding. Seminal root angle (SRA) at the seedling stage in durum wheat has been suggested to be a good indicator of RSA. However, research on correlating such lab-based seedling root phenotyping to RSA at later phases of plant growth is limited, resulting in the importance of root trait variation seen in seedlings often being overstated. To explore the role of SRA in modifying RSA at later phases of plant growth, we assessed 11 genotypes contrasting in SRA (wide and narrow), grown in a rhizobox designed for phenotyping root systems of plants during late-tillering. Above-ground traits and root dry mass in different soil depths and across the entire soil volume were measured manually, while root architectural traits were extracted using image analysis and summarised by multiple factor analysis to describe RSA. When comparing the wide and narrow genotypes, no differences were detected for above-ground traits and total root dry mass. However, differences were observed in the allocation of root dry mass at different depths. The wide and narrow genotypes showed distinct RSAs, particularly in the upper soil (0 - 30 cm). The wide genotypes exhibited a  spread-out root system with dense and thin roots, whereas the narrow genotypes had a compact root system with fewer but thicker roots. Our study demonstrated a clear difference in RSA between the wide and narrow genotypes, highlighting the association between SRA on the direction and distribution of root growth in plants at later growth stages.","creator":"Kang, Y., Rambla, C., Haeften, S., Fu, B., Akinlade, O., Potgieter, A., Borrell, A., Mace, E., Jordan, D., Alahmad, S., Hickey, L."}]},{"name":"Economics","feed":[{"id":"2403.04029","slug":"two-person-adversarial-games-are-zero-sum-a-resolution-of-the-luce-raiffa-aumann-lra-conjecture","title":"Two-Person adversarial games are zero-sum: A resolution of the Luce-Raiffa-Aumann (LRA) conjecture","link":"https://arxiv.org/abs/2403.04029","abstract":"Abstract: This letter: (i) reformulates the theorems of Adler-Daskalakis-Papadimitriou (2009) and Raimondo (2023) on two-player adversarial games as a generalized result with a simplified proof, (ii) forges connections to work on strategically zero-sum games by Moulin-Vial (1978), and on axiomatizations of multi-linear utilities of n-person games by Fishburn-Roberts (1976, 1978). The simplification and the connections on offer give prominence to two-person zero-sum games studied by Aumann (1961), Shapley (1964) and Rosenthal (1974), and also to recent algorithmic work in computer science. We give a productive reorientation to the subject by bringing the two communities together under the rubric of adversarial games.","creator":"M. Ali Khan, Arthur Paul Pedersen, David Schrittesser"},{"id":"2403.04328","slug":"a-dual-approach-to-nonparametric-characterization-for-random-utility-models","title":"A dual approach to nonparametric characterization for random utility models","link":"https://arxiv.org/abs/2403.04328","abstract":"Abstract: This paper develops a novel characterization for random utility models (RUM), which turns out to be a dual representation of the characterization by Kitamura and Stoye (2018, ECMA). For a given family of budgets and its \"patch\" representation \\'a la Kitamura and Stoye, we construct a matrix $\\Xi$ of which each row vector indicates the structure of possible revealed preference relations in each subfamily of budgets. Then, it is shown that a stochastic demand system on the patches of budget lines, say $\\pi$, is consistent with a RUM, if and only if $\\Xi\\pi \\geq \\mathbb{1}$. In addition to providing a concise closed form characterization, especially when $\\pi$ is inconsistent with RUMs, the vector $\\Xi\\pi$ also contains information concerning (1) sub-families of budgets in which cyclical choices must occur with positive probabilities, and (2) the maximal possible weights on rational choice patterns in a population. The notion of Chv\\'atal rank of polytopes and the duality theorem in linear programming play key roles to obtain these results.","creator":"Nobuo Koida, Koji Shirai"},{"id":"2403.04354","slug":"a-logarithmic-mean-divisia-index-decomposition-of-co-2-emissions-from-energy-use-in-romania","title":"A Logarithmic Mean Divisia Index Decomposition of CO$_2$ Emissions from Energy Use in Romania","link":"https://arxiv.org/abs/2403.04354","abstract":"Abstract: Carbon emissions have become a specific alarming indicators and intricate challenges that lead an extended argue about climate change. The growing trend in the utilization of fossil fuels for the economic progress and simultaneously reducing the carbon quantity has turn into a substantial and global challenge. The aim of this paper is to examine the driving factors of CO$_2$ emissions from energy sector in Romania during the period 2008-2022 emissions using the log mean Divisia index (LMDI) method and takes into account five items: CO$_2$ emissions, primary energy resources, energy consumption, gross domestic product and population, the driving forces of CO$_2$ emissions, based on which it was calculated the contribution of carbon intensity, energy mixes, generating efficiency, economy, and population. The results indicate that generating efficiency effect -90968.57 is the largest inhibiting index while economic effect is the largest positive index 69084.04 having the role of increasing CO$_2$ emissions.","creator":"Mariana Carmelia Balanica-Dragomir, Gabriel Murariu, Lucian Puiu Georgescu"},{"id":"2403.04512","slug":"a-topological-characterization-of-the-existence-of-w-stable-sets","title":"A topological characterization of the existence of w-stable sets","link":"https://arxiv.org/abs/2403.04512","abstract":"Abstract: The theory of optimal choice sets is a solution theory that has a long and well-established tradition in social choice and game theories. Some of important general solution concepts of choice problems when the set of best alternatives does not exist (this problem occurs when the preferences yielded by an economic process are cyclic) is the Stable Set (Von Neumann-Morgenstern set) and its variants (Generalized Stable set, Extended Stable set, m-Stable set and w-Stable set). The theory of w-stable sets solution is more realistic because: (1) It solves the existence problem of solution; (2) It expands the notions of maximal alternative set and (3) The concept of stability is defined in such a way as to prevent a chosen alternative from being dominated by another alternative and sets this stability within the solution. In this paper, we present a topological characterization of the existence of w-Stable sets solution of arbitrary binary relations over non-finite sets of alternatives.","creator":"Athanasios Andrikopoulos, Nikolaos Sampanis"},{"id":"2403.04766","slug":"nonparametric-regression-under-cluster-sampling","title":"Nonparametric Regression under Cluster Sampling","link":"https://arxiv.org/abs/2403.04766","abstract":"Abstract: This paper develops a general asymptotic theory for nonparametric kernel regression in the presence of cluster dependence. We examine nonparametric density estimation, Nadaraya-Watson kernel regression, and local linear estimation. Our theory accommodates growing and heterogeneous cluster sizes. We derive asymptotic conditional bias and variance, establish uniform consistency, and prove asymptotic normality. Our findings reveal that under heterogeneous cluster sizes, the asymptotic variance includes a new term reflecting within-cluster dependence, which is overlooked when cluster sizes are presumed to be bounded. We propose valid approaches for bandwidth selection and inference, introduce estimators of the asymptotic variance, and demonstrate their consistency. In simulations, we verify the effectiveness of the cluster-robust bandwidth selection and show that the derived cluster-robust confidence interval improves the coverage ratio. We illustrate the application of these methods using a policy-targeting dataset in development economics.","creator":"Yuya Shimizu"},{"id":"2403.04057","slug":"to-spend-or-to-gain-online-learning-in-repeated-karma-auctions","title":"To Spend or to Gain: Online Learning in Repeated Karma Auctions","link":"https://arxiv.org/abs/2403.04057","abstract":"arXiv:2403.04057v1 Announce Type: cross  Abstract: Recent years have seen a surge of artificial currency-based mechanisms in contexts where monetary instruments are deemed unfair or inappropriate, e.g., in allocating food donations to food banks, course seats to students, and, more recently, even for traffic congestion management. Yet the applicability of these mechanisms remains limited in repeated auction settings, as it is challenging for users to learn how to bid an artificial currency that has no value outside the auctions. Indeed, users must jointly learn the value of the currency in addition to how to spend it optimally. In this work, we study the problem of learning to bid in two prominent classes of artificial currency auctions: those in which currency, which users spend to obtain public resources, is only issued at the beginning of a finite period; and those where, in addition to the initial currency endowment, currency payments are redistributed to users at each time step. In the latter class, the currency has been referred to as karma, since users do not only spend karma to obtain public resources but also gain karma for yielding them. In both classes, we propose a simple learning strategy, called adaptive karma pacing, and show that this strategy a) is asymptotically optimal for a single user bidding against competing bids drawn from a stationary distribution; b) leads to convergent learning dynamics when all users adopt it; and c) constitutes an approximate Nash equilibrium as the number of users grows. Our results require a novel analysis in comparison to adaptive pacing strategies in monetary auctions, since we depart from the classical assumption that the currency has known value outside the auctions, and moreover consider that the currency is both spent and gained in the class of auctions with redistribution.","creator":"Damien Berriaud, Ezzat Elokda, Devansh Jalota, Emilio Frazzoli, Marco Pavone, Florian D\\\"orfler"},{"id":"2403.04131","slug":"extract-mechanisms-from-heterogeneous-effects-identification-strategy-for-mediation-analysis","title":"Extract Mechanisms from Heterogeneous Effects: Identification Strategy for Mediation Analysis","link":"https://arxiv.org/abs/2403.04131","abstract":"arXiv:2403.04131v1 Announce Type: cross  Abstract: Understanding causal mechanisms is essential for explaining and generalizing empirical phenomena. Causal mediation analysis offers statistical techniques to quantify mediation effects. However, existing methods typically require strong identification assumptions or sophisticated research designs. We develop a new identification strategy that simplifies these assumptions, enabling the simultaneous estimation of causal and mediation effects. The strategy is based on a novel decomposition of total treatment effects, which transforms the challenging mediation problem into a simple linear regression problem. The new method establishes a new link between causal mediation and causal moderation. We discuss several research designs and estimators to increase the usability of our identification strategy for a variety of empirical studies. We demonstrate the application of our method by estimating the causal mediation effect in experiments concerning common pool resource governance and voting information. Additionally, we have created statistical software to facilitate the implementation of our method.","creator":"Jiawei Fu"},{"id":"2403.04236","slug":"regularized-deepiv-with-model-selection","title":"Regularized DeepIV with Model Selection","link":"https://arxiv.org/abs/2403.04236","abstract":"arXiv:2403.04236v1 Announce Type: cross  Abstract: In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we present the first method and analysis that can avoid all three limitations, while still enabling general function approximation. Specifically, we propose a minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can converge to the least-norm IV solution. Our method consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution, we learn the estimator by minimizing a Tikhonov-regularized loss function. We further show that our method allows model selection procedures that can achieve the oracle rates in the misspecified regime. When extended to an iterative estimator, our method matches the current state-of-the-art convergence rate. Our method is a Tikhonov regularized variant of the popular DeepIV method with a non-parametric MLE first-stage estimator, and our results provide the first rigorous guarantees for this empirically used method, showcasing the importance of regularization which was absent from the original work.","creator":"Zihao Li, Hui Lan, Vasilis Syrgkanis, Mengdi Wang, Masatoshi Uehara"},{"id":"2403.04530","slug":"multi-district-school-choice-playing-on-several-fields","title":"Multi-District School Choice: Playing on Several Fields","link":"https://arxiv.org/abs/2403.04530","abstract":"arXiv:2403.04530v1 Announce Type: cross  Abstract: We extend the seminal model of Pathak and S\\\"onmez (2008) to a setting with multiple school districts, each running its own separate centralized match, and focus on the case of two districts. In our setting, in addition to each student being either sincere or sophisticated, she is also either constrained - able to apply only to schools within her own district of residence - or unconstrained - able to choose any single district within which to apply. We show that several key results from Pathak and S\\\"onmez (2008) qualitatively flip: A sophisticated student may prefer for a sincere student to become sophisticated, and a sophisticated student may prefer for her own district to use Deferred Acceptance over the Boston Mechanism, irrespective of the mechanism used by the other district. We furthermore investigate the preferences of students over the constraint levels of other students. Many of these phenomena appear abundantly in large random markets.","creator":"Yannai A. Gonczarowski, Michael Yin, Shirley Zhang"},{"id":"2211.00329","slug":"weak-identification-in-low-dimensional-factor-models-with-one-or-two-factors","title":"Weak Identification in Low-Dimensional Factor Models with One or Two Factors","link":"https://arxiv.org/abs/2211.00329","abstract":"arXiv:2211.00329v2 Announce Type: replace  Abstract: This paper describes how to reparameterize low-dimensional factor models with one or two factors to fit weak identification theory developed for generalized method of moments models. Some identification-robust tests, here called \"plug-in\" tests, require a reparameterization to distinguish weakly identified parameters from strongly identified parameters. The reparameterizations in this paper make plug-in tests available for subvector hypotheses in low-dimensional factor models with one or two factors. Simulations show that the plug-in tests are less conservative than identification-robust tests that use the original parameterization. An empirical application to a factor model of parental investments in children is included.","creator":"Gregory Cox"},{"id":"2308.00014","slug":"a-new-mapping-of-technological-interdependence","title":"A new mapping of technological interdependence","link":"https://arxiv.org/abs/2308.00014","abstract":"arXiv:2308.00014v2 Announce Type: replace  Abstract: How does technological interdependence affect a sector's ability to innovate? This paper answers this question by looking at knowledge interdependence (knowledge spillovers and technological complementarities) and structural interdependence (intersectoral network linkages). We examine these two dimensions of technological interdependence by applying novel methods of text mining and network analysis to the documents of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO) between 1976 and 2021. We show that both dimensions positively affect sector innovation. While the impact of knowledge interdependence is slightly larger in the long-term horizon, positive shocks affecting the network linkages (structural interdependence) produce greater and more enduring effects on innovation performance in a relatively short run. Our analysis also highlights that patent text contains a wealth of information often not captured by traditional innovation metrics, such as patent citations.","creator":"A. Fronzetti Colladon, B. Guardabascio, F. Venturini"},{"id":"2308.05564","slug":"large-skew-t-copula-models-and-asymmetric-dependence-in-intraday-equity-returns","title":"Large Skew-t Copula Models and Asymmetric Dependence in Intraday Equity Returns","link":"https://arxiv.org/abs/2308.05564","abstract":"arXiv:2308.05564v2 Announce Type: replace  Abstract: Skew-t copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate skew-t factor copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. We show that intraday predictive densities from the skew-t copula are more accurate than from some other copula models, while portfolio selection strategies based on the estimated pairwise tail dependencies improve performance relative to the benchmark index.","creator":"Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn"},{"id":"2309.03403","slug":"sources-of-capital-growth","title":"Sources of capital growth","link":"https://arxiv.org/abs/2309.03403","abstract":"arXiv:2309.03403v3 Announce Type: replace  Abstract: Data from national accounts show no effect of change in net saving or consumption, in ratio to market-value capital, on change in growth rate of market-value capital (capital acceleration). Thus it appears that capital growth and acceleration arrive without help from net saving or consumption restraint. We explore ways in which this is possible, and discuss implications for economic teaching and public policy","creator":"Gordon Getty, Nikita Tkachenko"},{"id":"2312.05481","slug":"artificial-intelligence-in-the-knowledge-economy","title":"Artificial Intelligence in the Knowledge Economy","link":"https://arxiv.org/abs/2312.05481","abstract":"arXiv:2312.05481v4 Announce Type: replace  Abstract: How does Artificial Intelligence (AI) affect the organization of work? We incorporate AI into an economy where humans endogenously sort into hierarchical firms: Less knowledgeable agents become \"workers\" (i.e., execute routine tasks), while more knowledgeable agents becomes \"managers\" (i.e., specialize in problem solving). We model AI as an algorithm that uses capital (\"computing power\") to mimic the behavior of humans with a given knowledge. AI leads to major reorganizations, affecting occupational choice, wages, and the size and productivity of firms. By explicitly incorporating the endogenous organization of knowledge, our results provide new insights into the effects of AI on the future of work.","creator":"Enrique Ide, Eduard Talamas"},{"id":"2403.03317","slug":"competing-mechanisms-in-games-played-through-agents-theory-and-experiment","title":"Competing Mechanisms in Games Played Through Agents: Theory and Experiment","link":"https://arxiv.org/abs/2403.03317","abstract":"arXiv:2403.03317v2 Announce Type: replace  Abstract: This paper proposes Competing Mechanism Games Played Through Agent (CMGPTA), an extension of the GPTA (Prat and Rustichini (2003)), where a Principal can offer any arbitrary mechanism that specifies a transfer schedule for each agent conditional on all Agents' messages. We identify the set of equilibrium allocations using deviator-reporting mechanisms (DRMs) on the path and single transfer schedules off the path. We design a lab experiment implementing DRMs. We observe that implemented outcomes are efficient more often than random. A majority of the time, Agents do tell the truth on the identity of a deviating Principal, despite potential gains from (tacit) collusion on false reports. As play progresses, Agents learn to play with their counterparty Agent with the average predicted probability of collusion on false reports across groups increasing from about 9% at the beginning of the experiment to just under 20% by the end. However, group heterogeneity is significant.","creator":"Seungjin Han, Andrew Leal"}]}]