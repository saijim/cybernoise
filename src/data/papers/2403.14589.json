{"intro":"Imagine a world where language models can learn from their own experiences, improving themselves through trial and error. That's the promise of ReAct Meets ActRe, a cutting-edge AI framework that enables autonomous annotations of agent trajectories for contrastive self-training.","keywords":["autonomous learning","language agents","trajectory annotation"],"prompt":"A high-tech laboratory filled with advanced computing equipment and robotics, with a humanoid AI in the center, learning and improving itself through trial and error using the ReAct Meets ActRe framework.","summary":"ReAct Meets ActRe: A framework for autonomous annotations of agent trajectories in language models","text":"In the ever-evolving world of artificial intelligence (AI), researchers are constantly pushing the boundaries of what's possible. One such breakthrough is ReAct Meets ActRe, a groundbreaking AI framework that enables language agents to learn from their own experiences through autonomous annotations of agent trajectories for contrastive self-training.\n\nLanguage agents have already demonstrated impressive decision-making abilities by reasoning with foundation models. However, improving their performance has traditionally required considerable human effort, either through artificial annotations or the implementation of diverse prompting frameworks. With ReAct Meets ActRe, this is no longer the case.\n\nThe central role in this framework is played by an ActRe prompting agent, which can explain the reasoning behind any given action. When randomly sampling an external action, the ReAct-style agent can query the ActRe agent to obtain its textual rationales. By synthesizing new trajectories that prepend the posterior reasoning from ActRe to the sampled action, the ReAct-style agent can execute multiple trajectories for failed tasks and select the successful ones to supplement its failed trajectory for contrastive self-training.\n\nThis closed loop of language agent self-improvement is made possible by policy gradient methods with binarized rewards, allowing the language agent to learn from its own experiences and continually improve over time. In experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2, agents trained with ReAct Meets ActRe significantly outperformed existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.\n\nIn AlfWorld, the agent trained with ReAct Meets ActRe obtained a 1-shot success rate of 96% and achieved 100% success with just four iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matched human average, and four rounds of iterative refinement led to performance approaching that of human experts.\n\nReAct Meets ActRe represents a major step forward in the field of AI, enabling language agents to learn from their own experiences and continually improve over time. With its ability to autonomously annotate agent trajectories for contrastive self-training, this framework has the potential to revolutionize the way we think about language models and their abilities.","title":"ReAct Meets ActRe: The Future of Autonomous Learning in Language Models","link":"https://arxiv.org/abs/2403.14589","id":"2403.14589","slug":"react-meets-actre-the-future-of-autonomous-learning-in-language-models","creator":"Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu","topic":"artificial-intelligence"}