{
  "title": "Boosting Automatic Speech Recognition with Contextual Biasing of Named-Entities using Large Language Models",
  "summary": "This paper presents a novel approach to enhancing Automatic Speech Recognition (ASR) performance by leveraging contextual biasing with Large Language Models (LLMs).",
  "intro": "Are you tired of faulty speech recognition systems that keep misunderstanding your commands? Well, get ready to experience a futuristic breakthrough in Automatic Speech Recognition (ASR) technology! In this groundbreaking study, researchers have discovered how to boost the performance of ASR systems using cutting-edge techniques involving Large Language Models (LLMs) and contextual biasing of named-entities. Say goodbye to speech recognition errors and hello to seamless communication!",
  "text": "As technology advances at an unprecedented pace, so does the need for more accurate and reliable speech recognition systems. Currently, Automatic Speech Recognition (ASR) systems suffer from various challenges, such as misinterpreting named-entities and producing errors in transcriptions. But fear not, as a team of brilliant scientists has revolutionized the field of ASR by introducing a game-changing technique called contextual biasing with Large Language Models (LLMs).\n\nThe main idea behind this technique is to provide additional contextual information to LLMs during the second-pass rescoring of ASR outputs. By incorporating prompts that include a biasing list and a few-shot examples, the researchers were able to boost the performance of ASR systems without the need for extensive fine-tuning. These prompts act as crucial hints to the LLMs, guiding them to understand and interpret named-entities more accurately, leading to improved speech recognition results.\n\nBut that's not all! The research team also proposed a multi-task training approach for the LLMs. This training involves teaching the LLMs to predict both the entity class and the next token. By simultaneously training the LLMs on these tasks, the models gain a deeper understanding of the context and enhance their ability to generate more accurate transcriptions.\n\nTo address the challenge of exceeding LLMs' maximum sequence lengths and optimize the efficiency of contextual biasing, the researchers introduced a concept called dynamic prompting. In dynamic prompting, the most likely entity class is selected using class tag predictions, and only the entities within this class are used as contexts for the next token prediction. This selective approach ensures that only the most relevant information is utilized, resulting in improved performance and avoiding any limitations imposed by sequence length constraints.\n\nTo evaluate the effectiveness of their proposed techniques, the research team conducted Word Error Rate (WER) evaluations on both an internal dataset consisting of calling, messaging, and dictation samples, as well as the widely-used SLUE-Voxpopuli dataset. The results were nothing short of remarkable. The use of biasing lists and few-shot examples achieved a relative improvement of 17.8% compared to the first pass ASR. Furthermore, the multi-task training approach and dynamic prompting led to relative WER improvements of 20.0% and 11.3%, respectively. These results demonstrate the immense potential of contextual biasing techniques in revolutionizing the field of ASR.\n\nImagine a world where you can seamlessly interact with your devices through flawless speech recognition. With contextual biasing and the power of Large Language Models, that world is now within reach. Say goodbye to frustrating communication barriers and hello to a future where machines understand us better than ever before!",
  "keywords": [
    "automatic speech recognition",
    "contextual biasing",
    "large language models",
    "named-entities",
    "multi-task training"
  ],
  "prompt": "a person speaking naturally to a smart device that flawlessly recognizes their speech and responds accurately.",
  "link": "http://arxiv.org/abs/2309.00723",
  "id": "86bed72b16fdc07c6332f85f8eb03d58",
  "slug": "boosting-automatic-speech-recognition-with-contextual-biasing-of-named-entities-using-large-language-models",
  "creator": "Chuanneng Sun, Zeeshan Ahmed, Yingyi Ma, Zhe Liu, Yutong Pang, Ozlem Kalinli",
  "topic": "artificial-intelligence"
}
