{"title":"CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics","summary":"Researchers introduce CombiBench, a benchmark for testing AI's ability to solve complex combinatorial math problems, paving the way for a new era in neurosymbolic AI.","intro":"Imagine an AI that can solve the most baffling combinatorial math problems, unlocking new secrets of the universe! Sounds like science fiction, but it's becoming a reality. Dive in to discover the latest breakthrough!","text":"The world of mathematics is on the cusp of a revolution, and it's all thanks to the rise of neurosymbolic AI. By combining the power of large language models with formal reasoning, researchers have achieved human-level performance on math competition problems in algebra, geometry, and number theory. But there's a new challenge on the horizon: combinatorics. To tackle this, the CombiBench benchmark has been introduced, comprising 100 combinatorial problems formalized in Lean 4 and paired with their corresponding informal statements. This comprehensive benchmark covers a wide range of difficulty levels, from middle school to university level, and spans over ten combinatorial topics. It's the perfect testing ground for AI's IMO solving capabilities, featuring all IMO combinatorial problems since 2000 (except IMO2004 P3). With the Fine-Eval evaluation framework, researchers can assess AI's performance on both proof-based problems and fill-in-the-blank questions. The results are promising, with Kimina-Prover achieving the best results among tested models, solving 7 problems out of 100. Although there's still a long way to go, this breakthrough paves the way for a new era in AI-driven mathematics. As researchers continue to push the boundaries, we may soon see AI tackling the most complex combinatorial problems, unlocking new insights and discoveries. The future of math has never looked brighter!","keywords":["AI in Mathematics","Combinatorial Problems","Neurosymbolic AI","CombiBench","IMO Solving"],"prompt":"Create a futuristic, cyberpunk-inspired image depicting a robotic mathematician surrounded by glowing mathematical equations and combinatorial diagrams, in the style of Syd Mead and Ash Thorp, with a mix of digital painting and 3D rendering, incorporating vibrant neon colors and a sense of dynamic energy","id":"2505.03171","slug":"revolutionizing-math-with-ai-can-machines-master-the-magic-of-combinatorics","link":"https://arxiv.org/abs/2505.03171","abstract":"Abstract: Neurosymbolic approaches integrating large language models with formal reasoning have recently achieved human-level performance on mathematics competition problems in algebra, geometry and number theory. In comparison, combinatorics remains a challenging domain, characterized by a lack of appropriate benchmarks and theorem libraries. To address this gap, we introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean~4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level, and span over ten combinatorial topics. CombiBench is suitable for testing IMO solving capabilities since it includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its statement contain an images). Furthermore, we provide a comprehensive and standardized evaluation framework, dubbed Fine-Eval (for $\\textbf{F}$ill-in-the-blank $\\textbf{in}$ L$\\textbf{e}$an Evaluation), for formal mathematics. It accommodates not only proof-based problems but also, for the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval as the evaluation method and Kimina Lean Server as the backend, we benchmark several LLMs on CombiBench and observe that their capabilities for formally solving combinatorial problems remain limited. Among all models tested (none of which has been trained for this particular task), Kimina-Prover attains the best results, solving 7 problems (out of 100) under both ``with solution'' and ``without solution'' scenarios. We open source the benchmark dataset alongside with the code of the proposed evaluation method at https://github.com/MoonshotAI/CombiBench/.","creator":"Junqi Liu, Xiaohan Lin, Jonas Bayer, Yael Dillies, Weijie Jiang, Xiaodan Liang, Roman Soletskyi, Haiming Wang, Yunzhou Xie, Beibei Xiong, Zhengfeng Yang, Jujian Zhang, Lihong Zhi, Jia Li, Zhengying Liu","topic":"artificial-intelligence"}