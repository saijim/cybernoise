{"title":"Empowering Language Agents: Autonomous Trajectory Annotation for Self-Improvement","summary":"A new study introduces A$^3$T, a framework that enables autonomous annotation of agent trajectories through the use of ReAct and ActRe prompting agents. This approach results in significant performance improvements in language agents.","intro":"In a groundbreaking study, researchers have developed a new method for training language agents that significantly improves their decision-making abilities. The method, called A$^3$T (Autonomous Annotation of Agent Trajectories), uses two prompting agents - ReAct and ActRe - to generate and annotate agent trajectories, which are then used in a closed loop for multiple rounds of language agent self-improvement.","text":"Language agents have shown remarkable abilities in decision-making by reasoning with foundation models. However, training these agents for performance improvement remains a challenge due to the need for multi-step reasoning and action trajectories. Traditional methods require significant human effort, either through artificial annotations or prompting frameworks.\n\nTo address this issue, researchers at [institution] have developed A$^3$T - a framework that enables autonomous annotation of agent trajectories in the style of ReAct. The central role in this framework is an ActRe prompting agent, which explains the reason for any action. When randomly sampling an external action, the ReAct-style agent can query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action.\n\nThrough this process, the ReAct-style agent executes multiple trajectories for failed tasks and selects successful ones to supplement its failed trajectory for contrastive self-training. This closed loop approach allows for multiple rounds of language agent self-improvement through policy gradient methods with binarized rewards.\n\nExperiments conducted using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2 showed significant improvements in performance. In AlfWorld, the agent trained with A$^3$T obtained a 1-shot success rate of 96% and 100% success with four iterative rounds. In WebShop, the 1-shot performance matched human average and approached that of human experts after four rounds of refinement.\n\nThese results demonstrate the potential of A$^3$T as a powerful tool for training language agents to make autonomous decisions in complex environments.","keywords":["language agent","trajectory annotation","ReAct","ActRe","self-improvement"],"prompt":"Generate an image of a futuristic robot using A$^3$T framework for decision-making in a cyberpunk world.","link":"https://arxiv.org/abs/2403.14100","id":"2403.14100","slug":"empowering-language-agents-autonomous-trajectory-annotation-for-self-improvement","creator":"Steven Mascaro, Yue Wu, Ross Pearson, Owen Woodberry, Jessica Ramsay, Tom Snelling, Ann E. Nicholson","topic":"artificial-intelligence"}