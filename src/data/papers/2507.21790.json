{"title":"Can large language models assist choice modelling? Insights into prompting strategies and current models capabilities","summary":"New breakthroughs show that advanced AI like ChatGPT and Claude 4 can help build smarter, more reliable models for predicting human choices—without needing expert coding or years of training.","intro":"Imagine an AI that doesn’t just answer questions—but designs better decision-making tools for cities, healthcare, and transportation… all by thinking through the logic like a top scientist. Sounds like sci-fi? It’s already happening—and the results are mind-blowing.","text":"In a world where every click, swipe, and choice shapes our lives, scientists and planners need smarter ways to predict what people will do. That’s where choice modelling comes in—using math and data to forecast decisions, from which train people take to which medicines they’ll accept. For decades, this was a complex, time-consuming job, requiring deep statistical knowledge and hours of coding. But now, thanks to the rise of Large Language Models (LLMs)—like ChatGPT, Claude, and Gemini—this field is getting a futuristic upgrade powered by artificial intelligence.\n\nA groundbreaking new study tested whether these AI systems could actually help design and even estimate Multinomial Logit (MNL) models—the gold standard in choice modelling. Researchers put 13 versions of six top AI models through their paces, asking them to suggest utility functions (the math behind choice predictions), and in some cases, even run the actual analysis. The results? Shocking—and full of promise.\n\nThe best performers? Proprietary models like GPT-4 and Claude 4 Sonnet. These AIs didn’t just spit out random formulas—they built models that fit real-world data better than many human-designed ones, with strong logic, realistic assumptions, and even complex structures. GPT-4, in particular, stood out for stability and consistency—its suggestions were reliable across different scenarios, like predicting travel choices or healthcare decisions.\n\nBut here’s the twist: sometimes, less is more. When researchers gave the AI only a data dictionary (a description of what each variable means), instead of raw data, the models performed even better. Why? Because without seeing the actual numbers, the AI focused more on logical reasoning—like a brilliant student studying theory instead of just crunching numbers. This suggests that limiting data access might actually help AI think more deeply about the problem.\n\nEven more impressive? GPT-o3 didn’t just suggest a model—it wrote the code to estimate it, ran it, and validated the results on its own. That’s not just assistance—this is AI becoming a full co-pilot in scientific research.\n\nOpen-source models like Llama and Gemma? They struggled. While they can process information, they lack the refined reasoning and consistency of their closed, proprietary counterparts. This doesn’t mean they’re useless—just that they need more training, better prompts, and smarter frameworks to catch up.\n\nSo what does this mean for the future? Imagine a city planner using an AI to instantly design a model predicting how a new subway line will affect commuter choices. Or a public health official asking an AI to suggest how to encourage vaccination by understanding what motivates people. These aren’t distant dreams—they’re real possibilities within reach.\n\nThe key takeaway? LLMs aren’t replacing human experts. They’re supercharging them. With the right prompting—like using step-by-step reasoning (Chain-of-Thought) or clear instructions—these AIs become powerful partners in building better models faster, cheaper, and with greater insight.\n\nAnd the best part? The technology is already here. You don’t need a PhD in statistics to use it. With a few well-crafted prompts, anyone—from urban designers to social scientists—can tap into AI’s ability to think through complex decisions. This isn’t just about automation—it’s about democratizing science, making advanced tools accessible to everyone.\n\nAs we move into a future where AI helps us make smarter choices—from personal decisions to global policies—this breakthrough marks a turning point. We’re no longer just analyzing data. We’re letting AI help us ask better questions, build smarter models, and ultimately, make better decisions for all of us.\n\nThe future of choice modelling isn’t just smarter. It’s human-powered, AI-assisted, and full of hope.","keywords":["AI decision-making","choice modelling","large language models","GPT-4","future of science"],"prompt":"Futuristic cyberpunk cityscape with glowing neural networks floating above streets, an AI hologram in the center explaining complex choice models with animated data streams, inspired by the art style of Syd Mead and Blade Runner 2049, vibrant neon colors, high detail, digital painting, cinematic lighting, 8K resolution","id":"2507.21790","slug":"ai-just-learned-to-design-better-choices-here-s-how-it-s-changing-the-future-of-decision-science","link":"https://arxiv.org/abs/2507.21790","abstract":"Abstract: Large Language Models (LLMs) are widely used to support various workflows across different disciplines, yet their potential in choice modelling remains relatively unexplored. This work examines the potential of LLMs as assistive agents in the specification and, where technically feasible, estimation of Multinomial Logit models. We implement a systematic experimental framework involving thirteen versions of six leading LLMs (ChatGPT, Claude, DeepSeek, Gemini, Gemma, and Llama) evaluated under five experimental configurations. These configurations vary along three dimensions: modelling goal (suggesting vs. suggesting and estimating MNLs); prompting strategy (Zero-Shot vs. Chain-of-Thoughts); and information availability (full dataset vs. data dictionary only). Each LLM-suggested specification is implemented, estimated, and evaluated based on goodness-of-fit metrics, behavioural plausibility, and model complexity. Findings reveal that proprietary LLMs can generate valid and behaviourally sound utility specifications, particularly when guided by structured prompts. Open-weight models such as Llama and Gemma struggled to produce meaningful specifications. Claude 4 Sonnet consistently produced the best-fitting and most complex models, while GPT models suggested models with robust and stable modelling outcomes. Some LLMs performed better when provided with just data dictionary, suggesting that limiting raw data access may enhance internal reasoning capabilities. Among all LLMs, GPT o3 was uniquely capable of correctly estimating its own specifications by executing self-generated code. Overall, the results demonstrate both the promise and current limitations of LLMs as assistive agents in choice modelling, not only for model specification but also for supporting modelling decision and estimation, and provide practical guidance for integrating these tools into choice modellers' workflows.","creator":"Georges Sfeir, Gabriel Nova, Stephane Hess, Sander van Cranenburgh","topic":"economics"}