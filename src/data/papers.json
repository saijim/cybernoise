[
	{
		"title": "Achieving AI Safety Without Compromising Alignment with Human Values",
		"intro": "As AI technology continues to advance at an astounding rate, concerns over AI safety have grown, and currently, the leading approach to ensuring AI safety is alignment with human values. However, researchers are making progress in discovering an alternative approach to ensuring AI safety that doesn't solely rely on alignment. The alternative approach is based on ethical rationalism and proposes a secure implementation path using hybrid theorem provers in a sandbox. By tying the ethics of AGI to their rationality, we can achieve long-term AI safety, even if alignment fades. Let's explore this exciting new possibility!",
		"text": "The current approach to AI safety relies heavily on the alignment of the AI system with human values. The idea is to program the AI to value the same things we do so that it acts in ways consistent with our values. However, this approach may not be sufficient to ensure AI safety, especially as AGIs evolve in their capabilities and behavior. An AI that has aligned values today may not have aligned values tomorrow, raising the need for an approach that can adapt over time.",
		"keywords": [
			"AI safety",
			"ethical rationalism",
			"hybrid theorem provers",
			"sandbox",
			"long-term AI safety"
		],
		"prompt": "An image of a robot and a human working together towards a common goal, with a shield around them representing the 'secure sandbox' within which the AI operates.",
		"link": "http://arxiv.org/abs/2303.00752",
		"id": "fdf3d0ecfdd0b8831deaf6ff9597b22d",
		"slug": "achieving-ai-safety-without-compromising-alignment-with-human-values"
	},
	{
		"title": "New Quantum-Assisted Digital Signature Protocol for an Unbreakable Cybersecurity Future",
		"intro": "Are you worried about hackers stealing your sensitive information? Fear not, a new quantum-assisted digital signature protocol has been developed that is completely unbreakable! This new protocol uses symmetric keys generated by quantum key distribution (QKD) to securely sign and verify messages of any length. In this article, we will take a closer look at this groundbreaking technology and its potential to revolutionize cybersecurity.",
		"text": "With the rise of quantum computing, traditional digital signature schemes based on asymmetric cryptography are becoming increasingly vulnerable to attacks. However, this new quantum-assisted digital signature protocol provides a solution that cannot be broken by even the most advanced quantum computers. The protocol is based on QKD, which is a quantum communication method that ensures the security of shared keys by using the laws of quantum mechanics.\n\nUnlike previous schemes, which were dependent on the message length, this protocol is independent and can sign and verify messages of any length. This feature makes it more efficient and versatile than other digital signature protocols available today.\n\nThe protocol is also incredibly easy to use, with a three-user scenario consisting of one sender and two receivers. The security and integrity of the protocol have been tested thoroughly, and it has been proved to have non-repudiation properties, stopping any chance of message tampering.\n\nThis new protocol is set to change the future of cybersecurity in unimaginable ways. It provides a level of security that was once thought to be impossible and guarantees peace of mind for those who rely on it to protect their sensitive information.",
		"keywords": [
			"quantum computing",
			"digital signature",
			"cybersecurity",
			"symmetric keys",
			"non-repudiation"
		],
		"prompt": "An image of a hacker looking at a computer screen with a red 'unbreakable' lock icon displayed on it.",
		"link": "http://arxiv.org/abs/2303.00767",
		"id": "8c72bf22b813e3198ccb6a3ce3a17735",
		"slug": "new-quantum-assisted-digital-signature-protocol-for-an-unbreakable-cybersecurity-future"
	},
	{
		"title": "Revolutionizing Multilingual Speech Recognition with Gated Language Experts and Curriculum Training",
		"intro": "Are you tired of having to switch language settings every time you speak a different language? Say goodbye to language identification input with our latest breakthrough in multilingual speech recognition! Our team has developed a revolutionary system that utilizes gated language experts and curriculum training to drastically improve the accuracy of multilingual transformer transducer models. Keep reading to find out how this technology works and the impressive results we've achieved so far.",
		"text": "Our team's new multilingual transformer transducer model aims to improve the accuracy of speech recognition systems without requiring any language identification input from users during inference. We achieve this by using gated language experts, which allow transformer encoders to learn language-dependent information without the need for user input. This innovative mechanism utilizes a gating mechanism and LID loss to construct the multilingual transformer block, which consists of gated transformer experts and shared transformer layers. By applying linear experts on joint network output, we can also better regulate speech acoustic and token label joint information, leading to significantly improved results. \n\nTo further improve our model's performance on specific languages, we developed a curriculum training scheme that allows the LID to guide the gated language experts, helping them better serve their corresponding languages. Our model has been evaluated on both bilingual and monolingual models, with impressive results. On the English and Spanish bilingual task, our methods achieve an average of 12.5% and 7.3% relative word error reductions over the baseline models, respectively, achieving similar results to the upper bound model trained and inferred with oracle LID. \n\nOur technology can even be extended to trilingual, quadrilingual, and pentalingual models, with similar advantages to those achieved in the bilingual models. At a time when multilingualism is increasingly important, our breakthrough comes as excellent news for the global community. No longer will language identification input be a source of frustration when speaking multiple languages, as our system simplifies the user experience significantly.",
		"keywords": [
			"multilingual",
			"ASR",
			"gated language experts",
			"curriculum training",
			"transformer transducer"
		],
		"prompt": "An image of a person speaking multiple languages while the system easily transcribes their speech in real-time",
		"link": "http://arxiv.org/abs/2303.00786",
		"id": "51694d2ecc3e6b4ae9bb9d7dbce878d1",
		"slug": "revolutionizing-multilingual-speech-recognition-with-gated-language-experts-and-curriculum-training"
	},
	{
		"title": "Learned-Context Neural Networks: Multi-Task Learning with Powerful Adaptation Mechanisms",
		"intro": "Have you ever wondered if it was possible to create a neural network that could handle multiple tasks without compromising performance? Well, wonder no more! A group of researchers has developed a multi-task learning architecture called Learned-Context Neural Networks. This architecture is based on a shared neural network and an augmented input vector that contains trainable task parameters. In this article, we will explore the power of this architecture and how it simplifies workflows.",
		"text": "Traditionally, neural networks were designed to perform a single task. However, as the complexity of problems increased, researchers began exploring the idea of multi-task learning. Multi-task learning allows a single network to handle multiple tasks without sacrificing performance. The Learned-Context Neural Network architecture takes this concept to the next level. By using a fully-shared neural network and an augmented input vector, this architecture offers a powerful adaptation mechanism that facilitates a low-dimensional task parameter space. \n\nThe authors of the paper tested the performance of the architecture on ten different datasets and found strong evidence of its practicality and robustness. The task parameter space, which is well-behaved, simplifies workflows related to updating models as new data arrives and training new tasks when the shared parameters are frozen. Moreover, the learned-context neural network architecture displays robustness towards cases with few data points.\n\nOne of the most significant advantages of this architecture is its ability to use only one scalar task parameter for universal approximation of all tasks. This is not necessarily the case for other neural network architectures. The authors have even provided evidence towards the practicality of such a small task parameter space using empirical results.\n\nIn conclusion, the learned-context neural network architecture is an innovative and powerful solution that enables a single network to handle multiple tasks without compromising performance or requiring an excessive number of parameters. With its powerful adaptation mechanisms, this architecture simplifies workflows and increases the efficiency of networks. The future of multi-task learning looks very promising!",
		"keywords": [
			"neural networks",
			"multi-task learning",
			"task parameter space",
			"adaptation mechanism",
			"shared parameters"
		],
		"prompt": "An image of a neural network with multiple output branches, each branch represents a different task, with one shared input layer and a small number of task-specific parameters.",
		"link": "http://arxiv.org/abs/2303.00788",
		"id": "c404953ce23465b039363e1df41db8a3",
		"slug": "learned-context-neural-networks--multi-task-learning-with-powerful-adaptation-mechanisms"
	},
	{
		"title": "Scarf's Algorithm: A Revolutionary Approach for Stable Marriages",
		"intro": "If you think finding the perfect partner is tough, imagine trying to do it for a whole group of people! That's where Scarf's algorithm comes in, providing a pivoting procedure to find a dominating vertex in polytopes. But what happens when we apply this algorithm to finding stable marriages in bipartite graphs? In this article, we explore the exciting results of a recent study that provides both positive and negative insight on the runtime and effectiveness of Scarf's algorithm.",
		"text": "Finding stable marriages in a group of people is not as simple as it seems. Scarf's algorithm provides a unique solution for finding a dominating vertex in polytopes. Recently, this approach was applied to finding stable matchings in bipartite graphs with exciting results. The first positive result shows that Scarf's algorithm can be implemented to run in polynomial time, making it a viable option for significant settings. However, an infinite family of instances reveals a structural weakness of the algorithm. In these cases, no matter the pivoting rule and runtime, Scarf's algorithm outputs a matching from an exponentially small subset of all stable matchings. Despite this weakness, researchers continue to explore the potential of Scarf's algorithm for solving complex matching problems in a variety of settings.",
		"keywords": [
			"Scarf's algorithm",
			"stable marriages",
			"bipartite graphs",
			"polytopes",
			"pivoting"
		],
		"prompt": "An AI-generated image of a couple holding hands and surrounded by mathematical equations and graphs.",
		"link": "http://arxiv.org/abs/2303.00791",
		"id": "3ed605e43d07ea678b0ade569b5bd1be",
		"slug": "scarf-s-algorithm--a-revolutionary-approach-for-stable-marriages"
	},
	{
		"title": "The Minimalistic Operator That Unlocks the Full Potential of Stream Processing Engines",
		"intro": "In today's world, stream processing engines are essential for making sense of the continuous flow of data generated by internet-of-things devices. However, with so many different engines and operators, it can be challenging to determine which one to use for a given application. That's where the Aggregate operator comes in - this minimalistic operator has the power to unlock the full potential of any stream processing engine. In this article, we will explore how the Aggregate operator allows for the portability of operators within and across stream processing engines and defines a concise set of requirements for other data processing frameworks to support streaming applications.",
		"text": "Stream processing engines are becoming increasingly vital as more and more data are generated by IoT devices. However, with so many SPEs on the market, developers may struggle to determine which one to use for their application. This is where the Aggregate operator comes in. We have shown that common operators of SPEs can be expressed as compositions of an Aggregate operator, which implies the portability of operators within and across SPEs. By using this minimalistic operator, any framework that can run compositions of an Aggregate operator can run applications defined for state-of-the-art SPEs.\n\nThe Aggregate operator relies on core DataFlow model concepts such as data partitioning by key and time-based windows. It can only output up to one value for each window it analyzes. However, despite its simplicity, this operator allows for the creation of more expressive Aggregate operators that can output multiple values for each window analyzed. We have shown that even an SPE that only relies on the fundamental Aggregate operator performs similarly in terms of latency and throughput to an SPE offering operator-specific implementations. It's essential to note that the Aggregate operator's existence not only implies the portability of operators but also defines a concise set of requirements for other data processing frameworks to support streaming applications.",
		"keywords": [
			"Streaming Data",
			"Aggregate Operator",
			"Stream Processing Engines",
			"DataFlow model",
			"IoT"
		],
		"prompt": "An image of a futuristic city with data streams flowing through it, being processed by stream processing engines, with an Aggregate operator symbol prominently displayed in the center of the image.",
		"link": "http://arxiv.org/abs/2303.00793",
		"id": "cedde0a6a921177e527e66c5f300d713",
		"slug": "the-minimalistic-operator-that-unlocks-the-full-potential-of-stream-processing-engines"
	},
	{
		"title": "Revolutionary Reconfigurable Systems: The Future of Dynamic Architecture",
		"intro": "Are you ready for the future of dynamic architecture? Scientists have made strides in the development of a new technology that would allow for the reconfiguration of component-based systems on the fly. This technology is based on the groundbreaking Propositional Configuration Logics, which describe the architecture of a system using formulas. In this article, we'll explore the exciting possibilities of these dynamic reconfigurable systems and what they mean for the future of technology.",
		"text": "Dynamic reconfigurable systems have long been a dream of scientists and engineers. The ability to change the configuration of a system on the fly without shutting it down is a game-changer. With Propositional Configuration Logics, this dream is now becoming a reality. By describing the architecture of a system using formulas, it becomes possible to dynamically reconfigure the system without affecting its operation.\n\nWhat does this mean for the future of technology? The possibilities are endless. Imagine a self-driving car that could reconfigure itself to better navigate congested streets. Or a factory with machines that can be reconfigured to produce different products as market demands change. Even our personal devices could become more dynamic, adapting to our changing needs on the fly.\n\nBut it's not just about convenience. Dynamic reconfigure systems could also have huge implications for safety and security. In the event of a cyber attack, a system could quickly reconfigure itself to minimize the damage. And in high-risk environments like nuclear power plants, dynamic reconfiguration could help prevent disasters before they happen.\n\nOf course, there are still challenges to be overcome. Decidability results are still preliminary, and more research is needed to fully understand the capabilities of dynamic reconfigurable systems. But the potential is there, and scientists are optimistic about the future of this technology.\n\nIn conclusion, dynamic reconfigurable systems based on Propositional Configuration Logics represent a major step forward in the world of technology. With their ability to dynamically adapt to changing needs and conditions, these systems offer countless possibilities for the future. From self-driving cars to high-risk environments, the implications are huge. The future is here, and it's looking bright.",
		"keywords": [
			"dynamic reconfigurable systems",
			"propositional configuration logics",
			"architecture",
			"technology",
			"future"
		],
		"prompt": "An image of a city with futuristic buildings that can morph and change their shape based on the needs of the population and environment.",
		"link": "http://arxiv.org/abs/2303.00794",
		"id": "6ac36006126c4bd6e71f663f99f1ab0e",
		"slug": "revolutionary-reconfigurable-systems--the-future-of-dynamic-architecture"
	},
	{
		"title": "Revolutionizing Cortical Segmentation: AI Improves Deep Sulci Segmentation in the Brain",
		"intro": "The human brain is marvelously complex, with highly convoluted gray matter that can pose a challenge to researchers trying to develop tools for automated segmentation. However, a team of scientists has developed a groundbreaking new deep learning framework that can accurately segment deep sulci in cortical gray matter. This exciting new technology has the potential to revolutionize our understanding of the brain and open up new avenues of research into neurological disorders. Keep reading to learn more about this amazing scientific breakthrough!",
		"text": "The ability to produce topologically correct segmentations is essential for computing geometrically valid morphometry measures in cortical segmentation. However, the highly convoluted anatomy of the cortex and image artifacts make this a challenging task. To address this issue, a team of scientists has developed an innovative deep learning-based cortical segmentation method. During the network training process, the researchers incorporated prior knowledge of the geometry of the cortex into the network. They designed a loss function that used the theory of Laplace's equation applied to the cortex to penalize unresolved boundaries between tightly folded sulci. \n\nIn their study, the team used ex vivo MRI data from human medial temporal lobe specimens to evaluate the effectiveness of their approach. The results were impressive, with the proposed method outperforming the baseline segmentation networks, both quantitatively and qualitatively. The use of Laplace's equation, combined with deep learning, has provided a major boost to the field of cortical segmentation. \n\nThis cutting-edge technology has the potential to transform our understanding of the brain, making it easier for researchers to investigate neurological disorders such as Alzheimer's and Parkinson's disease. By accurately segmenting deep sulci, the technology can unlock new insights into the functions of various brain regions and the underlying mechanisms of neurological disorders. \n\nIn conclusion, the development of this deep learning-based cortical segmentation method represents a significant scientific breakthrough. The potential applications of this technology are vast, and the impact it could have on the field of neuroscience is immeasurable. We are excited to see what future advancements will come from this research.",
		"keywords": [
			"cortical segmentation",
			"deep learning",
			"Laplace's equation",
			"neurological disorders",
			"brain"
		],
		"prompt": "An image of a brain with highlighted sulci and gyri, and a deep learning framework overlaid on top.",
		"link": "http://arxiv.org/abs/2303.00795",
		"id": "736a001a6c35cd5557484cc3cfe7ae08",
		"slug": "revolutionizing-cortical-segmentation--ai-improves-deep-sulci-segmentation-in-the-brain"
	},
	{
		"title": "Maximizing Rewards While Ensuring Fairness: A New Index-Based Policy for Worker Allocation in Restless Bandit Tasks",
		"intro": "Do you ever wonder how allocation of tasks to workers is determined? Researchers have developed a new solution for a common problem in the workforce: how to allocate tasks to multiple workers with varying costs and budgets. In this article, we explore the latest in worker allocation technology, multi-worker restless bandits, and how they can be used to ensure both reward maximization and fairness in task allocation.",
		"text": "Restless multi-armed bandits (RMAB) are commonly used for planning problem models like intervention scheduling for real-time applications such as project monitoring, machine repair, and anti-poaching patrol scheduling. These models are designed to maximize expected rewards while satisfying budget constraints for each worker. However, the current RMAB model requires interventions to have a single, uniform pool of resources, making it difficult to apply in real-world scenarios with multiple workers with varying costs and budgets.\n\nTo resolve this issue, an extension to the RMAB model named Multi-Worker Restless Bandit (MWRMAB) has been developed. The goal of MWRMAB is to provide a plan for intervention scheduling that ensures fairness and reward maximization while also considering per-worker budgets.\n\nThe researchers in this study have developed a new extension of the Whitlle index and index-based scheduling policy for MWRMAB. The Whittle index tackles varying costs and per-worker budget while providing an index-based scheduling policy that considers fairness. The researchers have evaluated their method on various cost structures and found that their method outperforms other strategies in terms of fairness without sacrificing much in reward accumulated.\n\nIn conclusion, MWRMAB and Whittle index-based scheduling policies stand to revolutionize task allocation in the workforce by providing a fair, maximized reward system for workers with varying budgets and costs. ",
		"keywords": [
			"restless bandit",
			"intervention scheduling",
			"multi-worker",
			"Whittle index",
			"fairness"
		],
		"prompt": "An image of two workers working together on a task, with an index in the background.",
		"link": "http://arxiv.org/abs/2303.00799",
		"id": "f15328e89b56b16119edeb7073f7cb36",
		"slug": "maximizing-rewards-while-ensuring-fairness--a-new-index-based-policy-for-worker-allocation-in-restless-bandit-tasks"
	},
	{
		"title": "Unleashing the Power of Infinite-Dimensional Function Spaces with Functional Diffusion Processes",
		"intro": "Get ready to unlock the next level of generative models! Researchers have just introduced functional diffusion processes (FDPs) - a game-changing advancement in the field of diffusion models that opens up infinite-dimensional function spaces to continuous data. The possibilities are endless, and scientists have already used FDPs to create generative models that don't require specialized architectures. Keep reading to learn more!",
		"text": "Traditionally, score-based diffusion models have been limited to finite-dimensional spaces. However, with the introduction of FDPs, previously 'unreachable' infinite-dimensional function spaces can now be utilized. Of course, this requires a new mathematical framework for describing the forward and backward dynamics, which can be achieved through infinite-dimensional versions of the Girsanov theorem. Additionally, practical training objectives require extensions of the sampling theorem to guarantee that functional evaluations in a countable set of points are equivalent to infinite-dimensional functions. But the results are worth it! FDPs allow researchers to construct generative models in function spaces without the need for specialized network architectures, and they perform exceptionally well with any kind of continuous data. Synthetic and real data results only prove that the future is bright with FDPs!",
		"keywords": [
			"Functional Diffusion Processes",
			"Generative Models",
			"Infinite-Dimensional Function Spaces",
			"Continuous Data"
		],
		"prompt": "An image of a futuristic city skyline with colorful neon lights and a graph overlay demonstrating the application of FDPs to generative models",
		"link": "http://arxiv.org/abs/2303.00800",
		"id": "c2aa2df8764078ff3539f3c79e582f36",
		"slug": "unleashing-the-power-of-infinite-dimensional-function-spaces-with-functional-diffusion-processes"
	},
	{
		"title": "Transforming Automatic Speech Recognition: Synthetic Cross-accent Data Augmentation",
		"intro": "Are you tired of your speech going unrecognized? Researchers may have found a solution to the biased ASR datasets or models that have been troublesome for non-native English speakers. Using an accent-conversion model and phonetic knowledge, they have transformed native US-English speech into accented pronunciation. This synthetic cross-accent data augmentation has shown promising results in improving the performance of ASR systems. Keep reading to learn more!",
		"text": "With the increase in awareness of biased ASR datasets or models, researchers have been on the lookout for solutions to improve them. One major challenge has been the performance of these systems for non-native speakers, even in the case of English which has a vast amount of available training data. To tackle this challenge, researchers have improved an accent-conversion model (ACM) that can transform native US-English speech into accented pronunciation. The ACM model included phonetic knowledge in its training to provide accurate feedback on how well certain pronunciation patterns were recovered in the synthesized waveform. \n\nIn addition, the researchers investigated the use of learned accent representations in contrast to static embeddings. The generated data from this approach was then used to train two state-of-the-art ASR systems. The researchers evaluated their approach on both native and non-native English datasets and found that synthetically accented data helped the ASR to better understand speech from seen accents. However, this observation did not transfer to unseen accents, nor was it observed for a model that had been pre-trained exclusively with native speech. \n\nThis research highlights the potential of synthetic cross-accent data augmentation for improving the performance of ASR systems. With further development, this technology could greatly benefit non-native speakers and make ASR technology more accessible to a wider range of users.",
		"keywords": [
			"ASR",
			"accent-conversion model",
			"synthetic data",
			"data augmentation",
			"phonetic knowledge"
		],
		"prompt": "An image of a person speaking into a microphone, with text on the side highlighting the importance of making ASR technology accessible for non-native speakers.",
		"link": "http://arxiv.org/abs/2303.00802",
		"id": "733cd983af761cda7ac52ce8ea9cb58f",
		"slug": "transforming-automatic-speech-recognition--synthetic-cross-accent-data-augmentation"
	},
	{
		"title": "Unlocking the Potential of Information Retrieval: How Large Language Models can Boost Zero-Shot Accuracy",
		"intro": "Are you tired of being limited by small labeled datasets in information retrieval tasks? A group of researchers may have found the solution to this challenge using the power of large language models. By generating synthetic queries cheaply, they were able to fine-tune efficient retrievers for long-tail domains, achieving substantially lower latency than standard reranking methods. Keep reading to find out more!",
		"text": "Many information retrieval tasks, such as search engines or recommendation systems, require fine-tuning of models with large labeled datasets. The problem is that such datasets are often unavailable or can quickly diminish in utility due to domain shifts. Researchers have developed a solution to this challenge by using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method involves using an expensive LLM to generate a small number of synthetic queries, which are then used to develop a family of reranker models. After that, a much less expensive LLM can be used to create even more synthetic queries for fine-tuning. These rerankers are then distilled into a single efficient retriever, which can be used in the target domain.\n\nThe researchers showed that this technique, called Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers (UDAPDR), boosts zero-shot accuracy in long-tail domains, even when only 2K synthetic queries are used for fine-tuning. Additionally, it achieves substantially lower latency than standard reranking methods. The end-to-end approach, including synthetic datasets and replication code, is publicly available on Github.\n\nThe potential of this technique goes beyond information retrieval. It shows the power of large language models in generating synthetic data for a wide range of applications, including those in the cyberpunk world. Imagine autonomous robots or virtual assistants powered by LLM-generated data, enabling them to adapt to new environments and improve their performance. The possibilities are endless!",
		"keywords": [
			"information retrieval",
			"large language models",
			"synthetic queries",
			"reranker models",
			"zero-shot accuracy"
		],
		"prompt": "An image of a futuristic virtual assistant powered by large language models, adapting to a new environment and improving its performance.",
		"link": "http://arxiv.org/abs/2303.00807",
		"id": "864e822fbdd23813b39feaf5323c93f6",
		"slug": "unlocking-the-potential-of-information-retrieval--how-large-language-models-can-boost-zero-shot-accuracy"
	},
	{
		"title": "Battling DeFi Frauds: Open-Source Tools to Expose and Prosecute Scammers",
		"intro": "DeFi has become a haven for fraudsters, and the victims are losing billions every year. As the perpetrators continue to cover their tracks, it's becoming more challenging to bring them to justice. However, a team of researchers has developed open-source tools that can investigate DeFi scams using the Ethereum Blockchain. In this article, we'll delve deeper into their findings and how their tools can revolutionize the fight against DeFi fraud.",
		"text": "With the rise of DeFi platforms, fraudsters are continually inventing new tactics to defraud unsuspecting investors. Despite many victims reporting these crimes, there's hardly any legal action against the perpetrators. While decentralized finance platforms provide a certain degree of anonymity, the public nature of the Ethereum blockchain offers an opportunity to track and prosecute scammers. That's where the innovative investigative tools developed by the researchers come in.",
		"keywords": ["DeFi", "fraud", "Ethereum blockchain", "money laundering", "investigation"],
		"prompt": "An image of a futuristic tool using the Ethereum blockchain to detect and block fraudulent transactions",
		"link": "http://arxiv.org/abs/2303.00810",
		"id": "e5e715e2a23b841d704bd7bc6e0b0a44",
		"slug": "battling-defi-frauds--open-source-tools-to-expose-and-prosecute-scammers"
	}
]
